{"cells":[{"cell_type":"markdown","source":["#### Spark Internals"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b441c5ee-e73a-443e-abdf-bad915da3d39"}}},{"cell_type":"markdown","source":["Example to remember -> A classroom has to count candies'bags\n\nThe classroom is the cluster, the teacher is the driver, a desk with 2 students is an executor and a student is a core. Two students at the same desk share the same accessories, pencils, notebooks...\nCandy bags are our dataset and each candy bag is a partition. Each candy piece is a record in our dataset."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"03e8388a-c98c-46a4-91df-fdb3c248f41a"}}},{"cell_type":"markdown","source":["**spark.speculation**\n\nApache Spark has the ‘speculative execution’ feature to handle the slow tasks in a stage due to environmental issues like slow network, disk, etc. If one task is running slowly in a stage, the Spark driver can launch a speculation task for it on a different host. Between the regular task and its speculation task, the Spark system will later take the result from the first successfully completed task and kill the slower one.\n\nSo, if I set `spark.speculation` to true -> it will re-launch one or more tasks if they are running slowly in a stage."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d5990701-9ab7-4f47-aaa2-71127742ed07"}}},{"cell_type":"markdown","source":["**spark.memory.fraction**\n\nIt expresses the size of M as a fraction of the (JVM heap space - 300MiB) (default 0.6). The rest of the space (40%) is reserved for user data structures, internal metadata in Spark, and safeguarding against OOM errors in the case of sparse and unusually large records.\n\nIt is the percentage of memory used for computation in shuffles, joins, sorts and aggregations."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c49471f1-b0f8-41e2-909b-5dc42dad062b"}}},{"cell_type":"markdown","source":["##### Shuffling"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fcec6587-17aa-4f16-ae5e-d0fc57975e0c"}}},{"cell_type":"markdown","source":["##### Narrow and wide transformations \n* In Narrow transformation (select, filter, cast, union, contains, map, flatMap, MapPartition, sample, union, drop, cache, coalesce --> when numPartitions is reduced), all the elements that are required to compute the records in single partition live in the single partition of parent RDD. A limited subset of partition is used to calculate the result. Narrow transformations are the result of map() and filter() functions and these compute data that live on a single partition meaning there will not be any data movement between partitions to execute narrow transformations.\n* In wide transformation (distinct, groupBy, sort, join, orderBy, repartition, collect, cartesian, intersection, reducedByKey, groupByKey), all the elements that are required to compute the records in the single partition may live in many partitions of parent RDD. Wider transformations are the result of groupByKey() and reduceByKey() functions and these compute data that live on many partitions meaning there will be data movements between partitions to execute wider transformations. Since these shuffles the data, they also called shuffle transformations."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a77d457c-84e5-43de-bb8d-3d2cdba98abc"}}},{"cell_type":"markdown","source":["Spark supports two types of shared variables: broadcast variables and accumulators.\n\n**Accumulators**, which are variables that are only “added” to, such as counters and sums, through an associative and commutative operation and can therefore be efficiently supported in parallel. They can be used to implement counters (as in MapReduce) or sums. Spark natively supports accumulators of numeric types, and programmers can add support for new types.\n\nAs a user, you can create named or unnamed accumulators. A named accumulator will display in the web UI for the stage that modifies that accumulator. Spark displays the value for each accumulator modified by a task in the “Tasks” table.\n\nTracking accumulators in the UI can be useful for understanding the progress of running stages.\n\nAn accumulator is created from an initial value v by calling `SparkContext.accumulator(v)`. Tasks running on a cluster can then add to it using the add method or the += operator. However, they cannot read its value. Only the driver program can read the accumulator’s value, using its value method.\n\nAccumulators do not change the lazy evaluation model of Spark. If they are being updated within an operation on an RDD, their value is only updated once that RDD is computed as part of an action. Consequently, accumulator updates are not guaranteed to be executed when made within a lazy transformation like map().\n\n**BROADCAST VARIABLES (and accumulators)**. For the exam remember that broadcast variables are immutable and lazily replicated across all nodes in the cluster when an action is triggered. Broadcast variables are efficient at scale, as they avoid the cost of serializing data for every task. They can be used in the context of RDDs or Structured APIs.\n\nIn Spark RDD and DataFrame, Broadcast variables are read-only shared variables that are cached and available on all nodes in a cluster in-order to access or use by the tasks. Instead of sending this data along with every task, spark distributes broadcast variables to the machine using efficient broadcast algorithms to reduce communication costs.\n\nLet me explain with an example, assume you are getting a two-letter country state code in a file and you wanted to transform it to full state name, (for example CA to California, NY to New York e.t.c) by doing a lookup to reference mapping. In some instances, this data could be large and you may have many such lookups (like zip code).\nInstead of distributing this information along with each task over the network (overhead and time consuming), we can use the broadcast variable to cache this lookup info on each machine and tasks use this cached info while executing the transformations.\n\nWhen you run a Spark RDD, DataFrame jobs that has the Broadcast variables defined and used, Spark does the following.\n\n* Spark breaks the job into stages that have distributed shuffling and actions are executed with in the stage.\n* Later Stages are also broken into tasks\n* Spark broadcasts the common data (reusable) needed by tasks within each stage.\n* The broadcasted data is cache in serialized format and deserialized before executing each task.\n\nYou should be creating and using broadcast variables for data that shared across multiple stages and tasks.\nNote that broadcast variables are not sent to executors with sc.broadcast(variable) call instead, they will be sent to executors when they are first used.\n\nThe Spark Broadcast is created using the broadcast(v) method of the SparkContext class. This method takes the argument v that you want to broadcast.\n\nA broadcast variable is entirely cached on each worker node so it doesn't need to be shipped or shuffled between nodes with each stage.\n\n`spark.sql.autoBroadcastJoinThreshold` -> property used to configure the broadcasting of a dataframe without the use of the broadcast() operation. Configures the maximum size in bytes for a table that will be broadcast to all worker nodes when performing a join. By setting this value to -1 broadcasting can be disabled. Note that currently statistics are only supported for Hive Metastore tables where the command ANALYZE TABLE tableName COMPUTE STATISTICS noscan has been run. (Default 10485760 (10 MB))\n\nbroadcast function: Default is 10mb but we have used till 300 mb which is controlled by spark.sql.autoBroadcastJoinThreshold.\n\nA custom broadcast class can be defined by extending org.apache.spark.utilbroadcastV2 in Java or Scala or pyspark.Accumulatorparams in Python.\n\nIt is a way of updating a value inside a variety of transformations and propagating that value to the driver node in an efficient and fault-tolerant way.\n\nIt provides a mutable variable that Spark cluster can safely update on a per-row basis."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dc2931a7-f21b-4fb5-af25-0a4a734060f8"}}},{"cell_type":"markdown","source":["Join operations are a common type of transformation in big data analytics in which two data sets, in the form of tables or DataFrames, are merged over a common\nmatching key. Join operations trigger a large amount of data movement across Spark executors.\n\nAt the heart of these transformations is how Spark computes what data to produce, what keys and associated data to write to the disk, and how to transfer those keys and data to nodes as part of operations like groupBy(), join(), agg(), sortBy(), and reduceByKey(). This movement is commonly referred to as the shuffle.\n\n\n-> **Join, big table-to-small table**\n\nWhen the table is small enough to fit into the memory of a single worker node, we can optimize our join. It can often be more efficient to use a broadcast join. What this means is that we will replicate our small DataFrame onto every worker node in the cluster. Now this sounds expensive. However, what this does is\nprevent us from performing the all-to-all communication during the entire join process. Instead, we perform it only once at the beginning and then let each individual worker node perform the work without having to wait or communicate with any other worker node.\n\n**BROADCAST JOIN** (Broadcast Hash Join, also known as map-side-only join): It is a join operation of a large data frame with a smaller data frame in PySpark Join model. It reduces the data shuffling by broadcasting the smaller data frame in the nodes of PySpark cluster. The data is sent and broadcasted to all nodes in the cluster. This is an optimal and cost-efficient join model that can be used in the PySpark application.\n\nBroadcasting is something that publishes the data to all the nodes of a cluster in PySpark data frame. Broadcasting further avoids the shuffling of data and the data network operation is comparatively lesser.\n\nThe broadcast hash join is employed when two data sets, one small (fitting in the driver’s and executor’s memory) and another large enough to ideally be spared from movement, need to be joined over certain conditions or columns. Using a Spark broadcast variable, the smaller data set is broadcasted by the driver to all Spark executors, and subsequently joined with the larger data set on each executor.\n\nBy default Spark will use a broadcast join if the smaller data set is less than 10 MB. This configuration is set in `spark.sql.autoBroadcastJoinThreshold`; you can decrease or increase the size depending on how much memory you have on each executor and in the driver. If you are confident that you have enough memory you can use a broadcast join with DataFrames larger than 10 MB (even up to 100 MB).\n\nThe default value of spark.sql.autoBroadcastJoinThreshold is 10MB but my table size is 15 MB. Hence, the broadcast join will not take place.\n\nThe BHJ is the easiest and fastest join Spark offers, since it does not involve any shuffle of the data set; all the data is available locally to the executor after a broadcast. You just have to be sure that you have enough memory both on the Spark driver’s and the executors’ side to hold the smaller data set in memory.\n\nIn Spark 3.0, you can use `joinedDF.explain('mode')` to display a readable and digestible output. The modes include 'simple', 'extended', 'codegen', 'cost', and\n'formatted'.\n\nSpecifying a value of -1 in `spark.sql.autoBroadcastJoinThreshold` will cause Spark to always resort to a shuffle sort merge join. (-1 disable broadcasting)\n\nBroadcast is not a valid join type. If i want to join the DataFrame \"itemsDF\" with the larger DataFrame \"transactionsDF\" on column \"itemID\" I can use this code:\n\n`transactionsDf.join(broadcast(itemsDf), \"itemId\")`\n\nThis would imply a inner join (the default in dataframe.join())\n\n**Shuffle Sort Merge Join**, as the name indicates, involves a sort operation. Shuffle Sort Merge Join has 3 phases.\n* Shuffle Phase – both datasets are shuffled\n* Sort Phase – records are sorted by key on both sides\n* Merge Phase – iterate over both sides and join based on the join key.\nShuffle Sort Merge Join is preferred when both datasets are big and can not fit in memory – with or without shuffle.\n\n`spark.sql.join.preferSortMergeJoin`  by default is set to true as this is preferred when datasets are big on both sides. \nSpark will pick Broadcast Hash Join if a dataset is small. In our case both datasets are small so to force a Sort Merge join we are setting `spark.sql.autoBroadcastJoinThreshold`  to -1 and this will disable Broadcast Hash Join.\n\nSort merge join doesn’t work on non equi joins. Both shuffle and sort are expensive operations. Use this join when a broadcast hash and shuffle hash joins are not possible.\n\nThe sort-merge algorithm is an efficient way to merge two large data sets over a common key that is sortable, unique, and can be assigned to or stored in the same partition—that is, two data sets with a common hashable key that end up being on the same partition. This join scheme has two phases: a sort phase followed by a merge phase. By default, the SortMergeJoin is enabled via spark.sql.join.preferSortMerge Join.\n\nWhen to use a shuffle sort merge join -> When each key within two large data sets can be sorted and hashed to the same partition by Spark.\n\nWhen different join strategy hints are specified on both sides of a join -> Spark prioritizes the broadcast hint over the merge."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6eefdc47-2f4b-4ae6-a489-f49a8e015b25"}}},{"cell_type":"markdown","source":["##### Spark accumulator variables\n\n* Accumulators provide a shared, mutable variable that a Spark cluster can safely update on a per-row basis.\n* For accumulator updates performed inside actions only, Spark guarantees that each task’s update to the accumulator will be applied only once, meaning that restarted tasks will not update the value.\n* In transformations, each task’s update can be applied more than once if tasks or job stages are re-executed.\n* You can define your own custom accumulator class by extending `org.apache.spark.util.AccumulatorV2` in Java or Scala or `pyspark.AccumulatorParam` in Python.\n* The Spark UI doesn't display all accumulators used by your application.\n\nSpark Accumulators are shared variables which are only “added” through an associative and commutative operation and are used to perform counters (Similar to Map-reduce counters) or sum operations.\nSpark by default supports to create an accumulators of any numeric type and provide a capability to add custom accumulator types.\n\nBroadcast variables are shared, immutable variables that are cached on every machine in the cluster instead of being serialized with every single task. The canonical use case is to pass around a small table that does fit in memory on executors.\n\nProgrammers can create following accumulators\n* named accumulators\n* unnamed accumulators\n\nWhen you create a named accumulator, you can see them on Spark web UI under the “Accumulator” tab. On this tab, you will see two tables; the first table “accumulable” – consists of all named accumulator variables and their values. And on the second table “Tasks” – value for each accumulator modified by a task.\n\nAnd, unnamed accumulators are not shows on Spark web UI, For all practical purposes it is suggestable to use named accumulators.\n\nSpark by default provides accumulator methods for long, double and collection types. All these methods are present in SparkContext class and return \n* LongAccumulator \n* DoubleAccumulator\n* CollectionAccumulator\n\nAccumulators are variables that are only “added” to through an associative operation and can therefore, be efficiently supported in parallel. They can be used to implement counters (as in MapReduce) or sums. Spark natively supports accumulators of numeric types, and programmers can add support for new types. If accumulators are created with a name, they will be displayed in Spark’s UI. An accumulator is created from an initial value v by calling SparkContext.accumulator(v)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"340aec61-5db5-4a63-a8f1-59f528b26b39"}}},{"cell_type":"markdown","source":["* In Apache Spark all transformations are evaluated lazily and all the actions are evaluated eagerly. In this case, the only command that will be evaluated lazily is df.join(). Eagerly evaluated commands are df.collect(), df.take(), df.show(), df.saveAsTable() (all actions)\n\n* Which command does not generate a shuffle? -> it must be a narrow transformation, for example map() as wide dependencies generate a shuffle (example collect, orderBy, repartition, distinct, join)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e99c96da-4191-42f6-aae9-23921e3cfed2"}}},{"cell_type":"markdown","source":["SHUFFLE: A shuffle is the process by which data is compared across partitions. We use shuffle to redistribute data among different executors or even among machines. Shuffle is an expensive operation."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"06087f4d-da3e-4918-9f78-4e89428c2fb2"}}},{"cell_type":"markdown","source":["The following spark config property represents the number of partition used in a wide transformation like join():\n\n`spark.sql.shuffle.partitions`\n\n-> Configures the number of partitions to use when shuffling data for joins or aggregations. (Default 200)\n\nYou can change the default number of 200 partitions:\n\n`spark.conf.set(\"spark.sql.shuffle.partitions, 100)`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0a5ef473-e41e-4073-9779-d1083ded6d72"}}},{"cell_type":"markdown","source":["Spark provides `spark.sql.shuffle.partitions` and `spark.default.parallelism` configurations to work with parallelism or partitions. \n\nBefore we jump into the differences let’s understand what is Spark shuffle? The Spark **shuffle** is a mechanism for redistributing or re-partitioning data so that the data grouped differently across partitions (functions like groupBy(), union(), join() can give a shuffle as a result, a shuffle among the executors or even among the machines). Spark shuffle is a very expensive operation as it moves the data between executors or even between worker nodes in a cluster. Spark automatically triggers the shuffle when we perform aggregation and join operations on RDD and DataFrame.\n\n`spark.default.parallelism` was introduced with RDD hence this property is only applicable to RDD. The default value for this configuration set to the number of all cores on all nodes in a cluster, on local, it is set to the number of cores on your system.\n\nWhereas `spark.sql.shuffle.partitions` was introduced with DataFrame and it only works with DataFrame, the default value for this configuration set to 200."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"85a6ad37-4fd6-4574-9efc-10e56fefba18"}}},{"cell_type":"markdown","source":["When we program in spark we start with \"read\" and then for example \"select, filter, group by, filter and finally write\" but for spark is the opposite, it starts executing backwards, from \"write\" and then it sees which is the transformation tha precedes it. \n\nShuffles introduce stage boundaries\nShuffles demarcate stage boundaries\n-> shuffle write\n-> shuffle read\n\n**Caching data** explicity accomplish the same thing. First execution caches some results, subsequent execution can read cache. For example if we cached at step 3, we would skip the first 3 operations."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d5553e7b-5c42-4985-a4e0-9ee87af993e0"}}},{"cell_type":"markdown","source":["##### Catalyst optimizer and query optimization\n\nSpark uses two engines to optimize and run the queries - **Catalyst and Tungsten**, in that order. Catalyst basically generates an optimized physical query plan from the logical query plan by applying a series of transformations like predicate pushdown, column pruning, and constant folding on the logical plan. This optimized query plan is then used by Tungsten to generate optimized code, that resembles hand written code, by making use of Whole-stage Codegen functionality introduced in Spark 2.0.\n\nCatalyst optimizer > This is the mechanism that makes our queries so incredibly fast.\nExtensible query optimizer, it contains a general library for representing trees and applying rules to manipulate them. It has several public extension points, including external data sources and users'defined types.\n\nAt the core of Spark SQL is the Catalyst optimizer, which leverages advanced programming language features (e.g. Scala’s pattern matching and quasi quotes) in a novel way to build an extensible query optimizer. Catalyst is based on functional programming constructs in Scala and designed with these key two purposes:\n* Easily add new optimization techniques and features to Spark SQL\n* Enable external developers to extend the optimizer (e.g. adding data source specific rules, support for new data types, etc.)\n\nCatalyst contains a general library for representing trees and applying rules to manipulate them. On top of this framework, it has libraries specific to relational query processing (e.g., expressions, logical query plans), and several sets of rules that handle different phases of query execution: analysis, logical optimization, physical planning, and code generation to compile parts of queries to Java bytecode. For the latter, it uses another Scala feature, quasiquotes, that makes it easy to generate code at runtime from composable expressions. Catalyst also offers several public extension points, including external data sources and user-defined types. As well, Catalyst supports both rule-based and cost-based optimization."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"58a0ca52-6c6b-46ac-9e32-d1cbc6cdf5ae"}}},{"cell_type":"markdown","source":["##### Project Tungsten\n\nProject Tungsten will be the largest change to Spark’s execution engine since the project’s inception. It focuses on substantially improving the efficiency of memory and CPU for Spark applications, to push performance closer to the limits of modern hardware. The goal of Project Tungsten is to improve Spark execution by optimising Spark jobs for CPU and memory efficiency. This effort includes three initiatives:\n\n* Memory Management and Binary Processing: leveraging application semantics to manage memory explicitly and eliminate the overhead of JVM object model and garbage collection.\n* Cache-aware computation: algorithms and data structures to exploit memory hierarchy.\n* Code generation: using code generation to exploit modern compilers and CPUs.\n\nImprovements of project Tungsten take place at code generation (the second-generation Tungsten engine, introduced in Spark 2.0, uses this approach to generate compact RDD code for final execution)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"98d7c0f2-9c11-4ba6-849b-fa8c9f719069"}}},{"cell_type":"markdown","source":["![query optimization](https://files.training.databricks.com/images/aspwd/query_optimization_catalyst.png)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"74c759aa-120e-44d9-be57-cbed7a6b036e"}}},{"cell_type":"markdown","source":["* Unresolved logical plan -> the instructions, what the developer logically wants to happen but columns, UDFS ecc are not resolved so the may not exist or we may have typos in the code.\nThis logical plan only represents a set of abstract transformations, it’s purely to convert the user’s set of expressions into the most optimized version. It does this by converting user code into an **unresolved logical plan**. This plan is unresolved because although your code might be valid, the tables or columns that it refers to might or might not exist. Spark uses the catalog, a repository of all table and DataFrame information, to resolve columns and tables in the analyzer. The analyzer might reject the unresolved logical plan if the required table or column name does not exist in the catalog. If the analyzer can resolve it, the result is passed through the Catalyst Optimizer, a collection of rules that attempt to optimize the\nlogical plan by pushing down predicates or selections.\n* (Metadata catalog) Then analysis happens, here is where we evaluate columns names, tables names ecc. \n* Logical plan -> then we make sure we are not referring to no existing columns or we have an order by to sort. We pontentially rewrite and reorder and so on the logical sequence. From this we get the OPTIMIZED LOGICAL PLAN.\n* Next comes the physical plans where the catalyst optimizer determines that there are multiple ways of executing a query. They represent what the engine will actually do. It is distinct from the logical plan because all of the optimization has been applied. Each optimization provides a different benefit -> cost model, each possible physical plan is evaluated according to the cost model and one is selected. After successfully creating an optimized logical plan, Spark then begins the physical planning process. The physical plan, often called a Spark plan, specifies how the logical plan will execute on the cluster by generating different physical execution strategies and comparing them through a cost model.\n* Finally we have code generation. Once the process is done the selected physical plan is compiled down to RDDs. Physical planning results in a series of RDDs and transformations.\n\n**CATALOG** The catalog is an interface through which the user may create, drop, alter or query underlying databases, tables, functions, etc."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"478ef681-798c-4bf7-a9cb-4aa7c066fb46"}}},{"cell_type":"markdown","source":["##### Adaptive Query Execution (AQE)\n\n-> Spark Query optimization technique that simplifies tuning of shuffle partition number.\n\n(AQE) is a new feature of spark 3.0. Disabled by default but it is recommended to enable it. It creates runtime statistics, these are based on the statistics of the finished planned nodes and re-optimize the execution plan of the remaining queries. Adaptive Query Execution re-optimises queries at materialisation points.\nAQE does not apply to all kinds of queries, but only to queries that are not streaming queries and that contain at least one exchange (typically expressed through a join, aggregate, or window operator) or one subquery.\n\nAdaptive Query Execution features are dynamically switching join strategies and dynamically optimising skew joins.\n\nAs soon as one or more of these stages finish materialization, the framework marks them complete in the physical query plan and updates the logical query plan accordingly, with the runtime statistics retrieved from completed stages. Based on these new statistics, the framework then runs the optimizer (with a selected list of logical optimization rules), the physical planner, as well as the physical optimization rules, which include the regular physical rules and the adaptive-execution-specific rules, such as coalescing partitions, skew join handling, etc. Now that we’ve got a newly optimized query plan with some completed stages, the adaptive execution framework will search for and execute new query stages whose child stages have all been materialized, and repeat the above execute-reoptimize-execute process until the entire query is done.\n\nThe AQE framework is shipped with three features:\n\n- Dynamically coalescing shuffle partitions\n- Dynamically switching join strategies\n- Dynamically optimizing skew joins\n\n\n**Data skew** can severely downgrade the performance of join queries. This feature dynamically handles skew in sort-merge join by splitting (and replicating if needed) skewed tasks into roughly evenly sized tasks. Spark  dynamically handles skew in sort-merge join by splitting skewed partitions. The configuration used to enable this feature is `spark.sql.adaptive.enabled` `spark.sql.adaptive.skewJoin.enabled` (For the last one: Spark dynamically handles skew in sort-merge join by splitting skewed partitions.)\n\nAQE converts sort-merge join to broadcast hash join when the runtime statistics of any join side is smaller than the broadcast hash join threshold. Which property is used to enable this feature? -> `spark.sql.adaptive.localShuffleReader.enabled`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ca5f117e-04a4-44dc-b5b5-07b890a415fb"}}},{"cell_type":"markdown","source":["`storesDF.describe(\"sqft\")` returns a DataFrame containing summary statistics only for columns qft in DataFrame storesDF."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9fb77823-be2b-483b-aed6-df8b701e808f"}}},{"cell_type":"markdown","source":["`explain(..)` prints the query plans, optionally formatted by a given explain mode."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9567dfd4-7b84-4824-9488-4badea3e246f"}}},{"cell_type":"code","source":["df = spark.read.parquet(\"/mnt/training/ecommerce/events/events.parquet\")\ndf.show(7)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d7cf3c7d-93ff-4ab6-ab3f-03a68f8370c3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------+------------------+----------+------------------------+----------------+--------------------+--------------------+--------------+--------------------------+-----------------+\n| device|         ecommerce|event_name|event_previous_timestamp| event_timestamp|                 geo|               items|traffic_source|user_first_touch_timestamp|          user_id|\n+-------+------------------+----------+------------------------+----------------+--------------------+--------------------+--------------+--------------------------+-----------------+\n|  macOS|{null, null, null}|  warranty|        1593878899217692|1593878946592107|      {Montrose, MI}|                  []|        google|          1593878899217692|UA000000107379500|\n|Windows|{null, null, null}|     press|        1593876662175340|1593877011756535|   {Northampton, MA}|                  []|        google|          1593876662175340|UA000000107359357|\n|  macOS|{null, null, null}|  add_item|        1593878792892652|1593878815459100|       {Salinas, CA}|[{null, M_STAN_T,...|       youtube|          1593878455472030|UA000000107375547|\n|    iOS|{null, null, null}|mattresses|        1593878178791663|1593878809276923|       {Everett, MA}|                  []|      facebook|          1593877903116176|UA000000107370581|\n|Windows|{null, null, null}|mattresses|                    null|1593878628143633| {Cottage Grove, MN}|                  []|        google|          1593878628143633|UA000000107377108|\n|Windows|{null, null, null}|      main|                    null|1593878634344194|        {Medina, MN}|                  []|       youtube|          1593878634344194|UA000000107377161|\n|    iOS|{null, null, null}|      main|                    null|1593877936171803|{Mount Pleasant, UT}|                  []|        direct|          1593877936171803|UA000000107370851|\n+-------+------------------+----------+------------------------+----------------+--------------------+--------------------+--------------+--------------------------+-----------------+\nonly showing top 7 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------+------------------+----------+------------------------+----------------+--------------------+--------------------+--------------+--------------------------+-----------------+\n| device|         ecommerce|event_name|event_previous_timestamp| event_timestamp|                 geo|               items|traffic_source|user_first_touch_timestamp|          user_id|\n+-------+------------------+----------+------------------------+----------------+--------------------+--------------------+--------------+--------------------------+-----------------+\n|  macOS|{null, null, null}|  warranty|        1593878899217692|1593878946592107|      {Montrose, MI}|                  []|        google|          1593878899217692|UA000000107379500|\n|Windows|{null, null, null}|     press|        1593876662175340|1593877011756535|   {Northampton, MA}|                  []|        google|          1593876662175340|UA000000107359357|\n|  macOS|{null, null, null}|  add_item|        1593878792892652|1593878815459100|       {Salinas, CA}|[{null, M_STAN_T,...|       youtube|          1593878455472030|UA000000107375547|\n|    iOS|{null, null, null}|mattresses|        1593878178791663|1593878809276923|       {Everett, MA}|                  []|      facebook|          1593877903116176|UA000000107370581|\n|Windows|{null, null, null}|mattresses|                    null|1593878628143633| {Cottage Grove, MN}|                  []|        google|          1593878628143633|UA000000107377108|\n|Windows|{null, null, null}|      main|                    null|1593878634344194|        {Medina, MN}|                  []|       youtube|          1593878634344194|UA000000107377161|\n|    iOS|{null, null, null}|      main|                    null|1593877936171803|{Mount Pleasant, UT}|                  []|        direct|          1593877936171803|UA000000107370851|\n+-------+------------------+----------+------------------------+----------------+--------------------+--------------------+--------------+--------------------------+-----------------+\nonly showing top 7 rows\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import col\n\nlimitEventsDF = (df\n                 .filter(col(\"event_name\") != \"reviews\")\n                 .filter(col(\"event_name\") != \"checkout\")\n                 .filter(col(\"event_name\") != \"register\")\n                 .filter(col(\"event_name\") != \"email_coupon\")\n                 .filter(col(\"event_name\") != \"cc_info\")\n                 .filter(col(\"event_name\") != \"delivery\")\n                 .filter(col(\"event_name\") != \"shipping_info\")\n                 .filter(col(\"event_name\") != \"press\")\n                )\n\nlimitEventsDF.explain(True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ec9a8afa-5a6f-4ca2-8e67-14d3ab2b30d9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"== Parsed Logical Plan ==\n'Filter NOT ('event_name = press)\n+- Filter NOT (event_name#4083 = shipping_info)\n   +- Filter NOT (event_name#4083 = delivery)\n      +- Filter NOT (event_name#4083 = cc_info)\n         +- Filter NOT (event_name#4083 = email_coupon)\n            +- Filter NOT (event_name#4083 = register)\n               +- Filter NOT (event_name#4083 = checkout)\n                  +- Filter NOT (event_name#4083 = reviews)\n                     +- Relation [device#4081,ecommerce#4082,event_name#4083,event_previous_timestamp#4084L,event_timestamp#4085L,geo#4086,items#4087,traffic_source#4088,user_first_touch_timestamp#4089L,user_id#4090] parquet\n\n== Analyzed Logical Plan ==\ndevice: string, ecommerce: struct<purchase_revenue_in_usd:double,total_item_quantity:bigint,unique_items:bigint>, event_name: string, event_previous_timestamp: bigint, event_timestamp: bigint, geo: struct<city:string,state:string>, items: array<struct<coupon:string,item_id:string,item_name:string,item_revenue_in_usd:double,price_in_usd:double,quantity:bigint>>, traffic_source: string, user_first_touch_timestamp: bigint, user_id: string\nFilter NOT (event_name#4083 = press)\n+- Filter NOT (event_name#4083 = shipping_info)\n   +- Filter NOT (event_name#4083 = delivery)\n      +- Filter NOT (event_name#4083 = cc_info)\n         +- Filter NOT (event_name#4083 = email_coupon)\n            +- Filter NOT (event_name#4083 = register)\n               +- Filter NOT (event_name#4083 = checkout)\n                  +- Filter NOT (event_name#4083 = reviews)\n                     +- Relation [device#4081,ecommerce#4082,event_name#4083,event_previous_timestamp#4084L,event_timestamp#4085L,geo#4086,items#4087,traffic_source#4088,user_first_touch_timestamp#4089L,user_id#4090] parquet\n\n== Optimized Logical Plan ==\nFilter (isnotnull(event_name#4083) AND (((NOT (event_name#4083 = reviews) AND NOT (event_name#4083 = checkout)) AND (NOT (event_name#4083 = register) AND NOT (event_name#4083 = email_coupon))) AND (((NOT (event_name#4083 = cc_info) AND NOT (event_name#4083 = delivery)) AND NOT (event_name#4083 = shipping_info)) AND NOT (event_name#4083 = press))))\n+- Relation [device#4081,ecommerce#4082,event_name#4083,event_previous_timestamp#4084L,event_timestamp#4085L,geo#4086,items#4087,traffic_source#4088,user_first_touch_timestamp#4089L,user_id#4090] parquet\n\n== Physical Plan ==\n*(1) Filter ((((((((isnotnull(event_name#4083) AND NOT (event_name#4083 = reviews)) AND NOT (event_name#4083 = checkout)) AND NOT (event_name#4083 = register)) AND NOT (event_name#4083 = email_coupon)) AND NOT (event_name#4083 = cc_info)) AND NOT (event_name#4083 = delivery)) AND NOT (event_name#4083 = shipping_info)) AND NOT (event_name#4083 = press))\n+- *(1) ColumnarToRow\n   +- FileScan parquet [device#4081,ecommerce#4082,event_name#4083,event_previous_timestamp#4084L,event_timestamp#4085L,geo#4086,items#4087,traffic_source#4088,user_first_touch_timestamp#4089L,user_id#4090] Batched: true, DataFilters: [isnotnull(event_name#4083), NOT (event_name#4083 = reviews), NOT (event_name#4083 = checkout), N..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[dbfs:/mnt/training/ecommerce/events/events.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(event_name), Not(EqualTo(event_name,reviews)), Not(EqualTo(event_name,checkout)), Not(..., ReadSchema: struct<device:string,ecommerce:struct<purchase_revenue_in_usd:double,total_item_quantity:bigint,u...\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["== Parsed Logical Plan ==\n'Filter NOT ('event_name = press)\n+- Filter NOT (event_name#4083 = shipping_info)\n   +- Filter NOT (event_name#4083 = delivery)\n      +- Filter NOT (event_name#4083 = cc_info)\n         +- Filter NOT (event_name#4083 = email_coupon)\n            +- Filter NOT (event_name#4083 = register)\n               +- Filter NOT (event_name#4083 = checkout)\n                  +- Filter NOT (event_name#4083 = reviews)\n                     +- Relation [device#4081,ecommerce#4082,event_name#4083,event_previous_timestamp#4084L,event_timestamp#4085L,geo#4086,items#4087,traffic_source#4088,user_first_touch_timestamp#4089L,user_id#4090] parquet\n\n== Analyzed Logical Plan ==\ndevice: string, ecommerce: struct<purchase_revenue_in_usd:double,total_item_quantity:bigint,unique_items:bigint>, event_name: string, event_previous_timestamp: bigint, event_timestamp: bigint, geo: struct<city:string,state:string>, items: array<struct<coupon:string,item_id:string,item_name:string,item_revenue_in_usd:double,price_in_usd:double,quantity:bigint>>, traffic_source: string, user_first_touch_timestamp: bigint, user_id: string\nFilter NOT (event_name#4083 = press)\n+- Filter NOT (event_name#4083 = shipping_info)\n   +- Filter NOT (event_name#4083 = delivery)\n      +- Filter NOT (event_name#4083 = cc_info)\n         +- Filter NOT (event_name#4083 = email_coupon)\n            +- Filter NOT (event_name#4083 = register)\n               +- Filter NOT (event_name#4083 = checkout)\n                  +- Filter NOT (event_name#4083 = reviews)\n                     +- Relation [device#4081,ecommerce#4082,event_name#4083,event_previous_timestamp#4084L,event_timestamp#4085L,geo#4086,items#4087,traffic_source#4088,user_first_touch_timestamp#4089L,user_id#4090] parquet\n\n== Optimized Logical Plan ==\nFilter (isnotnull(event_name#4083) AND (((NOT (event_name#4083 = reviews) AND NOT (event_name#4083 = checkout)) AND (NOT (event_name#4083 = register) AND NOT (event_name#4083 = email_coupon))) AND (((NOT (event_name#4083 = cc_info) AND NOT (event_name#4083 = delivery)) AND NOT (event_name#4083 = shipping_info)) AND NOT (event_name#4083 = press))))\n+- Relation [device#4081,ecommerce#4082,event_name#4083,event_previous_timestamp#4084L,event_timestamp#4085L,geo#4086,items#4087,traffic_source#4088,user_first_touch_timestamp#4089L,user_id#4090] parquet\n\n== Physical Plan ==\n*(1) Filter ((((((((isnotnull(event_name#4083) AND NOT (event_name#4083 = reviews)) AND NOT (event_name#4083 = checkout)) AND NOT (event_name#4083 = register)) AND NOT (event_name#4083 = email_coupon)) AND NOT (event_name#4083 = cc_info)) AND NOT (event_name#4083 = delivery)) AND NOT (event_name#4083 = shipping_info)) AND NOT (event_name#4083 = press))\n+- *(1) ColumnarToRow\n   +- FileScan parquet [device#4081,ecommerce#4082,event_name#4083,event_previous_timestamp#4084L,event_timestamp#4085L,geo#4086,items#4087,traffic_source#4088,user_first_touch_timestamp#4089L,user_id#4090] Batched: true, DataFilters: [isnotnull(event_name#4083), NOT (event_name#4083 = reviews), NOT (event_name#4083 = checkout), N..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[dbfs:/mnt/training/ecommerce/events/events.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(event_name), Not(EqualTo(event_name,reviews)), Not(EqualTo(event_name,checkout)), Not(..., ReadSchema: struct<device:string,ecommerce:struct<purchase_revenue_in_usd:double,total_item_quantity:bigint,u...\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##### Caching\n\nWhat is the difference between caching and persistence? In Spark they are synonymous. Two API calls, cache() and persist(), offer these capabilities. The latter provides more control over how and where your data is stored—in memory and on disk, serialized and unserialized. Both contribute to better performance for frequently accessed DataFrames or tables.\n\n\nBy default the data of a DataFrame is present on a Spark cluster only while it is being processed during a query -- it is not automatically persisted on the cluster afterwards. (Spark is a data processing engine, not a data storage system.) You can explicity request Spark to persist a DataFrame on the cluster by invoking its `cache` method.\n\nIf you do cache a DataFrame, you should always explictly evict it from cache by invoking `unpersist` when you no longer need it. (`cachedDF.unpersist()`)\n\nThe elements that should be cached are frequently accessed DataFrames or tables. (As a general rule, you should use memory caching judiciously, as it can incur resource costs in serializing and deserializing, depending on the StorageLevel used, don't cache dataframes that are too big to fit in memory).\n\n<img src=\"https://files.training.databricks.com/images/icon_best_32.png\" alt=\"Best Practice\"> Caching a DataFrame can be appropriate if you are certain that you will use the same DataFrame multiple times, as in:\n\n- Exploratory data analysis\n- Machine learning model training\n- Common use cases for caching are scenarios where you will want to access a large data set repeatedly for queries or transformations. -> DataFrames accessed commonly for doing frequent transformations during ETL or building data pipelines.\n\n<img src=\"https://files.training.databricks.com/images/icon_warn_32.png\" alt=\"Warning\"> Aside from those use cases, you should **not** cache DataFrames because it is likely that you'll *degrade* the performance of your application.\n\n- Caching consumes cluster resources that could otherwise be used for task execution\n- Caching can prevent Spark from performing query optimizations, as shown in the next example\n- You should not cache pr persist when DataFrames that are too big to fit in memory\n\nWhen you use cache() or persist(), the DataFrame is not fully cached until you invoke an action that goes through every record (e.g., count()). If you use an action like take(1), only one partition will be cached because Catalyst realizes that you do not need to compute all the partitions just to retrieve one record.\n\nWhen using DataFrame.persist() data on disk is always serialized. Data on disk is always serialized using either Java or Kryo serialization."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"99afec16-e697-4032-9a9c-c6e5d57e1609"}}},{"cell_type":"markdown","source":["Difference cache() and persist() \n* cache() command always places data in memory and disk by default (MEMORY_AND_DISK). Cache() will store as many of the partitions read in memory across Spark executors as memory allows. While a DataFrame may be fractionally cached, partitions cannot be fractionally cached (e.g., if you have 8 partitions but only 4.5 partitions can fit in memory, only 4 will be cached). However, if not all your partitions are cached, when you want to access the data again, the partitions that are not cached will have to be recomputed, slowing down your Spark job.\n* persist() method can take a StorageLevel object to specify exactly where to cache data. (default MEMORY_AND_DISK) The default argument can be updated. Persist is nuanced, providing control over how your data is cached.\n\nData is always serialized when stored on disk, whereas you need to specify if you wish to serialize data in memory.\n\nThe cache() method will store as many of the partitions read in memory across Spark executors as memory allows. DataFrame may be fractionally cached, partitions cannot be fractionally cached (e.g., if you have 8 partitions but only 4.5 partitions can fit in memory, only 4 will be cached)\n\nExample:\n\"Cache a df as SERIALIZED Java objects in the JVM and; \nIf the df does not fit in memory, store the partitions that don’t fit on disk, and read them from \nthere when they’re needed; \nReplicate each partition on two cluster nodes.\"\n-> `df.persist(StorageLevel.MEMORY_AND_DISK_2_SER)`\n\n* The default storage level for a DataFrame is StorageLevel.MEMORY_AND_DISK.\n* The DataFrame class does not have an uncache() operation.\n* Explicit caching can decrease application performance by interferring with the Catalyst optimizer's ability to optimize some queries.\n* The default storage level is MEMORY_AND_DISK for cache() and persist() both in Spark 3.0.0. However, it was changed to MEMORY_AND_DISK_DESER in later versions (Spark 3.1.1). IF the exam is having an option as MEMORY_AND_DISK_DESER then go for it else MEMORY_AND_DISK is the correct answer."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e7a4ea31-26c1-4243-94b2-86cb9526f6f2"}}},{"cell_type":"markdown","source":["* MEMORY_ONLY: Este es el comportamiento predeterminado del método cache() para almacenar RDD, y almacena DataFrames o RDD como objetos deserializados en la memoria JVM. Cuando no hay suficiente memoria disponible, no se guardarán los DataFrame de algunas particiones y se volverán a calcular cuando sea necesario. Esto requiere más memoria y a diferencia de RDD, esto sería más lento que el nivel MEMORY_AND_DISK ya que vuelve a calcular las particiones no guardadas y volver a calcular la representación en columnas en memoria de la tabla subyacente y esto es costoso.\n* MEMORY_ONLY_SER: Es lo mismo que MEMORY_ONLY, pero la diferencia es que almacena el RDD como objetos serializados en la memoria JVM. Se necesita menos memoria (uso eficiente del espacio) que MEMORY_ONLY, ya que guarda objetos como serializados y requiere algunos ciclos adicionales de CPU para deserializar lo que puede implicar mas tiempo de procesamiento.\n* MEMORY_AND_DISK: Este es el comportamiento predeterminado para almacenar DataFrames o Datasets. En este nivel de almacenamiento, el DataFrame se almacenará en la memoria JVM como objetos deserializados. Cuando el almacenamiento requerido es mayor que la memoria disponible, almacena algunas de las particiones sobrantes en el disco y lee los datos del disco cuando es necesario. ( Data is stored directly as objects in memory and a copy is serialized and stored on disk. Data is stored directly as objects in memory, but if there’s insufficient memory the rest is serialized and stored on disk.)\n* MEMORY_AND_DISK_SER: Esto es parecido a MEMORY_AND_DISK, la diferencia es que serializa los objetos DataFrame en la memoria y en el disco cuando no hay espacio disponible. Like MEMORY_AND_DISK, but data is serialized when stored in memory. (Data is always serialized when stored on disk.)\n* DISK_ONLY: En este nivel de almacenamiento, DataFrame se almacena sólo en el disco y el tiempo de cálculo de la CPU suele ser alto.\n\nSi usamos la opción memoria only y el RDD no entra en memoria, desecha lo que no entre, y si hace falta usarlo de nuevo volverá a ejecutar la secuencia de transformaciones previas. En estos casos es muy útil la opción compartida de memoria y disco. \n\nLa opción memoria y disco deja datos en disco cuando la memoria se llena.\n\nLa opción memoria y disco ser guarda en memoria la representación serializada de los datos. \n\nEl metodo **persist** permite elegir el tipo de persistencia. \n\nBear in mind that data is always serialized when stored on disk, whereas you need to specify if you wish to serialize data in memory (example MEMORY_AND_DISK_2_SER). StorageLevel.MEMORY_AND_DISK_SER_2 is same as MEMORY_AND_DISK_SER storage level but replicate each partition to two cluster nodes.\n\nStorageLevel.MEMORY_AND_DISK_SER is same as MEMORY_AND_DISK storage level difference being it serializes the DataFrame objects in memory and on disk when space is not available."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cb02b363-dd17-40e0-9b5e-69bc69b107e4"}}},{"cell_type":"markdown","source":["The tab \"storage\" of Spark UI is useful to investigate information about your Cached DataFrames.\n\nThe amount of memory available to each executor is controlled by `spark.executor.memory`. During map and shuffle operations, Spark writes to and reads from the local disk’s shuffle files, so there is heavy I/O activity. This can result in a bottleneck, because the default configurations are suboptimal for large-scale Spark jobs."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d714b064-766e-4f2c-9e45-b56ace4d21ce"}}},{"cell_type":"markdown","source":["Not only can you cache DataFrames, but you can also cache the tables or views derived from DataFrames. \n\n**Spark SQL** can cache tables using an in-memory columnar format by calling **spark.catalog.cacheTable(\"tableName\")** or **dataFrame.cache()**. Then Spark SQL will scan only required columns and will automatically tune compression to minimize memory usage and GC pressure. You can call **spark.catalog.uncacheTable(\"tableName\")** to remove the table from memory.\n\nIf you do CACHE TABLE MyTableName in SQL though, it is defaulted to be eager caching and will cache the entire table. You can choose LAZY caching in SQL like so:\n\n`CACHE LAZY TABLE MyTableName`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0dfc719c-b3d9-4775-b80a-57f5bb329349"}}},{"cell_type":"markdown","source":["##### Predicate pushdown\n\nPredicate push down is another feature of Spark and Parquet that can improve query performance by reducing the amount of data read from Parquet files. Predicate push down works by evaluating filtering predicates in the query against metadata stored in the Parquet files. Parquet can optionally store statistics (in particular the minimum and maximum value for a column chunk) in the relevant metadata section of its files and can use that information to take decisions, for example, to skip reading chunks of data if the provided filter predicate value in the query is outside the range of values stored for a given column. \n\nPredicate Pushdown points to the filter conditions typically the ‘where clause’ which determines the number of rows to be returned. It basically relates to which rows will be filtered, not which columns.\n\n**Catalyst** basically generates an optimized physical query plan from the logical query plan by applying a series of transformations like predicate pushdown, column pruning, and constant folding on the logical plan.\n\n* Predicate pushdown corresponds to WHERE clause in the SQL query. If these can be use directly by external system (like Relational Databases ) or for partition pruning (like in Parquet) this means reduced amount of data that has to be transferred / loaded from disk.\n* Constant folding is the process of recognizing and evaluating constant expressions at compile time rather than computing them at runtime. This is not in any particular way specific to Catalyst. It is just a standard compilation technique and its benefits should be obvious. It is better to compute expression once than repeat this for each row.\n* Projection pruning benefits are pretty much the same as for predicate pushdown. If some columns are not used, downstream data source may discard this on read."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b2af3d5e-0d18-4647-8ed4-aaeef1638087"}}},{"cell_type":"code","source":["%scala\n// Ensure that the driver class is loaded\nClass.forName(\"org.postgresql.Driver\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ec41417c-d345-4897-82da-f8a8af7b0064"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">res0: Class[_] = class org.postgresql.Driver\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">res0: Class[_] = class org.postgresql.Driver\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["jdbcURL = \"jdbc:postgresql://54.213.33.240/training\"\n\n# Username and Password w/read-only rights\nconnProperties = {\n    \"user\" : \"training\",\n    \"password\" : \"training\"\n}\n\nppDF = (spark\n        .read\n        .jdbc(\n            url=jdbcURL,                  # the JDBC URL\n            table=\"training.people_1m\",   # the name of the table\n            column=\"id\",                  # the name of a column of an integral type that will be used for partitioning\n            lowerBound=1,                 # the minimum value of columnName used to decide partition stride\n            upperBound=1000000,           # the maximum value of columnName used to decide partition stride\n            numPartitions=8,              # the number of partitions/connections\n            properties=connProperties     # the connection properties\n        )\n        .filter(col(\"gender\") == \"M\")   # Filter the data by gender\n       )\n\nppDF.explain()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"38de1b53-866a-4205-aa16-f77b264ac929"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"== Physical Plan ==\n*(1) Scan JDBCRelation(training.people_1m) [numPartitions=8] [id#4149,firstName#4150,middleName#4151,lastName#4152,gender#4153,birthDate#4154,ssn#4155,salary#4156] PushedFilters: [*IsNotNull(gender), *EqualTo(gender,M)], ReadSchema: struct<id:int,firstName:string,middleName:string,lastName:string,gender:string,birthDate:timestam...\n\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["== Physical Plan ==\n*(1) Scan JDBCRelation(training.people_1m) [numPartitions=8] [id#4149,firstName#4150,middleName#4151,lastName#4152,gender#4153,birthDate#4154,ssn#4155,salary#4156] PushedFilters: [*IsNotNull(gender), *EqualTo(gender,M)], ReadSchema: struct<id:int,firstName:string,middleName:string,lastName:string,gender:string,birthDate:timestam...\n\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["Note the lack of a **Filter** and the presence of a **PushedFilters** in the **Scan**. The filter operation is pushed to the database and only the matching records are sent to Spark. This can greatly reduce the amount of data that Spark needs to ingest."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3dda142a-4d35-474b-97c4-1674d3f0d951"}}},{"cell_type":"code","source":[" # Caching the data before filtering eliminates the possibility for the predicate push down.\n\ncachedDF = (spark\n            .read\n            .jdbc(\n                url=jdbcURL,\n                table=\"training.people_1m\",\n                column=\"id\",\n                lowerBound=1,\n                upperBound=1000000,\n                numPartitions=8,\n                properties=connProperties\n            )\n           )\n\ncachedDF.cache()\nfilteredDF = cachedDF.filter(col(\"gender\") == \"M\")\n\nfilteredDF.explain()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e0b1758e-5706-4e52-a3df-a4bc1d6a86ab"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"== Physical Plan ==\n*(1) Filter (isnotnull(gender#4169) AND (gender#4169 = M))\n+- InMemoryTableScan [id#4165, firstName#4166, middleName#4167, lastName#4168, gender#4169, birthDate#4170, ssn#4171, salary#4172], [isnotnull(gender#4169), (gender#4169 = M)]\n      +- InMemoryRelation [id#4165, firstName#4166, middleName#4167, lastName#4168, gender#4169, birthDate#4170, ssn#4171, salary#4172], StorageLevel(disk, memory, deserialized, 1 replicas)\n            +- *(1) Scan JDBCRelation(training.people_1m) [numPartitions=8] [id#4165,firstName#4166,middleName#4167,lastName#4168,gender#4169,birthDate#4170,ssn#4171,salary#4172] PushedFilters: [], ReadSchema: struct<id:int,firstName:string,middleName:string,lastName:string,gender:string,birthDate:timestam...\n\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["== Physical Plan ==\n*(1) Filter (isnotnull(gender#4169) AND (gender#4169 = M))\n+- InMemoryTableScan [id#4165, firstName#4166, middleName#4167, lastName#4168, gender#4169, birthDate#4170, ssn#4171, salary#4172], [isnotnull(gender#4169), (gender#4169 = M)]\n      +- InMemoryRelation [id#4165, firstName#4166, middleName#4167, lastName#4168, gender#4169, birthDate#4170, ssn#4171, salary#4172], StorageLevel(disk, memory, deserialized, 1 replicas)\n            +- *(1) Scan JDBCRelation(training.people_1m) [numPartitions=8] [id#4165,firstName#4166,middleName#4167,lastName#4168,gender#4169,birthDate#4170,ssn#4171,salary#4172] PushedFilters: [], ReadSchema: struct<id:int,firstName:string,middleName:string,lastName:string,gender:string,birthDate:timestam...\n\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["In addition to the **Scan** (the JDBC read) we saw in the previous example, here we also see the **InMemoryTableScan** followed by a **Filter** in the explain plan.\n\nThis means Spark had to read ALL the data from the database and cache it, and then scan it in cache to find the records matching the filter condition."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3cdc7116-1ae2-4bf9-bdfb-3fe065826d87"}}},{"cell_type":"code","source":["cachedDF.unpersist() # For DataFrames there is no method such as uncache()."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e462105a-0285-4cdd-a098-2f3717aa0f0e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[5]: DataFrame[id: int, firstName: string, middleName: string, lastName: string, gender: string, birthDate: timestamp, ssn: string, salary: int]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[5]: DataFrame[id: int, firstName: string, middleName: string, lastName: string, gender: string, birthDate: timestamp, ssn: string, salary: int]"]}}],"execution_count":0},{"cell_type":"markdown","source":["`spark.sql(\"CACHE LAZY TABLE flights_tbl\")` -> Only cache the table when it is first used, instead of immediately."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8907292f-e8eb-4daf-a35b-4a1df346bf53"}}},{"cell_type":"markdown","source":["Uncache a table named MY_TABLE -> `spark.catalog.uncacheTable(\"MY_TABLE\")` For tables there is NOT a method like unpersist().\n\nSpark catalog allows you to cache or uncache tables. You can also do it using spark.sql(\"uncache table table_name\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0e7a8f47-642d-4905-a117-4ca941d5b6a4"}}},{"cell_type":"markdown","source":["##### Partitioning"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"70821d8b-dae9-43a2-b757-7f97465cb262"}}},{"cell_type":"markdown","source":["Partitioning of the DataFrame defines the layout of the DataFrame or Dataset’s physical distribution across the cluster.\n\nThe spark API uses the term core meaning a thread available for parallel execution, we can also refer to it as a slot to avoid confusion with the number of cores in the underliying cpu. It is not necessary an equal number. \n\nIn most cases if you create a cluster you should know how many cores you have. However to check you can use:\n`sc.defaultParallelism`\n`spark.sparkContext.defaultParallelism`\n\nIn local mode you have a number of cores on the local machine. \n\npartitions of data: `df.rdd.getNumPartitions()`\n\nA partition is a small piece of the total dataset"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dcdc878b-a963-4238-af04-7f567e530ad9"}}},{"cell_type":"code","source":["df = spark.read.parquet(\"/mnt/training/ecommerce/events/events.parquet\")\ndf.rdd.getNumPartitions()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3dc6c917-0e36-4d58-9b36-82736b018a71"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[6]: 4","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[6]: 4"]}}],"execution_count":0},{"cell_type":"code","source":["# Access \"SparkContext\" through \"SparkSession\" to get the number of cores or slots.\n# Use the \"defaultParallelism\" attribute to get the number of cores in a cluster.\n\nprint(spark.sparkContext.defaultParallelism)\n\n# print(sc.defaultParallelism) \"SparkContext\" is also provided in Databricks notebooks as the variable \"sc\"."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2b53e0e9-0807-4b37-9f85-993c63d974dd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"8\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["8\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["- **coalesce()** --> returns new DF with exactly N partitions when N < current # partitions (narrow transformation)\nCan only be used to reduce the number of partitions.  It cannot increase the number of partitions. It will not even throw an error but simply ignore the operation.\n- **repartition()** --> returns new DF with exactly N partitions (wide transformation)\nCan be used to reduce or increase the number of partitions. Repartition will incur a full shuffle of the data, regardless of whether one is necessary. This means that you should typically only repartition when the future number of partitions is greater than your current number of partitions or when you are looking to partition by a set of columns:\n`df.repartition(5, col(\"DEST_COUNTRY_NAME\"))`\n\n\n`df.repartition(5, col(\"DEST_COUNTRY_NAME\")).coalesce(2)`\n\nAs an argument you can also have the col name, but it is the second argument after the number of partitions. Example: `transactionsDf.repartition(24, \"itemId\")`\n\nSo if I want to increase the number of partition I have only 1 option (repartition), if I want to reduce I can choose. Coalesce is a narrow transformation and perform better because it avoids shuffle, however it cannot guarantee even distribution of records across the partitions (for example I can end up with only a few partitions with the 80% of the data). It is faster. Repartition will give us a uniform distribution, but it is a wide transformation so we have the added cost of a shuffling operation, means it is slower."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c20142d9-f6b2-4922-a189-99e7fd1460d7"}}},{"cell_type":"markdown","source":["Example: dataframe is very larga and has a large number of partitions, more than there are executors in the cluster. Assuming that there is one core per executor -> performance will be suboptimal because not all executors will be utilized at the same time."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e63f07d5-d23c-48bc-a5d4-e3c18c40be40"}}},{"cell_type":"markdown","source":["Example:\n\nI have a DataFrame (df1) which I want to repartition on the country column and create a new DataFrame (df2) I use -> `df2 = df1.repartition(\"Country\")`\n\nHow many partitions will I have in df2? It depends on the `spark.sql.shuffle.partitions` value. If the value for `spark.sql.shuffle.partitions=200` then your new DataFrame df2 will have 200 partitions.\n\n---\nYou have a DataFrame df. You already know that the DataFrame has got 20 partitions. You want to write this DataFrame as a Parquet file on a given path. How many parquet files will be created after the write operation? 20 parquet files.\n\n\nThe number of parquet files depends on the number of DataFRame partitions."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d77c32f8-1eea-446d-b538-349bbd327df6"}}},{"cell_type":"markdown","source":["**Dynamic partition pruning (DPP)** -> to skip over the data you don’t need in a query’s results.\nThe key optimisation technique in DPP is to take the result of the filter from the dimension table and inject it into the fact table as part of the scan operation to limit the data read. It is enabled by default so that you don’t have to explicitly configure it. All this happens \ndynamically when you perform joins between two tables.\n\nThe typical scenario where DPP is optimal is when you are joining two tables: a fact table (partitioned over multiple columns) and a dimension table (nonpartitioned).\n\n-> Allows you to read only as much data as you need.\n\nIt works based on the PushDownPredicate property. Using this, Spark can read the partitions only that are needed for the processing, rather than processing all the partitions. \nProcessing the whole dataset and applying filtration is dilatory. Rather, pushing down the filter phase, before processing, will reduce the processing overhead.\n\n`spark.sql.optimizer.dynamicPartitionPruning.enabled` -> Spark will generate predicate for partition column when it's used as join key to allow you to read as only required data.\n\nThe key optimization technique in DPP is to take the result of the filter from the dimension table (unpartitioned) and inject it into the fact table (partitioned) as part of the scan operation to limit the data read."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"af16414b-17f4-4672-b4b8-6ea85380027d"}}},{"cell_type":"markdown","source":["repartitionedDF = df.repartition(8)\n\ndf2 = df1.repartition(10, \"Country\")\n\nThe API is df.repartition(numPartitions, Column*). Repartition on a column name will create a hash partitioned DataFrame. The number of the partition depends on the spark.sql.shuffle.partitions value. If the value for spark.sql.shuffle.partitions=200 then your new Dataframe df2 will have 200 partitions.\nHowever, you can override the spark.sql.shuffle.partitions passing the numPartitions in the API call.\n\n`spark.files.maxPartitionBytes` -> The configuration you will change if you want to control the maximum partition size when reading files.\n\n`spark.executor.cores` -> The configuration you will change if you want to control the number of available cores for the executors."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"12098fa5-12f6-4a74-9c66-45163fac7869"}}},{"cell_type":"code","source":["repartitionedDF.rdd.getNumPartitions()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e0d12e1c-b637-4588-8b83-e8e1907cfe0f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[9]: 8","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[9]: 8"]}}],"execution_count":0},{"cell_type":"code","source":["coalesceDF = df.coalesce(8)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dc957fea-47fe-4992-ae24-0759e7a523a4"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["coalesceDF.rdd.getNumPartitions()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7f8d4321-50c4-4e10-ae18-ea996ca2a96d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[11]: 4","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[11]: 4"]}}],"execution_count":0},{"cell_type":"markdown","source":["We always want the number of partitions to be a multiple of the number of slots (cores), that way every slot is used and every slot is assigned a task. \nIs advised that each partition is roughly around 200mb (based on efficiency)\nCSVs are large on disc but small on ram\nParquet are highly compressed but not on ram\n\nExample: I have 10 partitions and 8 slots (cores), is it better to increase it to 16 partitions or decrease to 8? It depends, if I decrese to 8 partitions they should not be more than 200MB, in this case better to choose 16 partitions."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2517e391-fdd5-45fa-ba31-d0a1f4f723f5"}}},{"cell_type":"markdown","source":["##### Maximizing Spark parallelism\n\nMuch of Spark’s efficiency is due to its ability to run multiple tasks in parallel at scale.\n\nIn data management parlance, a partition is a way to arrange data into a subset of configurable and readable chunks or blocks of contiguous data on disk. These subsets of data can be read or processed independently and in parallel, if necessary, by more than a single thread in a process.\n\nSpark is embarrassingly efficient at processing its tasks in parallel. For large-scale workloads a Spark job will have many stages, and within each stage there will be many tasks. \n\nTo optimize resource utilization and maximize parallelism, the ideal is at least as many partitions as there are cores on the executor. If there are more partitions than there are cores on each executor, all the cores are kept busy. You can think of partitions as atomic units of parallelism: a single thread running on a single core can work on a single partition.\n\nThe size of a partition in Spark is dictated by `spark.sql.files.maxPartitionBytes`. The default is 128 MB."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"17376192-832c-4c8b-9963-b1d356a9d0a3"}}},{"cell_type":"markdown","source":["Shuffle partitions are created during the shuffle stage. By default, the number of shuffle partitions is set to 200 in spark.sql.shuffle.partitions. You can adjust this number depending on the size of the data set you have, to reduce the amount of small partitions being sent across the network to executors’ tasks.\n\nThe default value for spark.sql.shuffle.partitions is too high for smaller or streaming workloads; you may want to reduce it to a lower value such as the number of cores on the executors or less. \n\nThere is no magic formula for the number of shuffle partitions to set for the shuffle stage; the number may vary depending on your use case, data set, number of cores, and the amount of executor memory available. It's a trial and error approach.\n\nCreated during operations like groupBy() or join(), also known as wide transformations, shuffle partitions consume both network and disk I/O resources.\n\nWide transformations have to shuffle data, once the data is shuffled it has to be repartitioned.\nUnlike repartition and coalesce we do not specify how many partitions to use.\nThe problem is the number of partition we eneded up with. \n\nSpark engineers decided the number 200 for the new partition size. However we can change it, see below.\n\ndeafult shuffle partitions:\n`spark.conf.get(\"spark.sql.shuffle.partitions\")`\n`spark.conf.get(\"spark.sql.shuffle.partitions\", \"8\")`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"332ae7b7-2543-44d5-be04-278a2fa41f81"}}},{"cell_type":"code","source":["spark.conf.get(\"spark.sql.shuffle.partitions\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ead091e6-c486-4320-99b8-345174d5b318"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[12]: '200'","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[12]: '200'"]}}],"execution_count":0},{"cell_type":"code","source":["# Assuming that the data set isn't too large, you could configure the default number of shuffle partitions to match the number of cores:\n\nspark.conf.set(\"spark.sql.shuffle.partitions\", spark.sparkContext.defaultParallelism)\nprint(spark.conf.get(\"spark.sql.shuffle.partitions\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"712e8d1d-ca24-4083-96e1-88537a198cf0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"8\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["8\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["* Don't allow partition size to increase > 200mb per 8gb of core total memory (For small data 3 partitions per core)\n\n* Always err on the side of too many small than too few large partitions\n\n* Size default shuffle partitions by dividing largest shuffle stage input by the target partition size\nexample: 4TB/200MB = 20000 shuffle partition count\n\nShuffle is very expensive. Best number of partitions dependes on data. If there are too many small partitions it can also slow down the query. Important to have the right number of partitions. \n\n When writing a DataFrame to storage, the number of DataFrame partitions determines the number of data files written."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"77af4353-0098-4632-a3f0-ef44a237c757"}}},{"cell_type":"markdown","source":["##### Bucketing\n\nBucketing is another file organization approach with which you can control the data that is specifically written to each file. This can help avoid shuffles later when you go to read the data because data with the same bucket ID will all be grouped together into one physical partition. This means that the data is prepartitioned according to how you expect to use that data later on, meaning you can avoid expensive shuffles when joining or aggregating.\nRather than partitioning on a specific column (which might write out a ton of directories), it’s probably worthwhile to explore bucketing the data instead. This will create a certain number of files and organize our data into those “buckets”.\n\nExample with scala:\n\n`val numberBuckets = 10`\n\n`val columnToBucketBy = \"count\"`\n\n`csvFile.write.format(\"parquet\").mode(\"overwrite\")\n.bucketBy(numberBuckets, columnToBucketBy).saveAsTable(\"bucketedFiles\")\n`\n\nWe can eliminate the Exchange (shuffle) step from the Shuffle Sort Join scheme if we create partitioned buckets columns on which we want to perform frequent equi-joins. Presorting and reorganizing data in this way boosts performance, as it allows us to skip the expensive Exchange operation and go straight to WholeStageCodegen."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4a81299e-651e-470e-9ded-d9bd9b0c8b2a"}}},{"cell_type":"markdown","source":["Adaptive Query Execution (AQE) is now able to dynamically coalesce shuffle partitions</a> at runtime. This means that you can set `spark.sql.shuffle.partitions` based on the largest data set your application processes and allow AQE to reduce the number of partitions automatically when there is less data to process.\n\nThe `spark.sql.adaptive.enabled` configuration option controls whether AQE is turned on/off.\n\nOne of the most important questions for Adaptive Query Execution is when to reoptimize. Spark operators are often pipelined and executed in parallel processes. However, a shuffle or broadcast exchange breaks this pipeline. We call them materialization points and use the term “query stages” to denote subsections bounded by these materialization points in a query. Each query stage materializes its intermediate result and the following stage can only proceed if all the parallel processes running the materialization have completed. This provides a natural opportunity for reoptimization, for it is when data statistics on all partitions are available and successive operations have not started yet.\n\nWhen the query starts, the Adaptive Query Execution framework first kicks off all the leaf stages — the stages that do not depend on any other stages. As soon as one or more of these stages finish materialization, the framework marks them complete in the physical query plan and updates the logical query plan accordingly, with the runtime statistics retrieved from completed stages. Based on these new statistics, the framework then runs the optimizer (with a selected list of logical optimization rules), the physical planner, as well as the physical optimization rules, which include the regular physical rules and the adaptive-execution-specific rules, such as coalescing partitions, skew join handling, etc. Now that we’ve got a newly optimized query plan with some completed stages, the adaptive execution framework will search for and execute new query stages whose child stages have all been materialized, and repeat the above execute-reoptimize-execute process until the entire query is done.\n\nIn Spark 3.0, the AQE framework is shipped with three features:\n\n* Dynamically coalescing shuffle partitions\n* Dynamically switching join strategies\n* Dynamically optimizing skew joins"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d3b75006-0e61-489a-8352-07a28b9813a8"}}},{"cell_type":"code","source":["spark.conf.get(\"spark.sql.adaptive.enabled\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ce55b4cf-2b1f-4691-882b-e0c721548d7c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[14]: 'true'","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[14]: 'true'"]}}],"execution_count":0},{"cell_type":"markdown","source":["##### Out-of-Memory errors\n\nAn out-of-memory error occurs when either the driver or an executor does not have enough memory to collect or process the data allocated to it.\n\nThe OutOfMemory Exception can occur at the Driver or Executor level.\n\n* Driver is a Java process where the main() method of our Java/Scala/Python program runs. It executes the code and creates a SparkSession/ SparkContext which is responsible to create Data Frame, Dataset, RDD to execute SQL, perform Transformation & Action, etc.\n* Executors are launched at the start of a Spark Application with the help of Cluster Manager. These can be dynamically launched and removed by the Driver as and when required. It runs an individual task and returns the result to the Driver. It can also persist data in the worker nodes for re-usability.\n\n* OutOfMemory at the Driver Level:\nOutOfMemory error can occur here due to incorrect usage of Spark. The driver in the Spark architecture is only supposed to be an orchestrator and is therefore provided less memory than the executors. You should always be aware of what operations or tasks are loaded to your driver. Few unconscious operations which we might have performed could also be the cause of error: `Collect()`\n* OutOfMemory at the Executor Level:\nThere are a few common reasons also that would cause this failure:\n* Inefficient queries\n* High concurrency\n* Incorrect configuration"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0737b810-d698-434f-ab19-26dca38e6912"}}},{"cell_type":"markdown","source":["##### Garbage collection tuning\n\nJVM garbage collection can be a problem when you have large “churn” in terms of the RDDs stored by your program. (It is usually not a problem in programs that just read an RDD once and then run many operations on it.) When Java needs to evict old objects to make room for new ones, it will need to trace through all your Java objects and find the unused ones. The main point to remember here is that the cost of garbage collection is proportional to the number of Java objects, so using data structures with fewer objects (e.g. an array of Ints instead of a LinkedList) greatly lowers this cost. An even better method is to persist objects in serialized form, as described above: now there will be only one object (a byte array) per RDD partition. Before trying other techniques, the first thing to try if GC is a problem is to use serialized caching.\n\nGC can also be a problem due to interference between your tasks’ working memory (the amount of space needed to run the task) and the RDDs cached on your nodes. We will discuss how to control the space allocated to the RDD cache to mitigate this.\n\n-> A simplified description of the garbage collection procedure: When Eden is full, a minor GC is run on Eden and objects that are alive from Eden and Survivor1 are copied to Survivor2. The Survivor regions are swapped. If an object is old enough or Survivor2 is full, it is moved to Old. Finally, when Old is close to full, a full GC is invoked.\n\nPossible strategies in order to decrease garbage collection time:\n- Use structured APIs and create fewer objects\n- Increase the Java Heap Size\n\nYou gather statistics on how frequently garbage collection occurs and the amount of time it takes by adding -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps to the Java options. You can find the GC statistics -> check in the worker's log file for garbage collection details."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d1056c51-3f24-4bd1-b642-a035de5276c3"}}},{"cell_type":"markdown","source":["Which property is used to set FAIR scheduler so you can allocate jobs to different resource pools? -> `spark.scheduler.mode`\n\nThe use of the `spark.scheduler.allocation.file` configuration is to create and configure FAIR schedular pools."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e09941aa-ee5d-4f08-95aa-0c2215014f8e"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"SparkCertification-SparkInternals","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":2582948588281464}},"nbformat":4,"nbformat_minor":0}
