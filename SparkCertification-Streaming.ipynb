{"cells":[{"cell_type":"markdown","source":["#### Structured Streaming"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5ae462fd-fac5-4139-9484-94ef8df7bec5"}}},{"cell_type":"markdown","source":["With Structured Streaming, you can take the same operations that you perform in batch mode using Sparkâ€™s structured APIs and run them in a streaming fashion. This can reduce latency and allow for incremental processing.\n\n\n* Stream processing is the act of continuously incorporating new data to compute the result. It is unbounded. We will have multiple versions of the result.\n* Batch processing only computes the result once. Fixed input dataset.\n\nStream and batch processing often need to work together.\n\nStream processing use cases: notifications, real-time reporting, incremental ETL, real-time decision making, online ML, update data to serve in real-time.\n\nAdvantages of stream processing: lower latency, efficient updates, automatic bookkeeping on new data.\n\nIs it expensive? Not necessarly, databricks allow you to schedule a stream to process only the currently available data."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a443d262-4337-4118-a1ca-cef1199e9065"}}},{"cell_type":"markdown","source":["##### Micro-batch processing\n\nThe data from a source (example kafka) is coming in faster than it can be consumed. We solve this problem with the micro-batch model. -> We collect data for a set interval of time: the trigger interval. \nThere are 2 models for strea processing systems: **Continuous or micro-batch processing**. \nIn the continuous model each node in the system continuously listen to messages from other nodes and outputing new updates to the external nodes. \nMicro batch processing wait to accumulate more batches of data. Then process them in parallel.\nWhen you have to decide between these 2 modes you have to consider your desired latency and the total cost of the operation (TCO).\nMicro-batch systems can deliver latency from 1 ms to 1 second depending on the application. \nWith micro-batches new rows are appended to unbounded table."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"87e900b1-f28e-4e16-8d4b-0766db8bb428"}}},{"cell_type":"markdown","source":["input sources: kafka, event hubs, files\nfor testing: sockets and generator\n`spark.readStream <insert input configuration>`\n\nConfigure data stream writer: \n`spark.readStream <insert input configuration>\n.filter(col(\"event_name\") == \"finalize\")\n.groupBy(\"traffic_source\").count()\n.writeStream\n<insert sink configurations>`\n\nOutput skinks: kafka, event hubs, files, foreach\nfor debugging: console, memory\n\nSINKS specify the destination of the result set of a stream. \n\nFile -> Structured Streaming sink type that is idempotent and can provide end-to-end exactly-once semantics in a Structured Streaming job."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"529c29f7-fe63-4b1b-9db6-6c32908f8f92"}}},{"cell_type":"markdown","source":["**Output modes**:\n* append: add new records only.\n* update: update changed records.\n* complete: rewrite full output."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"34b2d5d0-29cf-4259-a3cb-9090a7ecf39e"}}},{"cell_type":"markdown","source":["**Trigger types**:\n\n* Default: process each micro-batch as soon as the previous one has been processed.\n* Fixed interval: micro batch processing kicked off at the user-specified interval.\n* One-time: process all of the available data as a single micro-batch and then automatically stop the query.\n* Continuous processing: long running tasks that continously read, process, and write data as soon as events are available."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9a7371c7-7900-4eb9-964e-66c25bd71159"}}},{"cell_type":"markdown","source":["Fault tolerance: guaranteed by checkpointing and write-ahead logs, idempotent sinks, replayable data source\n\nSome streaming query operations: stop stream, await termination, status, is active, recent progress, name, ID, runID"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"18700c4b-d3fb-4590-8f83-6455c0b60469"}}},{"cell_type":"code","source":["# Obtain an initial streaming DataFrame from a Parquet-format file source.\n\nschema = \"device STRING, ecommerce STRUCT<purchase_revenue_in_usd: DOUBLE, total_item_quantity: BIGINT, unique_items: BIGINT>, event_name STRING, event_previous_timestamp BIGINT, event_timestamp BIGINT, geo STRUCT<city: STRING, state: STRING>, items ARRAY<STRUCT<coupon: STRING, item_id: STRING, item_name: STRING, item_revenue_in_usd: DOUBLE, price_in_usd: DOUBLE, quantity: BIGINT>>, traffic_source STRING, user_first_touch_timestamp BIGINT, user_id STRING\"\n\ndf = (spark\n      .readStream\n      .schema(schema)\n      .option(\"maxFilesPerTrigger\", 1)\n      .parquet(\"/mnt/training/ecommerce/events/events.parquet\")\n     )\n\ndf.isStreaming"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"017cb1f5-e664-4262-9f18-ff16b544ebd8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[2]: True","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[2]: True"]}}],"execution_count":0},{"cell_type":"code","source":["# Apply some transformations, producing new streaming DataFrames.\n\nfrom pyspark.sql.functions import col, approx_count_distinct, count\n\nemailTrafficDF = (df\n                  .filter(col(\"traffic_source\") == \"email\")\n                  .withColumn(\"mobile\", col(\"device\").isin([\"iOS\", \"Android\"]))\n                  .select(\"user_id\", \"event_timestamp\", \"mobile\")\n                 )\n\nemailTrafficDF.isStreaming"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"021761f8-cb9b-4833-9f53-5fda13fd4b6e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[3]: True","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[3]: True"]}}],"execution_count":0},{"cell_type":"code","source":["# Take the final streaming DataFrame (our result table) and write it to a file sink in \"append\" mode.\n\ncheckpointPath = userhome + \"/email_traffic/checkpoint\"\noutputPath = userhome + \"/email_traffic/output\"\n\ndevicesQuery = (emailTrafficDF\n                .writeStream\n                .outputMode(\"append\")\n                .format(\"parquet\")\n                .queryName(\"email_traffic\")\n                .trigger(processingTime=\"1 second\")\n                .option(\"checkpointLocation\", checkpointPath)\n                .start(outputPath)\n               )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1b8fadaf-dad7-41ce-9713-deaef5170be8"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##### Monitor streaming query \n\n-> Use the streaming query \"handle\" to monitor and control it."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f6b2994f-6af9-40ae-b3b5-38c3e181e845"}}},{"cell_type":"code","source":["devicesQuery.id"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"907cc658-8b7b-4d22-b31b-2b35c692abc7"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["devicesQuery.status"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f3cf29ce-e6db-46fb-adf1-f5f9243b7f8a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["devicesQuery.lastProgress"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"91583497-3995-4540-8464-56f93693ad46"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["devicesQuery.awaitTermination(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0e693929-635a-44c9-af05-ec03a22bce16"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["devicesQuery.stop()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eb846836-c711-4450-b4c6-76657a408fe4"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"SparkCertification-Streaming","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":3285350366510574}},"nbformat":4,"nbformat_minor":0}
