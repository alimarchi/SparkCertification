{"cells":[{"cell_type":"markdown","source":["#### Transformations"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d356315a-22cb-4e6e-be88-396adfc36f1a"}}},{"cell_type":"markdown","source":["###### Aggregations \nGroup data by specified columns.\n\nUse the DataFrame `groupBy` method to create a grouped data object. \n\nThis grouped data object is called `RelationalGroupedDataset` in Scala and `GroupedData` in Python."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3cca8669-bb19-405a-97d1-27f3815a4d8d"}}},{"cell_type":"markdown","source":["##### Grouped data methods\n| Method | Description |\n| --- | --- |\n| agg | Compute aggregates by specifying a series of aggregate columns |\n| avg | Compute the mean value for each numeric columns for each group |\n| count | Count the number of rows for each group |\n| max | Compute the max value for each numeric columns for each group |\n| mean | Compute the average value for each numeric columns for each group |\n| min | Compute the min value for each numeric column for each group |\n| pivot | Pivots a column of the current DataFrame and performs the specified aggregation |\n| sum | Compute the sum for each numeric columns for each group |"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1f1547d1-3967-4f70-a5df-18a37effda39"}}},{"cell_type":"markdown","source":["`df.groupBy(\"Year\").pivot(\"CourseName\").agg(expr(\"sum(Students)\"))`\n\nWe need one row per year so you will group by the year. We need Course Names to be presented as columns so you will Pivot on the columns. The computation is adding up students for a year and course name so you will use sum() in your aggregation."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1ba26455-66b1-4291-8477-357e962f84bb"}}},{"cell_type":"markdown","source":["countDistinct()\ndf.groupBy(“InvoiceNo”).agg(expr(“countDistinct(Quantity)”))\nRemember that a function can be passed as an expression within agg(). This makes it possible to pass arbitrary expressions that just need to have some aggregation specified.\n(df.groupBy(\"InvoiceNo\")\n   .agg(count(\"Quantity\").alias(\"quan1\") #method1\n   ,expr(\"count(Quantity) as quan2\")) #method2\n   .show())\n   \n   \nYou can use the countDistinct() for counting unique records. But remember, this function is not available in Spark SQL.\n\n`df.selectExpr(\"count(distinct InvoiceNo)\")`\n\n`df.select(countDistinct(\"InvoiceNo\"))`\n\n`df.select(\"InvoiceNo\").distinct().agg(count(\"InvoiceNo\"))`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1030378d-f63b-411c-851c-c1fcfa6c6324"}}},{"cell_type":"markdown","source":["##### Built-In Functions\nIn addition to DataFrame and Column transformation methods, there are a ton of helpful functions in Spark's built-in SQL functions module.\n\nIn Scala, this is <a href=\"https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/functions$.html\" target=\"_bank\">`org.apache.spark.sql.functions`</a>, and <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#functions\" target=\"_blank\">`pyspark.sql.functions`</a> in Python. Functions from this module must be imported into your code."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"313c4e6f-ff9a-46f2-afad-d2164a99f903"}}},{"cell_type":"markdown","source":["##### Aggregate Functions\n\nHere are some of the built-in functions available for aggregation.\n\n| Method | Description |\n| --- | --- |\n| approx_count_distinct | Returns the approximate number of distinct items in a group |\n| avg | Returns the average of the values in a group |\n| collect_list | Returns a list of objects with duplicates |\n| corr | Returns the Pearson Correlation Coefficient for two columns |\n| max | Compute the max value for each numeric columns for each group |\n| mean | Compute the average value for each numeric columns for each group |\n| stddev_samp | Returns the sample standard deviation of the expression in a group |\n| sumDistinct | Returns the sum of distinct values in the expression |\n| var_pop | Returns the population variance of the values in a group |\n\nUse the grouped data method <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.GroupedData.agg.html#pyspark.sql.GroupedData.agg\" target=\"_blank\">`agg`</a> to apply built-in aggregate functions\n\nThis allows you to apply other transformations on the resulting columns, such as <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.Column.alias.html\" target=\"_blank\">`alias`</a>.\n\nRemember that a function can be passed as an expression within agg(). This makes it possible to pass arbitrary expressions that just need to have some aggregation specified. You can even alias a column after transforming it for later use in your data flow.\n\n**countDistinct** Sometimes, the total number is not relevant; rather, it’s the number of unique groups that you want. To get this number, you can use the countDistinct function.\n`df.select(countDistinct(\"StockCode\")).show()`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cc64378b-32f2-41a3-9718-7c2affcf70bb"}}},{"cell_type":"markdown","source":["**Grouping with expressions**\n\nRather than passing that function as an expression into a select statement, we specify it as within agg. This makes it possible for you to pass-in arbitrary expressions that just need to have some aggregation specified. You can even do things like alias a column after transforming it for later use in your data flow.\n\n`df.groupBy(\"InvoiceNo\").agg(\ncount(\"Quantity\").alias(\"quan\"),\nexpr(\"count(Quantity)\")).show()`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a29761f6-c4c8-4600-8a66-1b43a7661824"}}},{"cell_type":"markdown","source":["Variance and standard deviation. These are both measures of the spread of the data around the mean. The variance is the average of the squared differences from the mean, and the standard deviation is the square root of the variance.\n\n**mean()** Alias for Avg. Returns the average of the values in a column.\n\n**variance()** \talias for `var_samp`. \n\n**var_samp** function returns the unbiased variance of the values in a column.\n\n**std_dev()** stddev is an aggregation function used to return the standard deviation (A standard deviation is a measure of how dispersed the data is in relation to the mean).\n\n`df.select(var_pop(\"Quantity\"), var_samp(\"Quantity\"),\nstddev_pop(\"Quantity\"), stddev_samp(\"Quantity\")).show()`\n\n**corr()** To compute the correlation of two columns. For example, we can see the Pearson correlation coefficient for two columns to see if cheaper things are typically bought\nin greater quantities (The Pearson correlation measures the strength of the linear relationship between two variables).\n\n`from pyspark.sql.functions import corr`\n\n`df.stat.corr(\"Quantity\", \"UnitPrice\")`\n\n`df.select(corr(\"Quantity\", \"UnitPrice\")).show()\n`\n\nThe **covariance** is scaled according to the inputs in the data. Like the var function, covariance can be calculated either as the sample covariance or the\npopulation covariance.\n`df.select(corr(\"InvoiceNo\", \"Quantity\"), covar_samp(\"InvoiceNo\", \"Quantity\"),\ncovar_pop(\"InvoiceNo\", \"Quantity\")).show()`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5fe918b1-b199-4922-8b33-b89d4afbe710"}}},{"cell_type":"markdown","source":["##### Window functions\n\nWe can use window functions to carry out some unique aggregations by either computing some aggregation on a specific “window” of data, which you define by using a reference to the current data. A group-by takes data, and every row can go only into one grouping. A window function calculates a return value for every input row of a table based on a group of rows, called a frame.\nSpark supports three kinds of window functions: \n* Ranking functions. \n* Analytic functions.\n* Aggregate functions.\nThe first step to a window function is to create a window specification. Note that the partition by is unrelated to the partitioning scheme concept that we have covered thus far. It’s just a similar concept that describes how we will be breaking up our group."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"14f11d9d-2120-4c5b-b4e9-030e098300e7"}}},{"cell_type":"code","source":["from pyspark.sql.window import Window\nfrom pyspark.sql.functions import desc\n\nwindowSpec = Window\\\n    .partitionBy(\"CustomerId\", \"date\")\\\n    .orderBy(desc(\"Quantity\"))\\\n    .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n\nfrom pyspark.sql.functions import max\n\nmaxPurchaseQuantity = max(col(\"Quantity\")).over(windowSpec)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"293b396f-6778-4980-8035-e116a1099bce"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##### Math Functions\nHere are some of the built-in functions for math operations.\n\n| Method | Description |\n| --- | --- |\n| ceil | Computes the ceiling of the given column. |\n| cos | Computes the cosine of the given value. |\n| log | Computes the natural logarithm of the given value. |\n| round | Returns the value of the column e rounded to 0 decimal places with HALF_UP round mode. |\n| sqrt | Computes the square root of the specified float value. |"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4659f734-2444-4ec6-8dca-5e17d051379d"}}},{"cell_type":"markdown","source":["By default, the **round** function rounds up if you’re exactly in between two numbers. You can round down by using the **bround**.\n\nExample: round(2.5, 0) -> 3.0 or bround(2.5, 0) -> 2.0\n\n**pow()** Returns the value of the first argument raised to the power of the second argument.\n\n`transactionsDF.withColumn(\"predErrorSq\", pow(col(\"predError\"),lit(2)))`\n\nThe following code blocks would also work:\n\n`transactionsDf.withColumn(\"predErrorSquared\", pow(\"predError\", 2))`\n\n`transactionsDf.withColumn(\"predErrorSquared\", pow(\"predError\", lit(2)))`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5c110137-c196-4d9d-8b00-f484b63ebdf5"}}},{"cell_type":"markdown","source":["##### Datetimes"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b063cf9e-b1a9-447b-8942-1a7f50e389c0"}}},{"cell_type":"markdown","source":["##### Built-In Functions: Date Time Functions\nHere are a few built-in functions to manipulate dates and times in Spark.\n\n| Method | Description |\n| --- | --- |\n| add_months | Returns the date that is numMonths after startDate |\n| current_timestamp | Returns the current timestamp at the start of query evaluation as a timestamp column |\n| date_format | Converts a date/timestamp/string to a value of string in the format specified by the date format given by the second argument. |\n| dayofweek | Extracts the day of the month as an integer from a given date/timestamp/string |\n| from_unixtime | Converts the number of seconds from unix epoch (1970-01-01 00:00:00 UTC) to a string representing the timestamp of that moment in the current system time zone in the yyyy-MM-dd HH:mm:ss format |\n| minute | Extracts the minutes as an integer from a given date/timestamp/string. |\n| unix_timestamp | Converts time string with given pattern to Unix timestamp (in seconds) |"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2551bf3f-6958-4e93-bcd3-c28042f69346"}}},{"cell_type":"markdown","source":["unixtimestamp() does not include a timezone argument as it is meant to use the default one."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"769ee801-3c4e-4595-9af5-fb20f8bab744"}}},{"cell_type":"code","source":["from pyspark.sql.functions import col\n\ndf = spark.read.parquet(\"/mnt/training/ecommerce/events/events.parquet\").select(\"user_id\", col(\"event_timestamp\").alias(\"timestamp\"))\n\ndf.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5da0ab23-428d-48df-8f22-e7bf2f1908df"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----------------+----------------+\n|          user_id|       timestamp|\n+-----------------+----------------+\n|UA000000107379500|1593878946592107|\n|UA000000107359357|1593877011756535|\n|UA000000107375547|1593878815459100|\n|UA000000107370581|1593878809276923|\n|UA000000107377108|1593878628143633|\n+-----------------+----------------+\nonly showing top 5 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----------------+----------------+\n|          user_id|       timestamp|\n+-----------------+----------------+\n|UA000000107379500|1593878946592107|\n|UA000000107359357|1593877011756535|\n|UA000000107375547|1593878815459100|\n|UA000000107370581|1593878809276923|\n|UA000000107377108|1593878628143633|\n+-----------------+----------------+\nonly showing top 5 rows\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# \"cast()\"\n# Casts column to a different data type, specified using string representation or DataType.\n\ntimestampDF = df.withColumn(\"timestamp\", (col(\"timestamp\") / 1e6).cast(\"timestamp\"))\n\ntimestampDF.show(5, False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"70714021-2742-4023-a1e9-23d87831816e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----------------+--------------------------+\n|user_id          |timestamp                 |\n+-----------------+--------------------------+\n|UA000000107379500|2020-07-04 16:09:06.592107|\n|UA000000107359357|2020-07-04 15:36:51.756535|\n|UA000000107375547|2020-07-04 16:06:55.4591  |\n|UA000000107370581|2020-07-04 16:06:49.276923|\n|UA000000107377108|2020-07-04 16:03:48.143633|\n+-----------------+--------------------------+\nonly showing top 5 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----------------+--------------------------+\n|user_id          |timestamp                 |\n+-----------------+--------------------------+\n|UA000000107379500|2020-07-04 16:09:06.592107|\n|UA000000107359357|2020-07-04 15:36:51.756535|\n|UA000000107375547|2020-07-04 16:06:55.4591  |\n|UA000000107370581|2020-07-04 16:06:49.276923|\n|UA000000107377108|2020-07-04 16:03:48.143633|\n+-----------------+--------------------------+\nonly showing top 5 rows\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.types import TimestampType\n\ntimestampDF = df.withColumn(\"timestamp\", (col(\"timestamp\") / 1e6).cast(TimestampType()))\ntimestampDF.show(5, False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5ab4bf7c-ab8f-4dda-a3f9-4f47c8c334b5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----------------+--------------------------+\n|user_id          |timestamp                 |\n+-----------------+--------------------------+\n|UA000000107379500|2020-07-04 16:09:06.592107|\n|UA000000107359357|2020-07-04 15:36:51.756535|\n|UA000000107375547|2020-07-04 16:06:55.4591  |\n|UA000000107370581|2020-07-04 16:06:49.276923|\n|UA000000107377108|2020-07-04 16:03:48.143633|\n+-----------------+--------------------------+\nonly showing top 5 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----------------+--------------------------+\n|user_id          |timestamp                 |\n+-----------------+--------------------------+\n|UA000000107379500|2020-07-04 16:09:06.592107|\n|UA000000107359357|2020-07-04 15:36:51.756535|\n|UA000000107375547|2020-07-04 16:06:55.4591  |\n|UA000000107370581|2020-07-04 16:06:49.276923|\n|UA000000107377108|2020-07-04 16:03:48.143633|\n+-----------------+--------------------------+\nonly showing top 5 rows\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##### Datetime Patterns for Formatting and Parsing\nThere are several common scenarios for datetime usage in Spark:\n\n- CSV/JSON datasources use the pattern string for parsing and formatting datetime content.\n- Datetime functions related to convert StringType to/from DateType or TimestampType e.g. `unix_timestamp`, `date_format`, `from_unixtime`, `to_date`, `to_timestamp`, etc.\n\nSpark uses pattern letters for date and timestamp parsing and formatting. A subset of these patterns are shown below.\n\n| Symbol | Meaning         | Presentation | Examples               |\n| ------ | --------------- | ------------ | ---------------------- |\n| G      | era             | text         | AD; Anno Domini        |\n| y      | year            | year         | 2020; 20               |\n| D      | day-of-year     | number(3)    | 189                    |\n| M/L    | month-of-year   | month        | 7; 07; Jul; July       |\n| d      | day-of-month    | number(3)    | 28                     |\n| Q/q    | quarter-of-year | number/text  | 3; 03; Q3; 3rd quarter |\n| E      | day-of-week     | text         | Tue; Tuesday           |"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"89fe3182-736a-4a06-8357-11073dce04fb"}}},{"cell_type":"markdown","source":["The **to_date** function allows you to convert a string to a date, optionally with a specified format. We specify our format in the Java SimpleDateFormat.\n\n`spark.range(5).withColumn(\"date\", lit(\"2017-01-01\"))\\\n.select(to_date(col(\"date\"))).show(1)`\n\nSpark will not throw an error if it cannot parse the date; rather, it will just return null. \n\n**to_timestamp** always requires a format to be specified.\n`\ncleanDateDF.select(to_timestamp(col(\"date\"), dateFormat)).show()\n`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eda7dc68-4f84-44ed-bef3-ce152edf6915"}}},{"cell_type":"code","source":["# \"date_format()\"\n# Converts a date/timestamp/string to a string formatted with the given date time pattern.\n\nfrom pyspark.sql.functions import date_format\n\nformattedDF = (timestampDF\n               .withColumn(\"date string\", date_format(\"timestamp\", \"MMMM dd, yyyy\"))\n               .withColumn(\"time string\", date_format(\"timestamp\", \"HH:mm:ss.SSSSSS\"))\n              )\nformattedDF.show(5, False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"70fcb558-67df-4f3e-9cd6-a5802abef0a0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----------------+--------------------------+-------------+---------------+\n|user_id          |timestamp                 |date string  |time string    |\n+-----------------+--------------------------+-------------+---------------+\n|UA000000107379500|2020-07-04 16:09:06.592107|July 04, 2020|16:09:06.592107|\n|UA000000107359357|2020-07-04 15:36:51.756535|July 04, 2020|15:36:51.756535|\n|UA000000107375547|2020-07-04 16:06:55.4591  |July 04, 2020|16:06:55.459100|\n|UA000000107370581|2020-07-04 16:06:49.276923|July 04, 2020|16:06:49.276923|\n|UA000000107377108|2020-07-04 16:03:48.143633|July 04, 2020|16:03:48.143633|\n+-----------------+--------------------------+-------------+---------------+\nonly showing top 5 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----------------+--------------------------+-------------+---------------+\n|user_id          |timestamp                 |date string  |time string    |\n+-----------------+--------------------------+-------------+---------------+\n|UA000000107379500|2020-07-04 16:09:06.592107|July 04, 2020|16:09:06.592107|\n|UA000000107359357|2020-07-04 15:36:51.756535|July 04, 2020|15:36:51.756535|\n|UA000000107375547|2020-07-04 16:06:55.4591  |July 04, 2020|16:06:55.459100|\n|UA000000107370581|2020-07-04 16:06:49.276923|July 04, 2020|16:06:49.276923|\n|UA000000107377108|2020-07-04 16:03:48.143633|July 04, 2020|16:03:48.143633|\n+-----------------+--------------------------+-------------+---------------+\nonly showing top 5 rows\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# Extract datetime attribute from timestamp\n# Example: YEAR -> Extracts the year as an integer from a given date/timestamp/string.\n# Similar methods: month, dayofweek, minute, second, etc.\n\nfrom pyspark.sql.functions import year, month, dayofweek, minute, second\n\ndatetimeDF = (timestampDF\n              .withColumn(\"year\", year(col(\"timestamp\")))\n              .withColumn(\"month\", month(col(\"timestamp\")))\n              .withColumn(\"dayofweek\", dayofweek(col(\"timestamp\")))\n              .withColumn(\"minute\", minute(col(\"timestamp\")))\n              .withColumn(\"second\", second(col(\"timestamp\")))\n             )\ndatetimeDF.show(5, False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ddca43ad-2af8-43f7-bd2f-323567fc2ade"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----------------+--------------------------+----+-----+---------+------+------+\n|user_id          |timestamp                 |year|month|dayofweek|minute|second|\n+-----------------+--------------------------+----+-----+---------+------+------+\n|UA000000107379500|2020-07-04 16:09:06.592107|2020|7    |7        |9     |6     |\n|UA000000107359357|2020-07-04 15:36:51.756535|2020|7    |7        |36    |51    |\n|UA000000107375547|2020-07-04 16:06:55.4591  |2020|7    |7        |6     |55    |\n|UA000000107370581|2020-07-04 16:06:49.276923|2020|7    |7        |6     |49    |\n|UA000000107377108|2020-07-04 16:03:48.143633|2020|7    |7        |3     |48    |\n+-----------------+--------------------------+----+-----+---------+------+------+\nonly showing top 5 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----------------+--------------------------+----+-----+---------+------+------+\n|user_id          |timestamp                 |year|month|dayofweek|minute|second|\n+-----------------+--------------------------+----+-----+---------+------+------+\n|UA000000107379500|2020-07-04 16:09:06.592107|2020|7    |7        |9     |6     |\n|UA000000107359357|2020-07-04 15:36:51.756535|2020|7    |7        |36    |51    |\n|UA000000107375547|2020-07-04 16:06:55.4591  |2020|7    |7        |6     |55    |\n|UA000000107370581|2020-07-04 16:06:49.276923|2020|7    |7        |6     |49    |\n|UA000000107377108|2020-07-04 16:03:48.143633|2020|7    |7        |3     |48    |\n+-----------------+--------------------------+----+-----+---------+------+------+\nonly showing top 5 rows\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# \"to_date\" -> Converts the column into DateType by casting rules to DateType.\n\nfrom pyspark.sql.functions import to_date\n\ndateDF = timestampDF.withColumn(\"date\", to_date(col(\"timestamp\")))\ndateDF.show(5, False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e2eaa4f6-14f3-40f5-a0f5-95bfb28f8513"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----------------+--------------------------+----------+\n|user_id          |timestamp                 |date      |\n+-----------------+--------------------------+----------+\n|UA000000107379500|2020-07-04 16:09:06.592107|2020-07-04|\n|UA000000107359357|2020-07-04 15:36:51.756535|2020-07-04|\n|UA000000107375547|2020-07-04 16:06:55.4591  |2020-07-04|\n|UA000000107370581|2020-07-04 16:06:49.276923|2020-07-04|\n|UA000000107377108|2020-07-04 16:03:48.143633|2020-07-04|\n+-----------------+--------------------------+----------+\nonly showing top 5 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----------------+--------------------------+----------+\n|user_id          |timestamp                 |date      |\n+-----------------+--------------------------+----------+\n|UA000000107379500|2020-07-04 16:09:06.592107|2020-07-04|\n|UA000000107359357|2020-07-04 15:36:51.756535|2020-07-04|\n|UA000000107375547|2020-07-04 16:06:55.4591  |2020-07-04|\n|UA000000107370581|2020-07-04 16:06:49.276923|2020-07-04|\n|UA000000107377108|2020-07-04 16:03:48.143633|2020-07-04|\n+-----------------+--------------------------+----------+\nonly showing top 5 rows\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["The **date_sub()** and **date_add()** functions are used for adding and subtracting days from a date. \n\nYou have a DataFrame with a string type column \"today\". The value in the today column is in the \"DD-MM-YYYY\" format. You want to add a column \"week_later\" to this dataframe with a value of one week later to column \"today\".\n\n`myDF.withColumn(\"week_ago\", date_add(to_date(\"today\", \"dd-MM-yyyy\"), 7))`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e7275033-8eee-443e-9bd9-e84ff1919251"}}},{"cell_type":"code","source":["# \"date_add\" -> Returns the date that is the given number of days after start\n\nfrom pyspark.sql.functions import date_add\n\nplus2DF = timestampDF.withColumn(\"plus_two_days\", date_add(col(\"timestamp\"), 2))\nplus2DF.show(5, False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1b198ecd-2a6c-4646-bc57-0b01d2f831c5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----------------+--------------------------+-------------+\n|user_id          |timestamp                 |plus_two_days|\n+-----------------+--------------------------+-------------+\n|UA000000107379500|2020-07-04 16:09:06.592107|2020-07-06   |\n|UA000000107359357|2020-07-04 15:36:51.756535|2020-07-06   |\n|UA000000107375547|2020-07-04 16:06:55.4591  |2020-07-06   |\n|UA000000107370581|2020-07-04 16:06:49.276923|2020-07-06   |\n|UA000000107377108|2020-07-04 16:03:48.143633|2020-07-06   |\n+-----------------+--------------------------+-------------+\nonly showing top 5 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----------------+--------------------------+-------------+\n|user_id          |timestamp                 |plus_two_days|\n+-----------------+--------------------------+-------------+\n|UA000000107379500|2020-07-04 16:09:06.592107|2020-07-06   |\n|UA000000107359357|2020-07-04 15:36:51.756535|2020-07-06   |\n|UA000000107375547|2020-07-04 16:06:55.4591  |2020-07-06   |\n|UA000000107370581|2020-07-04 16:06:49.276923|2020-07-06   |\n|UA000000107377108|2020-07-04 16:03:48.143633|2020-07-06   |\n+-----------------+--------------------------+-------------+\nonly showing top 5 rows\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["**current_date()** – function return current system date without time in Spark DateType format “yyyy-MM-dd”\n\n**current_timestamp()** – function returns current system date & timestamp in Spark TimestampType format “yyyy-MM-dd HH:mm:ss”\n\nYou can use **datediff()** that will return the number of days in between two dates. You can also use the **months_between()** function that gives you the number of months between two dates."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bc6bf36b-7a4b-449d-a7ad-e9b24bbf8479"}}},{"cell_type":"markdown","source":["##### Complex types"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c5e0ce01-4703-4839-8e62-8baf65ba12c3"}}},{"cell_type":"code","source":["df = spark.read.parquet(\"/mnt/training/ecommerce/sales/sales.parquet\")\ndf.show(5, False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f5244da5-6fe5-4888-ae38-d519617b92bf"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------+-------------------------------+---------------------+-------------------+-----------------------+------------+-----------------------------------------------------------------+\n|order_id|email                          |transaction_timestamp|total_item_quantity|purchase_revenue_in_usd|unique_items|items                                                            |\n+--------+-------------------------------+---------------------+-------------------+-----------------------+------------+-----------------------------------------------------------------+\n|257437  |kmunoz@powell-duran.com        |1592194221828900     |1                  |1995.0                 |1           |[{null, M_PREM_K, Premium King Mattress, 1995.0, 1995.0, 1}]     |\n|282611  |bmurillo@hotmail.com           |1592504237604072     |1                  |940.5                  |1           |[{NEWBED10, M_STAN_Q, Standard Queen Mattress, 940.5, 1045.0, 1}]|\n|257448  |bradley74@gmail.com            |1592200438030141     |1                  |945.0                  |1           |[{null, M_STAN_F, Standard Full Mattress, 945.0, 945.0, 1}]      |\n|257440  |jameshardin@campbell-morris.biz|1592197217716495     |1                  |1045.0                 |1           |[{null, M_STAN_Q, Standard Queen Mattress, 1045.0, 1045.0, 1}]   |\n|283949  |whardin@hotmail.com            |1592510720760323     |1                  |535.5                  |1           |[{NEWBED10, M_STAN_T, Standard Twin Mattress, 535.5, 595.0, 1}]  |\n+--------+-------------------------------+---------------------+-------------------+-----------------------+------------+-----------------------------------------------------------------+\nonly showing top 5 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------+-------------------------------+---------------------+-------------------+-----------------------+------------+-----------------------------------------------------------------+\n|order_id|email                          |transaction_timestamp|total_item_quantity|purchase_revenue_in_usd|unique_items|items                                                            |\n+--------+-------------------------------+---------------------+-------------------+-----------------------+------------+-----------------------------------------------------------------+\n|257437  |kmunoz@powell-duran.com        |1592194221828900     |1                  |1995.0                 |1           |[{null, M_PREM_K, Premium King Mattress, 1995.0, 1995.0, 1}]     |\n|282611  |bmurillo@hotmail.com           |1592504237604072     |1                  |940.5                  |1           |[{NEWBED10, M_STAN_Q, Standard Queen Mattress, 940.5, 1045.0, 1}]|\n|257448  |bradley74@gmail.com            |1592200438030141     |1                  |945.0                  |1           |[{null, M_STAN_F, Standard Full Mattress, 945.0, 945.0, 1}]      |\n|257440  |jameshardin@campbell-morris.biz|1592197217716495     |1                  |1045.0                 |1           |[{null, M_STAN_Q, Standard Queen Mattress, 1045.0, 1045.0, 1}]   |\n|283949  |whardin@hotmail.com            |1592510720760323     |1                  |535.5                  |1           |[{NEWBED10, M_STAN_T, Standard Twin Mattress, 535.5, 595.0, 1}]  |\n+--------+-------------------------------+---------------------+-------------------+-----------------------+------------+-----------------------------------------------------------------+\nonly showing top 5 rows\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##### String Functions\nHere are some of the built-in functions available for manipulating strings.\n\n| Method | Description |\n| --- | --- |\n| translate | Translate any character in the src by a character in replaceString |\n| regexp_replace | Replace all substrings of the specified string value that match regexp with rep |\n| regexp_extract | Extract a specific group matched by a Java regex, from the specified string column |\n| ltrim | Removes the leading space characters from the specified string column |\n| lower | Converts a string column to lowercase |\n| split | Splits str around matches of the given pattern |"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"956a7a6a-eb39-4972-a06d-2d4874ed033a"}}},{"cell_type":"markdown","source":["By using **translate()** string function you can replace character by character of DataFrame column value. In the below example, every character of 1 is replaced with A, 2 replaced with B, and 3 replaced with C on the address column."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5703e58f-e079-4c8a-888b-4d9e05c2c2e2"}}},{"cell_type":"markdown","source":["**Structs**\nYou can think of structs as DataFrames within DataFrames. \n`from pyspark.sql.functions import struct\ncomplexDF = df.select(struct(\"Description\", \"InvoiceNo\").alias(\"complex\"))\ncomplexDF.createOrReplaceTempView(\"complexDF\")\n`\nWe now have a DataFrame with a column complex. We can query it just as we might another DataFrame, the only difference is that we use a dot syntax to do so, or the column method getField:\n`complexDF.select(\"complex.Description\")\ncomplexDF.select(col(\"complex\").getField(\"Description\"))`\n\nWe can also query all values in the struct by using *. This brings up all the columns to the top level DataFrame:\n`complexDF.select(\"complex.*\")`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6ad4fd9f-53be-4269-9b18-bd5d519de17c"}}},{"cell_type":"markdown","source":["The **split()** operation comes from the imported functions object. It accepts a Column object and split characteras arguments. It is not a method of a Column object. Split function has to specify the delimiter.\n`df.select(split(col(\"Description\"), \" \")).show(2)`\n\n**randomSplit()** -> Split a DataFrame by 70/30 into 2 new DataFrames. `df1, df2 = df.randomSplit([0.70, 0.30])` Randomly splits this DataFrame with the provided weights. Parameters:\n- weightslist: list of doubles as weights with which to split the DataFrame. Weights will be normalized if they don’t sum up to 1.0.\n- seedint, optional: the seed for sampling.\n\n`splits = df4.randomSplit([1.0, 2.0], 24)`\n\nThe **initcap** function will capitalize every word in a given string when that word is separated from another by a space.\n`df.select(initcap(col(\"Description\"))).show()`\n\n**upper and lower** \n`df.select(col(\"Description\"),\nlower(col(\"Description\")),\nupper(lower(col(\"Description\")))).show(2)`\n\n** Another trivial task is adding or removing spaces around a string. You can do this by using **lpad** (left-pad the string column),**ltrim, rpad and rtrim, trim**\n\nSometimes, rather than extracting values, we simply want to check for their existence. We can do this with the **contains** method on each column. This will return a Boolean declaring whether the value you specify is in the column’s string. In Python and SQL, we can use the **instr** function."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a61186ed-7433-4988-8327-53b19e624317"}}},{"cell_type":"markdown","source":["**concat()** function of Pyspark SQL is used to concatenate multiple DataFrame columns into a single column. It can also be used to concatenate column types string, binary, and compatible array columns.\n\n`df2=df.select(concat(df.firstname,df.middlename,df.lastname).alias(\"FullName\"))`\n\n**concat_ws()** function of Pyspark concatenates multiple string columns into a single column with a given separator or delimiter.\n\n`df3=df.select(concat_ws('_',df.firstname,df.middlename,df.lastname).alias(\"FullName\"))`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"345bdaea-15ba-4f71-a439-28daa1ae618f"}}},{"cell_type":"markdown","source":["##### Collection Functions\n\nHere are some of the built-in functions available for working with arrays.\n\n| Method | Description |\n| --- | --- |\n| array_contains | Returns null if the array is null, true if the array contains value, and false otherwise. |\n| element_at | Returns element of array at given index. Array elements are numbered starting with **1**. |\n| explode | Creates a new row for each element in the given array or map column. |\n| collect_set | Returns a set of objects with duplicate elements eliminated. |"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cb85e8f9-b656-4302-900b-f540fa8bd711"}}},{"cell_type":"markdown","source":["**getField()** An expression that gets a field by name in a StructField."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bb0a44a4-fc85-474a-a69e-abe182ff7941"}}},{"cell_type":"markdown","source":["The **explode** function takes a column that consists of arrays and creates one row (with the rest of the values duplicated) per value in the array.\n`df.withColumn(\"splitted\", split(col(\"Description\"), \" \"))\\\n.withColumn(\"exploded\", explode(col(\"splitted\")))\\\n.select(\"Description\", \"InvoiceNo\", \"exploded\").show(2)`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9a88e896-78ea-4d3c-af72-e922b3e7d521"}}},{"cell_type":"code","source":["from pyspark.sql.functions import *\n\ndetailsDF = (df\n             .withColumn(\"items\", explode(\"items\"))\n             .select(\"email\", \"items.item_name\")\n             .withColumn(\"details\", split(col(\"item_name\"), \" \"))\n             \n            )\ndetailsDF.show(5, False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b986ca58-9b35-4be5-8cd4-8ffaa9c37cae"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------------------------------+-----------------------+---------------------------+\n|email                          |item_name              |details                    |\n+-------------------------------+-----------------------+---------------------------+\n|kmunoz@powell-duran.com        |Premium King Mattress  |[Premium, King, Mattress]  |\n|bmurillo@hotmail.com           |Standard Queen Mattress|[Standard, Queen, Mattress]|\n|bradley74@gmail.com            |Standard Full Mattress |[Standard, Full, Mattress] |\n|jameshardin@campbell-morris.biz|Standard Queen Mattress|[Standard, Queen, Mattress]|\n|whardin@hotmail.com            |Standard Twin Mattress |[Standard, Twin, Mattress] |\n+-------------------------------+-----------------------+---------------------------+\nonly showing top 5 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------------------------------+-----------------------+---------------------------+\n|email                          |item_name              |details                    |\n+-------------------------------+-----------------------+---------------------------+\n|kmunoz@powell-duran.com        |Premium King Mattress  |[Premium, King, Mattress]  |\n|bmurillo@hotmail.com           |Standard Queen Mattress|[Standard, Queen, Mattress]|\n|bradley74@gmail.com            |Standard Full Mattress |[Standard, Full, Mattress] |\n|jameshardin@campbell-morris.biz|Standard Queen Mattress|[Standard, Queen, Mattress]|\n|whardin@hotmail.com            |Standard Twin Mattress |[Standard, Twin, Mattress] |\n+-------------------------------+-----------------------+---------------------------+\nonly showing top 5 rows\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["mattressDF = (detailsDF\n              .filter(array_contains(col(\"details\"), \"Mattress\"))\n              .withColumn(\"size\", element_at(col(\"details\"), 2))\n              .withColumn(\"quality\", element_at(col(\"details\"), 1))\n             )\nmattressDF.show(5, False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5aea5fd5-9daa-4d0f-91ff-916d3abc7d40"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------------------------------+-----------------------+---------------------------+-----+--------+\n|email                          |item_name              |details                    |size |quality |\n+-------------------------------+-----------------------+---------------------------+-----+--------+\n|kmunoz@powell-duran.com        |Premium King Mattress  |[Premium, King, Mattress]  |King |Premium |\n|bmurillo@hotmail.com           |Standard Queen Mattress|[Standard, Queen, Mattress]|Queen|Standard|\n|bradley74@gmail.com            |Standard Full Mattress |[Standard, Full, Mattress] |Full |Standard|\n|jameshardin@campbell-morris.biz|Standard Queen Mattress|[Standard, Queen, Mattress]|Queen|Standard|\n|whardin@hotmail.com            |Standard Twin Mattress |[Standard, Twin, Mattress] |Twin |Standard|\n+-------------------------------+-----------------------+---------------------------+-----+--------+\nonly showing top 5 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------------------------------+-----------------------+---------------------------+-----+--------+\n|email                          |item_name              |details                    |size |quality |\n+-------------------------------+-----------------------+---------------------------+-----+--------+\n|kmunoz@powell-duran.com        |Premium King Mattress  |[Premium, King, Mattress]  |King |Premium |\n|bmurillo@hotmail.com           |Standard Queen Mattress|[Standard, Queen, Mattress]|Queen|Standard|\n|bradley74@gmail.com            |Standard Full Mattress |[Standard, Full, Mattress] |Full |Standard|\n|jameshardin@campbell-morris.biz|Standard Queen Mattress|[Standard, Queen, Mattress]|Queen|Standard|\n|whardin@hotmail.com            |Standard Twin Mattress |[Standard, Twin, Mattress] |Twin |Standard|\n+-------------------------------+-----------------------+---------------------------+-----+--------+\nonly showing top 5 rows\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["pillowDF = (detailsDF\n            .filter(array_contains(col(\"details\"), \"Pillow\"))\n            .withColumn(\"size\", element_at(col(\"details\"), 1))\n            .withColumn(\"quality\", element_at(col(\"details\"), 2))\n           )\npillowDF.show(5, False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e1273302-b942-4797-9fa3-54825be9933b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+------------------------+--------------------+------------------------+--------+-------+\n|email                   |item_name           |details                 |size    |quality|\n+------------------------+--------------------+------------------------+--------+-------+\n|maxwelltara@edwards.com |Standard Foam Pillow|[Standard, Foam, Pillow]|Standard|Foam   |\n|marmstrong46@hotmail.com|Standard Foam Pillow|[Standard, Foam, Pillow]|Standard|Foam   |\n|johnsonderrick@yahoo.com|King Down Pillow    |[King, Down, Pillow]    |King    |Down   |\n|johnsonderrick@yahoo.com|Standard Down Pillow|[Standard, Down, Pillow]|Standard|Down   |\n|hilljoshua43@hotmail.com|Standard Foam Pillow|[Standard, Foam, Pillow]|Standard|Foam   |\n+------------------------+--------------------+------------------------+--------+-------+\nonly showing top 5 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+------------------------+--------------------+------------------------+--------+-------+\n|email                   |item_name           |details                 |size    |quality|\n+------------------------+--------------------+------------------------+--------+-------+\n|maxwelltara@edwards.com |Standard Foam Pillow|[Standard, Foam, Pillow]|Standard|Foam   |\n|marmstrong46@hotmail.com|Standard Foam Pillow|[Standard, Foam, Pillow]|Standard|Foam   |\n|johnsonderrick@yahoo.com|King Down Pillow    |[King, Down, Pillow]    |King    |Down   |\n|johnsonderrick@yahoo.com|Standard Down Pillow|[Standard, Down, Pillow]|Standard|Down   |\n|hilljoshua43@hotmail.com|Standard Foam Pillow|[Standard, Foam, Pillow]|Standard|Foam   |\n+------------------------+--------------------+------------------------+--------+-------+\nonly showing top 5 rows\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# pyspark.sql.DataFrame.unionByName\n# Returns a new DataFrame containing union of rows in this and another DataFrame.\n# This is different from both UNION ALL and UNION DISTINCT in SQL. To do a SQL-style set union (that does deduplication of elements), use this function followed by distinct().\n\nunionDF = mattressDF.unionByName(pillowDF).drop(\"details\")\nunionDF.show(5, False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d6704fe5-bc53-47b4-bd52-9e77164c6cdd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------------------------------+-----------------------+-----+--------+\n|email                          |item_name              |size |quality |\n+-------------------------------+-----------------------+-----+--------+\n|kmunoz@powell-duran.com        |Premium King Mattress  |King |Premium |\n|bmurillo@hotmail.com           |Standard Queen Mattress|Queen|Standard|\n|bradley74@gmail.com            |Standard Full Mattress |Full |Standard|\n|jameshardin@campbell-morris.biz|Standard Queen Mattress|Queen|Standard|\n|whardin@hotmail.com            |Standard Twin Mattress |Twin |Standard|\n+-------------------------------+-----------------------+-----+--------+\nonly showing top 5 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------------------------------+-----------------------+-----+--------+\n|email                          |item_name              |size |quality |\n+-------------------------------+-----------------------+-----+--------+\n|kmunoz@powell-duran.com        |Premium King Mattress  |King |Premium |\n|bmurillo@hotmail.com           |Standard Queen Mattress|Queen|Standard|\n|bradley74@gmail.com            |Standard Full Mattress |Full |Standard|\n|jameshardin@campbell-morris.biz|Standard Queen Mattress|Queen|Standard|\n|whardin@hotmail.com            |Standard Twin Mattress |Twin |Standard|\n+-------------------------------+-----------------------+-----+--------+\nonly showing top 5 rows\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##### Aggregate Functions\n\nHere are some of the built-in aggregate functions available for creating arrays, typically from GroupedData.\n\n| Method | Description |\n| --- | --- |\n| collect_list | Returns an array consisting of all values within the group. |\n| collect_set | Returns an array consisting of all unique values within the group. |"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cdd5cac0-4bf6-4528-a725-2dab54ce7fc9"}}},{"cell_type":"code","source":["optionsDF = (unionDF\n             .groupBy(\"email\")\n             .agg(collect_set(\"size\").alias(\"size options\"),\n                  collect_set(\"quality\").alias(\"quality options\"))\n            )\noptionsDF.show(5, False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"404efeb2-053b-403e-8f93-606216ebcbeb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----------------------+-------------+-------------------+\n|email                  |size options |quality options    |\n+-----------------------+-------------+-------------------+\n|aadkins@hill.biz       |[Twin]       |[Standard]         |\n|aalexander@hotmail.com |[King]       |[Standard]         |\n|aallen43@hotmail.com   |[Queen, Twin]|[Premium, Standard]|\n|aallen@keith-taylor.com|[Queen]      |[Standard]         |\n|aalvarez4@gmail.com    |[Queen]      |[Standard]         |\n+-----------------------+-------------+-------------------+\nonly showing top 5 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----------------------+-------------+-------------------+\n|email                  |size options |quality options    |\n+-----------------------+-------------+-------------------+\n|aadkins@hill.biz       |[Twin]       |[Standard]         |\n|aalexander@hotmail.com |[King]       |[Standard]         |\n|aallen43@hotmail.com   |[Queen, Twin]|[Premium, Standard]|\n|aallen@keith-taylor.com|[Queen]      |[Standard]         |\n|aalvarez4@gmail.com    |[Queen]      |[Standard]         |\n+-----------------------+-------------+-------------------+\nonly showing top 5 rows\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##### Additional functions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"59e68f7c-4167-448d-a1c4-ea841810ff66"}}},{"cell_type":"markdown","source":["##### DataFrameNaFunctions\nDataFrameNaFunctions is a DataFrame submodule with methods for handling null values. Obtain an instance of DataFrameNaFunctions by accessing the `na` attribute of a DataFrame.\n\n| Method | Description |\n| --- | --- |\n| drop | Returns a new DataFrame omitting rows with any, all, or a specified number of null values, considering an optional subset of columns |\n| fill | Replace null values with the specified value for an optional subset of columns |\n| replace | Returns a new DataFrame replacing a value with another value, considering an optional subset of columns |"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cf8cd224-b7e2-4dfb-a132-b52c39c7f53c"}}},{"cell_type":"markdown","source":["df.na.drop(\"all\").show(false)\n-> drops all rows that has NULL values on all columns.\n\nExample: \n- `df.na.drop(\"all\")` delete rows if all columns are null.\n- `df.na.drop()` delete all the rows having any null column.\n- `df.na.drop(subset=(\"Year\", \"CourseName\"))` delete all rows if Year or CourseName column is null.\n\n`df.na.drop(thresh=1)` -> Threashold=1 means \"keep the row if at least 1 column is not null\"\n\n**fill** Using the fill function, you can fill one or more columns with a set of values. This can be done by specifying a map—that is a particular value and a set of columns. For example, to fill all null values in columns of type String, you might specify the following:\n`df.na.fill(\"All Null values become this string\")`\n\nWe could do the same for columns of type Integer by using df.na.fill(5:Integer), or for Doubles df.na.fill(5:Double). To specify columns, we just pass in an array of column names.\n`df.na.fill(\"all\", subset=[\"StockCode\", \"InvoiceNo\"])`\n\n`df.na.fill(\"NA\")` Update all column values in the DataFrame with NA if the current value is null. \n\n`df.na.fill({\"Year\": \"2021\", \"CourseName\": \"Python\"})`\n\n**replace** In addition to replacing null values like we did with drop and fill, there are more flexible options that you can use with more than just null values. Probably the most common use case is to replace all values in a certain column according to their current value. The only requirement is that this value be the same type as the original value.\n`df.na.replace([\"\"], [\"UNKNOWN\"], \"Description\")`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b32964b3-1846-430e-b0e1-cfd43fcb5158"}}},{"cell_type":"markdown","source":["##### Non-aggregate and Miscellaneous Functions\nHere are a few additional non-aggregate and miscellaneous built-in functions.\n\n| Method | Description |\n| --- | --- |\n| col / column | Returns a Column based on the given column name. |\n| lit | Creates a Column of literal value |\n| isnull | Return true iff the column is null |\n| rand | Generate a random column with independent and identically distributed (i.i.d.) samples uniformly distributed in [0.0, 1.0) |"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8c00e4bb-1913-470c-b385-acec06a31ee3"}}},{"cell_type":"markdown","source":["sample( withReplacement, fraction, seed=None)\n\n- fraction – Fraction of rows to generate, range [0.0, 1.0]. Note that it doesn’t guarantee to provide the exact number of the fraction of records.\n- seed – Seed for sampling (default a random seed). Used to reproduce the same random sampling.\n- withReplacement – Sample with replacement or not (default False)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2b36cdde-379e-474e-9de1-d49e22a95659"}}},{"cell_type":"code","source":["salesDF = spark.read.parquet(\"/mnt/training/ecommerce/sales/sales.parquet\")\nsalesDF.show(5, False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e1366b40-9f9b-400e-b8a0-1696a34924d3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------+-------------------------------+---------------------+-------------------+-----------------------+------------+-----------------------------------------------------------------+\n|order_id|email                          |transaction_timestamp|total_item_quantity|purchase_revenue_in_usd|unique_items|items                                                            |\n+--------+-------------------------------+---------------------+-------------------+-----------------------+------------+-----------------------------------------------------------------+\n|257437  |kmunoz@powell-duran.com        |1592194221828900     |1                  |1995.0                 |1           |[{null, M_PREM_K, Premium King Mattress, 1995.0, 1995.0, 1}]     |\n|282611  |bmurillo@hotmail.com           |1592504237604072     |1                  |940.5                  |1           |[{NEWBED10, M_STAN_Q, Standard Queen Mattress, 940.5, 1045.0, 1}]|\n|257448  |bradley74@gmail.com            |1592200438030141     |1                  |945.0                  |1           |[{null, M_STAN_F, Standard Full Mattress, 945.0, 945.0, 1}]      |\n|257440  |jameshardin@campbell-morris.biz|1592197217716495     |1                  |1045.0                 |1           |[{null, M_STAN_Q, Standard Queen Mattress, 1045.0, 1045.0, 1}]   |\n|283949  |whardin@hotmail.com            |1592510720760323     |1                  |535.5                  |1           |[{NEWBED10, M_STAN_T, Standard Twin Mattress, 535.5, 595.0, 1}]  |\n+--------+-------------------------------+---------------------+-------------------+-----------------------+------------+-----------------------------------------------------------------+\nonly showing top 5 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------+-------------------------------+---------------------+-------------------+-----------------------+------------+-----------------------------------------------------------------+\n|order_id|email                          |transaction_timestamp|total_item_quantity|purchase_revenue_in_usd|unique_items|items                                                            |\n+--------+-------------------------------+---------------------+-------------------+-----------------------+------------+-----------------------------------------------------------------+\n|257437  |kmunoz@powell-duran.com        |1592194221828900     |1                  |1995.0                 |1           |[{null, M_PREM_K, Premium King Mattress, 1995.0, 1995.0, 1}]     |\n|282611  |bmurillo@hotmail.com           |1592504237604072     |1                  |940.5                  |1           |[{NEWBED10, M_STAN_Q, Standard Queen Mattress, 940.5, 1045.0, 1}]|\n|257448  |bradley74@gmail.com            |1592200438030141     |1                  |945.0                  |1           |[{null, M_STAN_F, Standard Full Mattress, 945.0, 945.0, 1}]      |\n|257440  |jameshardin@campbell-morris.biz|1592197217716495     |1                  |1045.0                 |1           |[{null, M_STAN_Q, Standard Queen Mattress, 1045.0, 1045.0, 1}]   |\n|283949  |whardin@hotmail.com            |1592510720760323     |1                  |535.5                  |1           |[{NEWBED10, M_STAN_T, Standard Twin Mattress, 535.5, 595.0, 1}]  |\n+--------+-------------------------------+---------------------+-------------------+-----------------------+------------+-----------------------------------------------------------------+\nonly showing top 5 rows\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import *\nconvertedUsersDF = (salesDF.select(\"email\")\n                           .dropDuplicates([\"email\"])\n                           .withColumn(\"converted\", lit(True))\n)\nconvertedUsersDF.show(5, False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2666b0e7-d23d-404d-8ed4-e7f63c4c9889"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+------------------------------+---------+\n|email                         |converted|\n+------------------------------+---------+\n|zacharyfisher@brown.com       |true     |\n|flowersrhonda@paul.com        |true     |\n|tanya8857@yahoo.com           |true     |\n|serranoerika@brooks-lawson.com|true     |\n|bishopamber@yahoo.com         |true     |\n+------------------------------+---------+\nonly showing top 5 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+------------------------------+---------+\n|email                         |converted|\n+------------------------------+---------+\n|zacharyfisher@brown.com       |true     |\n|flowersrhonda@paul.com        |true     |\n|tanya8857@yahoo.com           |true     |\n|serranoerika@brooks-lawson.com|true     |\n|bishopamber@yahoo.com         |true     |\n+------------------------------+---------+\nonly showing top 5 rows\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##### Joining DataFrames\nThe DataFrame <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.join.html?highlight=join#pyspark.sql.DataFrame.join\" target=\"_blank\">`join`</a> method joins two DataFrames based on a given join expression. Inner joins are the default join. Several different types of joins are supported. For example:\n\n```\n# Inner join based on equal values of a shared column called 'name' (i.e., an equi join)\ndf1.join(df2, 'name')\n\n# Inner join based on equal values of the shared columns called 'name' and 'age'\ndf1.join(df2, ['name', 'age'])\n\n# Full outer join based on equal values of a shared column called 'name'\ndf1.join(df2, 'name', 'outer')\n\n# Left outer join based on an explicit column expression\ndf1.join(df2, df1['customer_name'] == df2['account_name'], 'left_outer')\n```\n\n->  You can do **left_anti** join for implementing NOT EXISTS condition."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9b883903-133c-4b89-ab41-704f88617df5"}}},{"cell_type":"code","source":["usersDF = spark.read.parquet( \"/mnt/training/ecommerce/users/users.parquet\")\nusersDF.show(5, False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0cc494a8-4bca-4476-ac4d-5c08361e7efe"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----------------+--------------------------+-------------------------+\n|user_id          |user_first_touch_timestamp|email                    |\n+-----------------+--------------------------+-------------------------+\n|UA000000102357305|1592182691348767          |null                     |\n|UA000000102357308|1592183287634953          |null                     |\n|UA000000102357309|1592183302736627          |null                     |\n|UA000000102357321|1592184604178702          |david23@orozco-parker.com|\n|UA000000102357325|1592185154063628          |null                     |\n+-----------------+--------------------------+-------------------------+\nonly showing top 5 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----------------+--------------------------+-------------------------+\n|user_id          |user_first_touch_timestamp|email                    |\n+-----------------+--------------------------+-------------------------+\n|UA000000102357305|1592182691348767          |null                     |\n|UA000000102357308|1592183287634953          |null                     |\n|UA000000102357309|1592183302736627          |null                     |\n|UA000000102357321|1592184604178702          |david23@orozco-parker.com|\n|UA000000102357325|1592185154063628          |null                     |\n+-----------------+--------------------------+-------------------------+\nonly showing top 5 rows\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["conversionsDF = (usersDF\n                        .join(convertedUsersDF, \"email\", \"outer\")\n                        .filter(col(\"email\").isNotNull())\n                        .na.fill(False)\n)\n\nconversionsDF.show(5, False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"33978344-af31-4254-b7d0-9ab363fbff6d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----------------------------+-----------------+--------------------------+---------+\n|email                        |user_id          |user_first_touch_timestamp|converted|\n+-----------------------------+-----------------+--------------------------+---------+\n|aabbott@fischer-thompson.info|UA000000107293930|1593868005679801          |false    |\n|aacevedo@moss-young.com      |UA000000103755561|1592671212475050          |false    |\n|aacosta11@gmail.com          |UA000000106362980|1593540790039008          |false    |\n|aadams9@gmail.com            |UA000000103384927|1592575968245258          |false    |\n|aadams@coleman.org           |UA000000107105749|1593795399348718          |false    |\n+-----------------------------+-----------------+--------------------------+---------+\nonly showing top 5 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----------------------------+-----------------+--------------------------+---------+\n|email                        |user_id          |user_first_touch_timestamp|converted|\n+-----------------------------+-----------------+--------------------------+---------+\n|aabbott@fischer-thompson.info|UA000000107293930|1593868005679801          |false    |\n|aacevedo@moss-young.com      |UA000000103755561|1592671212475050          |false    |\n|aacosta11@gmail.com          |UA000000106362980|1593540790039008          |false    |\n|aadams9@gmail.com            |UA000000103384927|1592575968245258          |false    |\n|aadams@coleman.org           |UA000000107105749|1593795399348718          |false    |\n+-----------------------------+-----------------+--------------------------+---------+\nonly showing top 5 rows\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["crossJoin()\n\n`storesDF.crossJoin.employeesDF`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"813305d5-6d0e-479b-859e-81daab0e1120"}}},{"cell_type":"markdown","source":["You can handle ambiguous column names in three ways.\n1. Use the DataFrame to reference the column name such as df2. BatchID\n\n`joinType = \"inner\"`\n`joinExpr = df1.BatchID == df2.BatchID`\n`df1.join(df2, joinExpr, joinType).select(df1.BatchID, df1.Year).show()`\n\n2. Drop one of the two ambiguous columns after join\n\n`joinType = \"inner\"`\n`joinExpr = df1.BatchID == df2.BatchID`\n`df1.join(df2, joinExpr, joinType).drop(df2.BatchID).select(\"BatchID\", \"Year\").show()`\n\n3. Use the common name as your join expression so Spark can auto-remove one ambiguous column after join.\n\n`joinType = \"inner\"`\n`joinExpr = \"BatchID\"`\n`df1.join(df2, joinExpr, joinType).select(\"BatchID\", \"Year\").show()`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8b57116b-b2a9-4095-a45d-2d4273cda974"}}},{"cell_type":"markdown","source":["**Rank vs dense_rank**\n\nWindow function: returns the rank of rows within a window partition, without any gaps. The difference between rank and dense_rank is that dense_rank leaves no gaps in ranking sequence when there are ties. That is, if you were ranking a competition using dense_rank and had three people tie for second place, you would say that all three were in second place and that the next person came in third. Rank would give me sequential numbers, making the person that came in third place (after the ties) would register as coming in fifth."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"70e200d9-b70c-4c3f-b224-177e53dcbdb6"}}},{"cell_type":"markdown","source":["##### UDFs\n\nA custom column transformation function\n\n- Can’t be optimized by Catalyst Optimizer\n- Function is serialized and sent to executors\n- Row data is deserialized from Spark's native binary format to pass to the UDF, and the results are serialized back into Spark's native format\n- For Python UDFs, additional interprocess communication overhead between the executor and a Python interpreter running on each worker node"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bf590173-63f6-40b0-a266-31f51833dd59"}}},{"cell_type":"code","source":["# Define a function (on the driver) to get the first letter of a string from the \"email\" field.\n\ndef firstLetterFunction(email):\n    return email[0]\n\nfirstLetterFunction(\"annagray@kaufman.com\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"48e42b70-2edc-4440-8d81-ab163d58afea"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[23]: 'a'","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[23]: 'a'"]}}],"execution_count":0},{"cell_type":"markdown","source":["Now that we’ve created these functions and tested them, we need to register them with Spark so that we can use them on all of our worker machines. Spark will serialize the function on the\ndriver and transfer it over the network to all executor processes. This happens regardless of language.\n\nIf the function is written in Python, something quite different happens. Spark starts a Python process on the worker, serializes all of the data to a format that Python can understand\n(remember, it was in the JVM earlier), executes the function row by row on that data in the Python process, and then finally returns the results of the row operations to the JVM and Spark."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"db48612a-c71a-4418-90e3-d7f1b8891855"}}},{"cell_type":"code","source":["# Create and apply UDF\n# Register the function as a UDF. This serializes the function and sends it to executors to be able to transform DataFrame records.\n\nfirstLetterUDF = udf(firstLetterFunction)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"15d2c9e1-0307-4f49-8ea3-345c7403effe"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Apply the UDF on the \"email\" column of the sales dataframe.\n\nsalesDF.select(firstLetterUDF(col(\"email\"))).show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e04a3305-498e-4bc1-9af0-861be4db87eb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------------------------+\n|firstLetterFunction(email)|\n+--------------------------+\n|                         k|\n|                         b|\n|                         b|\n|                         j|\n|                         w|\n+--------------------------+\nonly showing top 5 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------------------------+\n|firstLetterFunction(email)|\n+--------------------------+\n|                         k|\n|                         b|\n|                         b|\n|                         j|\n|                         w|\n+--------------------------+\nonly showing top 5 rows\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# We can also register this UDF as a Spark SQL function.\n# Register the UDF using \"spark.udf.register\" to also make it available for use in the SQL namespace.\n\nfirstLetterUDF = spark.udf.register(\"sql_udf\", firstLetterFunction)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e104ce3f-aa31-4568-9f01-ff3bd0754c7e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["salesDF.createOrReplaceTempView(\"sales\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5b096e7e-0107-4405-a992-b3d9d11dbf68"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\n-- You can now also apply the UDF from SQL (but you can still apply the UDF from Python)\n\nSELECT sql_udf(email) AS firstLetter FROM sales LIMIT 5"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0b388607-790c-411e-b044-5a6fd714cdd6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["k"],["b"],["b"],["j"],["w"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"firstLetter","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>firstLetter</th></tr></thead><tbody><tr><td>k</td></tr><tr><td>b</td></tr><tr><td>b</td></tr><tr><td>j</td></tr><tr><td>w</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Example:\n\nspark.udf.register(\"ASSESS_PERFORMANCE\", assessPerformance)\n\nspark.sql(\"SELECT customerSatisfaction, ASSESS_PERFORMANCE(customerSatisfaction) AS result FROM stores\")\n\nIf you don't register the UDF as an SQL function you cannot use it in a string expression (example: `df.selectExpr(\"power3_udf(num)\").show()`). You can register a UDF in 2 ways:\n\n- func_name_udf = udf(func_name) -> this registers the UDF as a DataFrame function so you can use it in your DataFrame expressions as a function. But you cannot use it in a string expression.\n- spark.udf.register(\"func_name_udf\",func_name) -> it registers the UDF as a SQL function. So you can use it in the string expressions."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"871884c7-3912-48a3-b52a-e170dd77e3a0"}}},{"cell_type":"markdown","source":["##### Use Decorator Syntax (Python Only)\n\nAlternatively, you can define and register a UDF using <Python decorator syntax. The `@udf` decorator parameter is the Column datatype the function returns.\n\nYou will no longer be able to call the local Python function (i.e., `firstLetterUDF(\"annagray@kaufman.com\")` will not work)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"50637678-5ff0-4299-9ace-0cb5897ba2c2"}}},{"cell_type":"code","source":["# Our input/output is a string\n\n@udf(\"string\")\ndef firstLetterUDF(email: str) -> str:\n    return email[0]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d9bad174-a4f5-4a6b-a7c0-de07aa9f2b91"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# let's use our decorator UDF here.\n\nsalesDF.select(firstLetterUDF(col(\"email\"))).show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"570fa398-4912-4165-a38e-73ba3462a0cb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---------------------+\n|firstLetterUDF(email)|\n+---------------------+\n|                    k|\n|                    b|\n|                    b|\n|                    j|\n|                    w|\n+---------------------+\nonly showing top 5 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---------------------+\n|firstLetterUDF(email)|\n+---------------------+\n|                    k|\n|                    b|\n|                    b|\n|                    j|\n|                    w|\n+---------------------+\nonly showing top 5 rows\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["##### Pandas/Vectorized UDFs\n\nAs of Spark 2.3, there are Pandas UDFs available in Python to improve the efficiency of UDFs. Pandas UDFs utilize Apache Arrow to speed up computation.\n\nThe user-defined functions are executed using: \n* Apache Arrow, an in-memory columnar data format that is used in Spark to efficiently transfer data between JVM and Python processes with near-zero (de)serialization cost\n* Pandas inside the function, to work with Pandas instances and APIs\n\nAs of Spark 3.0, you should always define your Pandas UDF using Python type hints."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b48f817b-ca34-40fd-b7e6-52ccee86f0a3"}}},{"cell_type":"code","source":["import pandas as pd\nfrom pyspark.sql.functions import pandas_udf\n\n# We have a string input/output\n@pandas_udf(\"string\")\ndef vectorizedUDF(email: pd.Series) -> pd.Series:\n    return email.str[0]\n\n# Alternatively\n# def vectorizedUDF(email: pd.Series) -> pd.Series:\n#     return email.str[0]\n# vectorizedUDF = pandas_udf(vectorizedUDF, \"string\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"53b1f8a5-96e3-4547-8cf6-96aac8a78520"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["salesDF.select(vectorizedUDF(col(\"email\"))).show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1240fac4-62c4-430c-afae-3f9db92fa4a5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------------------+\n|vectorizedUDF(email)|\n+--------------------+\n|                   k|\n|                   b|\n|                   b|\n|                   j|\n|                   w|\n+--------------------+\nonly showing top 5 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------------------+\n|vectorizedUDF(email)|\n+--------------------+\n|                   k|\n|                   b|\n|                   b|\n|                   j|\n|                   w|\n+--------------------+\nonly showing top 5 rows\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# We can also register these Pandas UDFs to the SQL namespace.\n\nspark.udf.register(\"sql_vectorized_udf\", vectorizedUDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"551811da-3276-4423-a3e4-dd9406f289f1"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Define a UDF to label the day of week.\n\ndef labelDayOfWeek(day: str) -> str:\n    dow = {\"Mon\": \"1\", \"Tue\": \"2\", \"Wed\": \"3\", \"Thu\": \"4\",\n           \"Fri\": \"5\", \"Sat\": \"6\", \"Sun\": \"7\"}\n    return dow.get(day) + \"-\" + day"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eb377a2c-b418-466e-8d5b-445097673bf6"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["labelDowUDF = spark.udf.register(\"labelDow\", labelDayOfWeek)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e52ef235-c34d-4c77-80de-d33816facab3"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import approx_count_distinct, avg, col, date_format, to_date\n\ndf = (spark\n      .read\n      .parquet(\"/mnt/training/ecommerce/events/events.parquet\")\n      .withColumn(\"ts\", (col(\"event_timestamp\") / 1e6).cast(\"timestamp\"))\n      .withColumn(\"date\", to_date(\"ts\"))\n      .groupBy(\"date\").agg(approx_count_distinct(\"user_id\").alias(\"active_users\"))\n      .withColumn(\"day\", date_format(col(\"date\"), \"E\"))\n      .groupBy(\"day\").agg(avg(col(\"active_users\")).alias(\"avg_users\"))\n     )\n\ndisplay(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4f35837b-4053-4708-9682-b3200895b2e0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["Sun",282905.5],["Mon",238195.5],["Thu",264620.0],["Sat",278482.0],["Wed",227214.0],["Fri",247180.66666666666],["Tue",260942.5]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"day","type":"\"string\"","metadata":"{}"},{"name":"avg_users","type":"\"double\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>day</th><th>avg_users</th></tr></thead><tbody><tr><td>Sun</td><td>282905.5</td></tr><tr><td>Mon</td><td>238195.5</td></tr><tr><td>Thu</td><td>264620.0</td></tr><tr><td>Sat</td><td>278482.0</td></tr><tr><td>Wed</td><td>227214.0</td></tr><tr><td>Fri</td><td>247180.66666666666</td></tr><tr><td>Tue</td><td>260942.5</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["finalDF = (df.withColumn(\"day\", labelDowUDF(col(\"day\")))\n            .sort(\"day\")\n          )\n\ndisplay(finalDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ae74634a-40f1-4a7d-9e7d-2015a3b30fe4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["1-Mon",238195.5],["2-Tue",260942.5],["3-Wed",227214.0],["4-Thu",264620.0],["5-Fri",247180.66666666666],["6-Sat",278482.0],["7-Sun",282905.5]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"day","type":"\"string\"","metadata":"{}"},{"name":"avg_users","type":"\"double\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>day</th><th>avg_users</th></tr></thead><tbody><tr><td>1-Mon</td><td>238195.5</td></tr><tr><td>2-Tue</td><td>260942.5</td></tr><tr><td>3-Wed</td><td>227214.0</td></tr><tr><td>4-Thu</td><td>264620.0</td></tr><tr><td>5-Fri</td><td>247180.66666666666</td></tr><tr><td>6-Sat</td><td>278482.0</td></tr><tr><td>7-Sun</td><td>282905.5</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Example:\n\nassessPerformanceUDF = udf(assessPerformance, IntegerType())\n\nstoresDF.withColumn(\"result\",assessPerformanceUDF(col(\"customerSatisfaction\")))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3fe3dff3-7757-4ded-b895-cf43d20afba4"}}},{"cell_type":"markdown","source":["You can write Spark UDF in Python or Scala but you cannot use external libraries. \n\nPython UDF runs in a Python process on the worker node. In this case, Spark must serialize data and send it to the Python process from the JVM process. This transfer of data and returning results is extra work causing some performance overhead. This is why Spark usually recommends writing UDFs in Scala or Java.\n\nIf the UDF function is written in Python, Spark starts a Python process on the worker, serializes all of the data to a format that Python can understand (remember, it was in the JVM earlier), executes the function row by row on that data in the Python process, and then finally returns the results of the row operations to the JVM and Spark. -> So, when you create a spark UDF in python, it does not run in the executor JVM."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fcb3b6ed-b670-447c-9ca9-d8779a1e5fbd"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"SparkCertification-Transformations","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":3317787068164964}},"nbformat":4,"nbformat_minor":0}
