{"cells":[{"cell_type":"markdown","source":["#### Spark certification"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cb4eefed-4dcc-4189-aad5-c4fd0472ea47"}}},{"cell_type":"markdown","source":["Spark is a distributed data processing platform. It is and open-source unified analytics engine for large-scale data processing."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7f939c30-bd88-43cc-b50a-6708c66dd8db"}}},{"cell_type":"markdown","source":["##### Databricks platform\n\n* Magic commands: %python, %scala, %sql, %r, %sh, %md (%sh -> run shell commands on the driver)\n* Render HTML: displayHTML\n* DBFS = Databricks File System -> virtual file system, allows you to treat cloud object storage as though it were local files and directories on the cluster. You can run file system commands on DBFS using %fs. (example: %fs ls)\n* DBUtils: dbutils.fs (%fs), dbutils.notebook (%run), dbutils.widgets (example: dbutils.fs.ls(\"/databricks-datasets\"),\n%fs help)\n\nCREATE TABLE: You can use databricks sql commands to create a table:\n\n`%sql\nCREATE TABLE IF NOT EXISTS events USING parquet OPTIONS (path \"/mnt/training/ecommerce/events/events.parquet\");`\n\n* Databricks WIDGETS\nInput widgets allow you to add parameters to your notebooks and dashboards. The widget API consists of calls to create various types of input widgets, remove them, and get bound values. There are 4 types: text, dropdown, combobox, multiselect.\nDocumentation: dbutils.widgets.help()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f597a957-c138-4dc7-a203-49fd403a33f4"}}},{"cell_type":"markdown","source":["<img src=\"https://www.edureka.co/blog/wp-content/uploads/2018/09/Picture6-2.png\" alt=\"spark\">"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d0df11b8-0616-4678-b398-f6d033d5a5e3"}}},{"cell_type":"markdown","source":["##### Spark overview\n\nSpark is standard unified analytics engine for big data processing. It's the largest open-source project in data processing.\nFast (faster than Hadoop), easy to use, unified.\nCore modules: spark SQL + dataframe, spark streaming, MLlib, Spark core API (all other functionality are builton top of it). \n\nSpark execution: it uses clusters, to distribute the work across different machines. The secret of spark performance is PARALLELISM, each parallelize action is refered to as a JOB. A job is broken down in STAGES. Then we have TASKS that depends on stages, created by the driver and assigned a partition of data to process. \n\nSpark Application runs all the Spark Jobs in parallel.\n\nEach spark application may run as a series of spark jobs (the driver converts your spark application into one or more spark jobs). Spark job is internally represented as a DAG (spark's execution plan) of stages. Each node within a DAG could be a single or multiple spark stages. \n\n**DAG** (Directed Acyclic Graph) in Apache Spark is a set of Vertices and Edges, where vertices represent the RDDs and the edges represent the Operation to be applied on RDD. (Como sabes, una característica de Spark es ser Lazy. Esto quiere decir que no ejecuta ningún trabajo hasta que debe entregar un resultado final. Debe crear una “lista” de tareas, que no se ejecutaran hasta que se envíe una orden de ejecución. Esta lista de tareas se conoce como Grafio Aciclico dirigido(DAG), esto quiere decir que las tareas se ejecutan en cascadas sin retornar jamas.)\n\nTASK: a combination of a block of data and a set of transformers that will run on a single executor. If there is one big partition in our dataset, we will have one task. Each task maps to a single core and works on a single partition of data.  As such, an executor with 16 cores can have 16 or more tasks working on 16 or more partitions in parallel, making the execution of Spark’s tasks exceedingly parallel (they DON'T run in a sequence). So, one executor doesn't run only one task at a time.\n\nSTAGE: is a group of tasks that can be executed in parallel to compute the same set of operations on potentially multiple machines. A stage is a unit for work that is executed as a sequence of tasks in parallel without a shuffle. The Spark engine starts new stages after operations called shuffles. A shuffle represents a physical repartitioning of the data. This type of repartitioning requires coordinating across executors to move data around.\n\nEach stage is comprised of Spark tasks (a unit of execution).\n\nComponents that spark use to coordinate work across a cluster of computers:\nDRIVER is the machine in which the application runs. It is responsible for 3 main things:\n* Maintaining information about spark application\n* Responding to the user's programm \n* Analyzing, distributing and scheduling work across the executors\n\nA WORKER node hosts the executor process. It has a fixed number of executors allocated at any point in time.\nEXECUTORS: each executor will hold a chunk of the data to be processed, this chunk is called a SPARK PARTITION. It's a collection of rows that sit on one physical machine in the cluster. -> this is completely separated from harddisk partitions which have to do a storage space on a hardrive. Executors are responsible for carrying out work assigned by a driver. Each executor is responsible for 2 things:\n* Execute the code assigned\n* Report the state of the computation back to the driver\n\nSpark **executors** run on **worker nodes** in the spark cluster. Each worker node may run on one or more executors depending upon the resource availability on the worker node. Spark worker is a node on spark cluster where spark executor runs.\n\nSlots are not the same thing as executors. Executors could have multiple slots in them, and tasks are executed on slots.\n\nWorker nodes are fault-tolerant.\n\nSpark parallelize the work on 2 levels. One is split the work across the executors, the other is the SLOT. Each executors has a number of slots, each slot can be assigned a task. (Slot: CORE)\nSLOTS are resources for parallelization within a Spark application."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"61a43e80-31f3-49dd-8744-5ad5927596c9"}}},{"cell_type":"markdown","source":["<img src=\"https://miro.medium.com/max/1400/1*9lw9eMn9oUbLDN0SgbxQEw.png\" alt=\"spark\">"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"270db8e3-ed58-4367-a57c-2635ca219276"}}},{"cell_type":"markdown","source":["Inside a given Spark application (SparkContext instance), multiple parallel jobs can run simultaneously if they were submitted from separate threads. By “job”, in this section, we mean a Spark action (e.g. save, collect) and any tasks that need to run to evaluate that action. Spark’s scheduler is fully thread-safe and supports this use case to enable applications that serve multiple requests (e.g. queries for multiple users).\n\nBy default, Spark’s scheduler runs jobs in **FIFO fashion**. Each job is divided into “stages” (e.g. map and reduce phases), and the first job gets priority on all available resources while its stages have tasks to launch, then the second job gets priority, etc. If the jobs at the head of the queue don’t need to use the whole cluster, later jobs can start to run right away, but if the jobs at the head of the queue are large, then later jobs may be delayed significantly.\n\nStarting in Spark 0.8, it is also possible to configure fair sharing between jobs. Under fair sharing, Spark assigns tasks between jobs in a **“round robin”** fashion, so that all jobs get a roughly equal share of cluster resources. This means that short jobs submitted while a long job is running can start receiving resources right away and still get good response times, without waiting for the long job to finish. This mode is best for multi-user settings.\n\nTo enable the fair scheduler, simply set the **spark.scheduler.mode** property to **FAIR** when configuring a SparkContext:\n\nSpark's job scheduler:\n- By default, Spark's scheduler runs jobs in a FIFO fashion\n- FAIR scheduler assigns tasks between jobs in a \"round robin\" fashion\n\nIn general, there should be one Spark **job** for one action. Actions always return results. Each job breaks down into a series of **stages**, the number of which depends on how many shuffle operations need to take place.\n\nA Spark JOB is internally represented as a DAG of stages. (Each Spark application may run as a series of Spark Jobs.)\n\nNot all Spark operations can happen in a single stage, so they may be divided into multiple stages.\n\nSTAGES -> **Stages** in Spark represent groups of tasks that can be executed together to compute the same operation on multiple machines. Spark will try to pack as much work as possible (i.e., as many transformations as possible inside your job) into the same stage, but the engine starts new stages after operations called shuffles. A shuffle represents a physical repartitioning of the data—for example, sorting a DataFrame, or grouping data that was loaded from a file by key. Spark starts a new stage after each shuffle, and keeps track of what order the stages must run in to compute the final result. Stages are created based on what operations can be performed serially or in parallel. They represent groups of tasks that can be executed together. \n\nThe **spark.sql.shuffle.partitions** default value is 200, which means that when there is a shuffle performed during execution, it outputs 200 shuffle partitions by default. You can change this value, and the number of output partitions will change. (The number of partitions should be set according to the number of cores in your cluster to ensure efficient execution.)\n\n`spark.conf.set(\"spark.sql.shuffle.partitions\",100)`\n\nA good rule of thumb is that the number of partitions should be larger than the number of executors on your cluster, potentially by multiple factors depending on the workload. If you are running code on your local machine, it would behoove you to set this value lower because your local machine is unlikely to be able to execute that number of tasks in parallel.\n\nTASKS -> Stages in Spark consist of **tasks**. Each task corresponds to a combination of blocks of data and a set of transformations that will run on a single executor. If there is one big partition in our dataset, we will have one task. If there are 1,000 little partitions, we will have 1,000 tasks that can be executed in parallel. A task is just a unit of computation applied to a unit of data (the partition). Partitioning your data into a greater number of partitions means that more can be executed in parallel."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5fa9a6dd-8624-45df-aa7a-9a8a11437cf0"}}},{"cell_type":"markdown","source":["<img src=\"https://www.researchgate.net/publication/332829490/figure/fig1/AS:754304478633984@1556851610122/Job-paths-in-the-Apache-Spark-cluster-12.png\" alt=\"spark\">"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9e33ede7-f43a-42b1-8c5a-2d4561182b00"}}},{"cell_type":"markdown","source":["The **Spark driver** is the node in which the spark application's main method runs to coordinate the spark application. It contains the SparkContext object. It is responsible for scheduling the execution of data by various worker nodes in cluster mode. The spark driver should be as close as possible to worker nodes for optimal performance. \n\n-> NOT: the driver is horizontally scaled to increase overall processing throughput.\n\nThe spark driver is the controller of the execution of a Spark Application and maintains all of the state of the Spark cluster (the state and tasks of the executors). It must interface with the cluster manager in order to actually get physical resources and launch executors. At the end of the day, this is just a\nprocess on a physical machine that is responsible for maintaining the state of the application running on the cluster.\n\nThe driver converts your Spark application into one or more Spark jobs. It then transforms each job into a DAG (Spark’s execution plan), where each node within a DAG could be single or multiple Spark stages.\n\nThe Spark driver has multiple roles: \n\n- It communicates with the cluster manager; \n- It requests resources (CPU, memory, etc.) from the cluster manager for Spark’s executors (JVMs); \n- It transforms all the Spark operations into DAG computations, schedules them, and distributes their execution as tasks across the Spark executors. Once the resources are allocated, it communicates directly with the executors.\n\nOnce the resources are allocated for the executors, the driver communicates directly with the executors running on the worker nodes.\n\nEvery Spark Application creates one driver and one or more executors at run time. Drivers and executors are never shared. Every application will have its own dedicated Spark Driver."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e56a3efb-f60b-4937-a403-a8869d193d0d"}}},{"cell_type":"markdown","source":["##### Cluster mode\n\n-> the spark driver runs in a worker node inside the cluster. In the cluster mode the cluster manager launches the driver process on a worker node inside the cluster, in addition to the executor processes. This means that the cluster manager is responsible for maintaining all Spark worker nodes. Therefore, the cluster manager places the driver on a worker node and the executors on separate worker nodes.\n\nWorker nodes are machines that host the executors responsible for the execution of tasks.\n\nCluster mode is probably the most common way of running Spark Applications. In cluster mode, a user submits a pre-compiled JAR, Python script, or R script to a cluster manager. The cluster manager then launches the driver process on a worker node inside the cluster, in addition to the executor processes. This means that the cluster manager is responsible for maintaining all Spark Application–related processes. \n\nThere is a single worker node that contains the spark driver and the executors. Spark application running in cluster mode runs in one of the worker nodes in the Spark cluster. Spark driver never does any data processing so you will have at least one executor running on some worker node. (False: Spark driver is alone running spark application).\n\n**Cluster manager** creates worker nodes and allocates resources to them.\nThe cluster manager is responsible for maintaining a cluster of machines that will run your Spark Application(s). Somewhat confusingly, a cluster manager will have its own “driver” (sometimes called master) and “worker” abstractions. The core difference is that these are tied to physical machines rather than processes (as they are in Spark). -> A cluster manager may have its own master and worker nodes. When it comes time to actually run a Spark Application, we request resources from the cluster manager to run it.\n\nThe Cluster manager allocates resources and keeps track of resources across application. It is an external service for acquiring resources on the cluster (e.g. standalone manager, Mesos, YARN, kubernetes). These resources are assigned as Spark worker or containers. The cluster manager starts the spark driver and allocates resources for the cluster of nodes on which your Spark application runs. Currently Spark supports 4 cluster managers: the built-in standalone cluster manager, apache hadoop YARN, apache mesos and kubernetes.\n\n**Executors are Java Virtual Machines (JVMs) running on a worker node.**\n\n-> Driver program will translate your custom logic into stages, job and task.. and your **application master** will make sure to get enough resources from RM and also make sure to check the status of your tasks running in a container.\n\nSpark executors are the processes that perform the tasks assigned by the Spark driver. Executors have one core responsibility: take the tasks assigned by the driver, run them, and report back their state (success or failure) and results. Each Spark Application has its own separate executor processes.\n\nWorker nodes are machines that host the executors responsible for the execution of tasks."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8aae2980-760a-41a0-9e6f-f8a3cca0ce43"}}},{"cell_type":"markdown","source":["##### Spark Deploy Modes\n\nSpark's execution/deployment mode determines where the driver and executors are physically located when a Spark application is run.\n\nSpark deployment mode (--deploy-mode) specifies where to run the driver program of your Spark application/job, Spark provides two deployment modes, client and cluster, you could use these to run Java, Scala, and PySpark applications.\n* Cluster: In cluster mode, the driver runs on one of the worker nodes, and this node shows as a driver on the Spark Web UI of your application. cluster mode is used to run production jobs. You can submit your applications in cluster mode using the spark-submit utility. Cluster mode runs the driver with the YARN Application Master. (YARN cluster supports client and cluster modes.) Kubernetes Cluster manager does not support deployment mode and by default runs in cluster mode.\n* Client: In client mode, the driver runs locally from where you are submitting your application using spark-submit command. Client mode is majorly used for interactive and debugging purposes. Note that in client mode only the driver runs locally and all tasks run on cluster worker nodes. Client mode is nearly the same as cluster mode except that the Spark driver remains on the client machine that submitted the application. This means that the client machine is responsible for maintaining the Spark driver process, and the cluster manager maintains the executor processses.\n\n\nIn Client mode, Spark runs driver in local machine, and in cluster mode, it runs driver on one of the nodes in the cluster.\n\n**Local Mode** is also known as Spark in-process is the default mode of spark. It does not require any resource manager. It runs everything on the same machine. Because of local mode, we are able to simply download spark and run without having to install any resource manager.\nLocal mode is a significant departure from the previous two modes: it runs the entire Spark Application on a single machine. It achieves parallelism through threads on that single machine. This is a common way to learn Spark, to test your applications, or experiment iteratively with local development. However, we do not recommend using local mode for running production applications. \n\nLocal mode runs the spark driver and executor in the same JVM on a single computer.\n\nThe only different between client and cluster mode is in client, mode driver will run on the machine where we have executed/run spark application/job and AM runs in one of the cluster nodes. In cluster mode driver run inside application master, it means the application has much more responsibility.\n\nEvery SPARK APPLICATION creates one driver and one or more executors at run time. Drivers and executors are never shared. Every application will have its own dedicated Spark Driver.\n\nYou can submit spark applications in client mode or in cluster mode. The cluster mode starts the driver on the Spark cluster, whereas client mode starts the driver on the client machine."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"63b9624b-5e13-4aeb-bdf7-d8ec8c15daea"}}},{"cell_type":"markdown","source":["##### Spark SQL\n\nSpark SQL is a module used for structured data processing with multiple interfaces. We can also interact with Spark sql using the dataframe API. The same query can be expressed with SQL and the DataFrame API. \n\nQuery plans (sql queries, python/scala dataframe API) -> Optimized query plan -> RDDs -> execution\n\nResilient Distributed Datasets (RDDs) are the low-level representation of datasets processed by a Spark cluster. In early versions of Spark, you had to write code manipulating RDDs directly. In modern versions of Spark you should instead use the higher-level DataFrame APIs, which Spark automatically compiles into low-level RDD operations."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5cdb6391-7ff4-4509-aaee-99c7b1acd234"}}},{"cell_type":"markdown","source":["##### DataFrames\n\nDataFrames and Datasets are (distributed) table-like collections with well-defined rows and columns. To Spark, DataFrames and Datasets represent immutable, lazily evaluated plans that specify what operations to apply to data residing at a location to generate some output. When we perform an action on a DataFrame, we instruct Spark to perform the actual transformations and return the result. Tables and views are basically the same thing as DataFrames. We just execute SQL against them instead of DataFrame code.\n\n\n* DATAFRAMES: is a distributed collection of data grouped into named columns. They are a data abstraction in spark that help us think about data in a familiar, tabular way.\n* SCHEMA is what defines the column names and types of a Dataframe.\n* Dataframe TRANSFORMATIONS are methods that return a new Dataframe and are lazily evaluated. (example: select, where and orderBy)\n* Dataframe ACTIONS are methods that trigger computation (example: count, collect, show, first, foreach). An action is needed to trigger the execution of any dataframe transformations.\n\nRecall that expressing our query using methods in the DataFrame API returns results in a DataFrame. \n\nTo create a new DataFrame or Dataset, you call a transformation. To start computation or convert to native language types, you call an action."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f59abbb4-2cdf-49ed-a870-4f822df86532"}}},{"cell_type":"markdown","source":["Spark allows you to use the notion of schema-on-read using the infer schema option. However, this approach may have some problems. Schema-on-read is recommended for ad hoc analysis and Spark SQL. Infer schema is slow and it often infers column types incorrectly. So the recommendation is to define your schema manually for production use cases."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"43e7a228-6615-4bcc-96a6-5b0a72cda669"}}},{"cell_type":"code","source":["# Access a dataframe's schema using the \"schema\" attribute.\n\nbudgetDF.schema\n\nbudgetDF.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9eb9526e-dd7d-4480-99b0-6c981c15288a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["You can use the add() method to add new columns in your schema. However, you must be using `pyspark.sql.types.DataType`.\n\n`newSchema = mySchema.add(\"Department\", StringType())`\n\nMost of the time the schema infers the Date as a string (not date format)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cf33aad6-022a-4983-99b8-3260d49ec81b"}}},{"cell_type":"markdown","source":["##### DataFrame Action Methods\n| Method | Description |\n| --- | --- |\n| show | Displays the top n rows of DataFrame in a tabular form |\n| count | Returns the number of rows in the DataFrame |\n| describe,  summary | Computes basic statistics for numeric and string columns |\n| first, head | Returns the the first row |\n| collect | Returns an array that contains all rows in this DataFrame |\n| take | Returns an array of the first n rows in the DataFrame |"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bbbdbf3f-874d-4216-af5b-4235c60e6ec1"}}},{"cell_type":"markdown","source":["Another common task is to compute summary statistics for a column or set of columns. We can use the **describe** method to achieve exactly this. This will take all numeric columns and\ncalculate the count, mean, standard deviation, min, and max."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"deda99e9-49cd-49cd-899f-4ee8f174a25c"}}},{"cell_type":"code","source":["salesDF = spark.read.parquet(\"/mnt/training/ecommerce/sales/sales.parquet\")\nsalesDF.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e087bb70-a342-492e-a0c9-c7aaaba9fa5b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------+--------------------+---------------------+-------------------+-----------------------+------------+--------------------+\n|order_id|               email|transaction_timestamp|total_item_quantity|purchase_revenue_in_usd|unique_items|               items|\n+--------+--------------------+---------------------+-------------------+-----------------------+------------+--------------------+\n|  257437|kmunoz@powell-dur...|     1592194221828900|                  1|                 1995.0|           1|[{null, M_PREM_K,...|\n|  282611|bmurillo@hotmail.com|     1592504237604072|                  1|                  940.5|           1|[{NEWBED10, M_STA...|\n|  257448| bradley74@gmail.com|     1592200438030141|                  1|                  945.0|           1|[{null, M_STAN_F,...|\n|  257440|jameshardin@campb...|     1592197217716495|                  1|                 1045.0|           1|[{null, M_STAN_Q,...|\n|  283949| whardin@hotmail.com|     1592510720760323|                  1|                  535.5|           1|[{NEWBED10, M_STA...|\n+--------+--------------------+---------------------+-------------------+-----------------------+------------+--------------------+\nonly showing top 5 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------+--------------------+---------------------+-------------------+-----------------------+------------+--------------------+\n|order_id|               email|transaction_timestamp|total_item_quantity|purchase_revenue_in_usd|unique_items|               items|\n+--------+--------------------+---------------------+-------------------+-----------------------+------------+--------------------+\n|  257437|kmunoz@powell-dur...|     1592194221828900|                  1|                 1995.0|           1|[{null, M_PREM_K,...|\n|  282611|bmurillo@hotmail.com|     1592504237604072|                  1|                  940.5|           1|[{NEWBED10, M_STA...|\n|  257448| bradley74@gmail.com|     1592200438030141|                  1|                  945.0|           1|[{null, M_STAN_F,...|\n|  257440|jameshardin@campb...|     1592197217716495|                  1|                 1045.0|           1|[{null, M_STAN_Q,...|\n|  283949| whardin@hotmail.com|     1592510720760323|                  1|                  535.5|           1|[{NEWBED10, M_STA...|\n+--------+--------------------+---------------------+-------------------+-----------------------+------------+--------------------+\nonly showing top 5 rows\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["salesDF.describe().show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"791d0b75-c0a2-40fe-b462-bf6929488bd9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------+-----------------+-------------------+---------------------+-------------------+-----------------------+-------------------+\n|summary|         order_id|              email|transaction_timestamp|total_item_quantity|purchase_revenue_in_usd|       unique_items|\n+-------+-----------------+-------------------+---------------------+-------------------+-----------------------+-------------------+\n|  count|           210370|             210370|               210370|             210370|                 210370|             210370|\n|   mean|         362617.5|               null| 1.593088009692872...|  1.146166278461758|     1042.7902657223433| 1.1214098968484099|\n| stddev|60728.73240210701|               null| 4.600583796479087...| 0.3930559465510659|      494.5537027838925|0.35208445487528167|\n|    min|           257433|aacosta@hotmail.com|     1592181560601984|                  1|                   53.1|                  1|\n|    max|           467802|  zzuniga@quinn.com|     1593879294724110|                  5|                 5830.0|                  5|\n+-------+-----------------+-------------------+---------------------+-------------------+-----------------------+-------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------+-----------------+-------------------+---------------------+-------------------+-----------------------+-------------------+\n|summary|         order_id|              email|transaction_timestamp|total_item_quantity|purchase_revenue_in_usd|       unique_items|\n+-------+-----------------+-------------------+---------------------+-------------------+-----------------------+-------------------+\n|  count|           210370|             210370|               210370|             210370|                 210370|             210370|\n|   mean|         362617.5|               null| 1.593088009692872...|  1.146166278461758|     1042.7902657223433| 1.1214098968484099|\n| stddev|60728.73240210701|               null| 4.600583796479087...| 0.3930559465510659|      494.5537027838925|0.35208445487528167|\n|    min|           257433|aacosta@hotmail.com|     1592181560601984|                  1|                   53.1|                  1|\n|    max|           467802|  zzuniga@quinn.com|     1593879294724110|                  5|                 5830.0|                  5|\n+-------+-----------------+-------------------+---------------------+-------------------+-----------------------+-------------------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["If I want to extract the value for column \"email\" from the first row of the dataframe SalesDF:\n\n`salesDF.first().email`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f86151d0-15f8-4ceb-a983-aee92e62b07c"}}},{"cell_type":"markdown","source":["##### Spark session\n\nFirst step of any spark application: create a SPARK SESSION -> is the single entry point to all dataframe api functionality. (It's automatically created in a databricks notebook as the variable spark, so if I just write \"spark\" in databricks and execute I have info about spark session and I can also access to Spark UI, see below). \n\nIn a standalone Spark application, you can create a SparkSession using one of the high-level APIs in the programming language of your choice. In the Spark shell, the SparkSession is created for you, and you can access it via a global variable called spark or sc.\n\nspark session methods: \n* sql: returns a dataframe representing the result of the given query\n* table: returns the specified table as a dataframe\n* read: returns a dataframereader that can be used to read data in as a dataframe\n* range: create a dataframe with a columns containing elements in a range from start to end (exclusive) with step value and number of partitions\n* createDataFrame: creates a dataframe from a list of tuples, primarily used for testing\n\nIn Spark 2.0, the SparkSession became a unified conduit to all Spark operations and data. Not only did it subsume previous entry points to the Spark like the SparkContext, SQLContext, HiveContext, SparkConf, and StreamingContext, but it also made working with Spark simpler and easier.\nAlthough in Spark 2.x the SparkSession subsumes all other contexts, you can still access the individual contexts and their respective methods. In this way, the community maintained backward compatibility.\n\nSpark Session allows you to create JVM runtime parameters, define DataFrames and Datasets, read from data sources, access catalog metadata, and issue Spark SQL queries. SparkSession provides a single unified entry point to all of Spark’s functionality.\n\nA **SparkContext** object within the SparkSession represents the connection to the Spark cluster. This class is how you communicate with some of Spark’s lower-level APIs, such as RDDs. Through a SparkContext, you can create RDDs, accumulators, and broadcast variables, and you can run code on the cluster.\n\nA SparkContext object within the SparkSession represents the connection to the Spark cluster. This class is how you communicate with some of Spark’s lower-level APIs, such as RDDs.\n\n`spark = SparkSession\n   .builder\n   .appName(\"MySparkApplication\")\n   .getOrCreate()`\n   \n   \n`spark.scheduler.mode` This configuration sets the scheduling mode between jobs submitted to the same SparkContext."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7ebbd284-66dc-4e03-bdb4-5de57a75ec75"}}},{"cell_type":"markdown","source":["`spark-submit`-> it lets you send your application code to a cluster and launch it to execute there. Upon submission, the application will run until it exits (completes the task) or encounters an error.\nYou can do this with all of Spark’s support cluster managers including Standalone, Mesos, and YARN. The simplest example is running an application on your local machine. \n\nA way to specify Spark configurations directly in your Spark application or on the command line when submitting the application with spark-submit, using the --conf flag:\n\n`spark-submit --conf spark.sql.shuffle.partitions=5 --conf`\n\n\nTo avoid job failures due to resource starvation or gradual performance degradation, there are a handful of Spark configurations that you can enable or alter. These configurations affect three Spark components: the Spark driver, the executor, and the shuffle service running on the executor.\n\nSpark-submit useful to provide dependencies to the Spark application are:\n* --jars\n* --packages \n* --py-files\n\n(not useful: --files)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e0ce7825-4773-44a0-acfc-aa2229c3f4b4"}}},{"cell_type":"markdown","source":["##### Static versus dynamic resource allocation\n\nIf more resources are needed as tasks queue up in the driver due to a larger than anticipated workload, Spark cannot accommodate or allocate extra resources.\nIf instead you use Spark’s **dynamic resource allocation** configuration, the Spark driver can request more or fewer compute resources as the demand of large workloads flows and ebbs. In scenarios where your workloads are dynamic—that is, they vary in their demand for compute capacity using dynamic allocation helps to accommodate sudden peaks.\nBy default spark.dynamicAllocation.enabled is set to false.\n\n`spark.dynamicAllocation.enabled true`\n\nThis feature enables Spark driver to request more or fewer compute resources as the demand of large workloads flows.\n\n**Dynamic Allocation**  is used to scale up and down the number of executors dynamically based on the application's current number of pending tasks in a Spark cluster.\n\nThe following configurations are related to enabling dynamic adjustment of the resources for your application based on the workload:\n\n- `spark.dynamicAllocation.shuffleTracking.enabled`\n- `spark.dynamicAllocation.enabled`\n- `spark.shuffle.service.enabled`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6e21e6ea-8d1f-406d-ab77-f29295248077"}}},{"cell_type":"markdown","source":["**Permissive Mode** When it meets a corrupted record, puts the malformed string into a field configured by columnNameOfCorruptRecord, and sets malformed fields to null."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"42f7acf2-5385-4f43-90aa-807a10e2a57a"}}},{"cell_type":"code","source":["spark"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9fc21995-151a-44c3-8088-17d19a5cfdd7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=511229577565779#setting/sparkui/0617-070350-58wbx4rr/driver-5820759735915099498\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.2.1</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=511229577565779#setting/sparkui/0617-070350-58wbx4rr/driver-5820759735915099498\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.2.1</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "]}}],"execution_count":0},{"cell_type":"markdown","source":["You can use the SparkSession method `table` to create a DataFrame from the `products` table. Let's save this in the variable `productsDF`.\n\n`productsDF = spark.table(\"products\")`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c7afe4d0-a501-4441-adf0-9da9085e45b2"}}},{"cell_type":"markdown","source":["We can use `display()` to output the results of a dataframe.\n\n`display(budgetDF)`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8dff149d-dd36-44ba-9c57-d88f2b1c0790"}}},{"cell_type":"markdown","source":["##### Convert between DataFrames and SQL\n\n`createOrReplaceTempView` creates a temporary view based on the DataFrame. The lifetime of the temporary view is tied to the SparkSession that was used to create the DataFrame. Spark views are temporary and they disappear after your Spark application terminates.\n\n`budgetDF.createOrReplaceTempView(\"budget\")`\n\nYou can drop a Spark view in two ways.\n1. Using the DROP VIEW Spark SQL Expression. `spark.sql(\"DROP VIEW IF EXISTS my_view\"`\n2. Using the spark.catalog.dropTempView() `spark.catalog.dropTempView(\"my_view\")`\n\nHowever, do remember that temp view and global temp view are different. -> Global temp views are accessed via prefix global_temp\n\n`spark.read.table(\"global_temp.my_global_view\")`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f9872614-862c-4b78-8989-9fd018de0c0d"}}},{"cell_type":"markdown","source":["##### Data Sources\n\n* CSV, text file that use commas or other delimiters to separate values. There's an optional header.\n* Apache Parquet, a columnar storage format that provide compressed and efficient columnar data representation. Unlike csv, parquet allows you to load in only the columns you need since the value for a single record are not stored together. Schema is stored in a footer on the file, you don't need to infer it. Compression means you don't loose space storing missing values. \n* Delta lake: open source technology designed to be used with spark to build robust data lakes."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"80b12259-f582-4df6-9f8f-47caa2d8f2f0"}}},{"cell_type":"markdown","source":["##### DataFrame reader/writer\n\nDataFrameReader (interface used to load a dataframe from external storage systems)\n`spark.read.parquet(\"path/to/file\")`\n\nDataFrameReader is accessible through the SparkSession attribute `read`. This class includes methods to load DataFrames from different external storage systems.\n\nDataFrameWriter: write a dataframe to external storage systems\n`(df.write\n  .option(\"compression\", \"snappy\")\n  .mode(\"overwrite\")\n  .parquet(outPath)\n)`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cb651c50-bd3f-4f88-ad70-3c94c2ede221"}}},{"cell_type":"markdown","source":["##### DataFrame Reader"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9db46570-f8dd-4a18-91bb-f9c648e91c35"}}},{"cell_type":"markdown","source":["Spark DataFrameReader allows you to set mode configuration. DataFrameReader for the following behaviour is **permissive**: When it meets a corrupted record, puts the malformed string into a field configured by columnNameOfCorruptRecord, and sets malformed fields to null."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"37c948b4-6280-4b77-ad3d-c2fe6d5bad9f"}}},{"cell_type":"code","source":["# Read from CSV with the DataFrameReader's \"csv\" method and the following options: Tab separator, use first line as header, infer schema\n\nusersCsvPath = \"/mnt/training/ecommerce/users/users-500k.csv\"\n\nusersDF = (spark\n           .read\n           .option(\"sep\", \"\\t\")\n           .option(\"header\", True)\n           .option(\"inferSchema\", True)\n           .csv(usersCsvPath)\n          )\n\nusersDF.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d10bc2db-732e-4e01-8d50-d9cb8383f14a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- user_id: string (nullable = true)\n |-- user_first_touch_timestamp: long (nullable = true)\n |-- email: string (nullable = true)\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- user_id: string (nullable = true)\n |-- user_first_touch_timestamp: long (nullable = true)\n |-- email: string (nullable = true)\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["df = spark.read.format(\"csv\") \\\n    .option(\"inferSchema\", \"true\") \\\n    .option(\"header\", \"true\") \\\n    .option(\"sep\", \";\") \\\n    .load(\"data/my_data_file.txt\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3644ae97-6a43-4dad-af92-fd12d3054909"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Spark's Python API also allows you to specify the DataFrameReader options as parameters to the \"csv\" method\n\nusersDF = (spark\n           .read\n           .csv(usersCsvPath, sep=\"\\t\", header=True, inferSchema=True)\n          )\n\nusersDF.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"14449bee-73d3-4a1a-835d-bf7d0018b99b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- user_id: string (nullable = true)\n |-- user_first_touch_timestamp: long (nullable = true)\n |-- email: string (nullable = true)\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- user_id: string (nullable = true)\n |-- user_first_touch_timestamp: long (nullable = true)\n |-- email: string (nullable = true)\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# Manually define the schema by creating a \"StructType\" with column names and data types\n\nfrom pyspark.sql.types import LongType, StringType, StructType, StructField\n\n# from pyspark.sql.types import ArrayType, DoubleType, IntegerType, LongType, StringType, StructType, StructField\n\nuserDefinedSchema = StructType([\n    StructField(\"user_id\", StringType(), True),\n    StructField(\"user_first_touch_timestamp\", LongType(), True),\n    StructField(\"email\", StringType(), True)\n])\n\n# Read from CSV using this user-defined schema instead of inferring the schema\n\nusersDF = (spark\n           .read\n           .option(\"sep\", \"\\t\")\n           .option(\"header\", True)\n           .schema(userDefinedSchema)\n           .csv(usersCsvPath)\n          )\n\n# Alternatively, define the schema using data definition language (DDL) syntax.\n\nDDLSchema = \"user_id string, user_first_touch_timestamp long, email string\"\n\nusersDF = (spark\n           .read\n           .option(\"sep\", \"\\t\")\n           .option(\"header\", True)\n           .schema(DDLSchema)\n           .csv(usersCsvPath)\n          )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"82c59320-8001-413a-9d92-f50cb4431db1"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**DDL expressions** will create Spark Database objects which are stored in the meta store. Spark by default uses hive meta store. So you must enable hive support and include hive dependencies for using DDL expressions.\n\nYou must use the `enableHiveSupport()` while creating your Spark Session. Without hive support, you cannot run Spark DDL expressions."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"725bf7b1-8011-453e-a8b8-38a4f4b14f9a"}}},{"cell_type":"code","source":["# Read from JSON files \n\neventsJsonPath = \"/mnt/training/ecommerce/events/events-500k.json\"\n\neventsDF = (spark\n            .read\n            .option(\"inferSchema\", True)\n            .json(eventsJsonPath)\n           )\n\neventsDF.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5bdef4e1-91ad-455e-af06-a379aaf5bb80"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- device: string (nullable = true)\n |-- ecommerce: struct (nullable = true)\n |    |-- purchase_revenue_in_usd: double (nullable = true)\n |    |-- total_item_quantity: long (nullable = true)\n |    |-- unique_items: long (nullable = true)\n |-- event_name: string (nullable = true)\n |-- event_previous_timestamp: long (nullable = true)\n |-- event_timestamp: long (nullable = true)\n |-- geo: struct (nullable = true)\n |    |-- city: string (nullable = true)\n |    |-- state: string (nullable = true)\n |-- items: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- coupon: string (nullable = true)\n |    |    |-- item_id: string (nullable = true)\n |    |    |-- item_name: string (nullable = true)\n |    |    |-- item_revenue_in_usd: double (nullable = true)\n |    |    |-- price_in_usd: double (nullable = true)\n |    |    |-- quantity: long (nullable = true)\n |-- traffic_source: string (nullable = true)\n |-- user_first_touch_timestamp: long (nullable = true)\n |-- user_id: string (nullable = true)\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- device: string (nullable = true)\n |-- ecommerce: struct (nullable = true)\n |    |-- purchase_revenue_in_usd: double (nullable = true)\n |    |-- total_item_quantity: long (nullable = true)\n |    |-- unique_items: long (nullable = true)\n |-- event_name: string (nullable = true)\n |-- event_previous_timestamp: long (nullable = true)\n |-- event_timestamp: long (nullable = true)\n |-- geo: struct (nullable = true)\n |    |-- city: string (nullable = true)\n |    |-- state: string (nullable = true)\n |-- items: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- coupon: string (nullable = true)\n |    |    |-- item_id: string (nullable = true)\n |    |    |-- item_name: string (nullable = true)\n |    |    |-- item_revenue_in_usd: double (nullable = true)\n |    |    |-- price_in_usd: double (nullable = true)\n |    |    |-- quantity: long (nullable = true)\n |-- traffic_source: string (nullable = true)\n |-- user_first_touch_timestamp: long (nullable = true)\n |-- user_id: string (nullable = true)\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["%scala\n\n// You can use the `StructType` Scala method `toDDL` to have a DDL-formatted string created for you. In a Python notebook, create a Scala cell to create the string to copy and paste.\n\nspark.read.parquet(\"/mnt/training/ecommerce/events/events.parquet\").schema.toDDL"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6bf10db2-95b5-4631-b63d-1cbafb441c76"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">res0: String = device STRING,ecommerce STRUCT&lt;purchase_revenue_in_usd: DOUBLE, total_item_quantity: BIGINT, unique_items: BIGINT&gt;,event_name STRING,event_previous_timestamp BIGINT,event_timestamp BIGINT,geo STRUCT&lt;city: STRING, state: STRING&gt;,items ARRAY&lt;STRUCT&lt;coupon: STRING, item_id: STRING, item_name: STRING, item_revenue_in_usd: DOUBLE, price_in_usd: DOUBLE, quantity: BIGINT&gt;&gt;,traffic_source STRING,user_first_touch_timestamp BIGINT,user_id STRING\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">res0: String = device STRING,ecommerce STRUCT&lt;purchase_revenue_in_usd: DOUBLE, total_item_quantity: BIGINT, unique_items: BIGINT&gt;,event_name STRING,event_previous_timestamp BIGINT,event_timestamp BIGINT,geo STRUCT&lt;city: STRING, state: STRING&gt;,items ARRAY&lt;STRUCT&lt;coupon: STRING, item_id: STRING, item_name: STRING, item_revenue_in_usd: DOUBLE, price_in_usd: DOUBLE, quantity: BIGINT&gt;&gt;,traffic_source STRING,user_first_touch_timestamp BIGINT,user_id STRING\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["-> The default date format for JSON and CSV data files in Spark is yyyy-MM-dd"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8ac8a04c-a944-41b3-8008-f0a1a60099a1"}}},{"cell_type":"markdown","source":["The correct format for connecting to any JDBC compliant data source such as Oracle (example: connect to an Oracle database and read a table into your spark dataframe), MySQL, PostgreSQL is format(\"jdbc\")."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e5b6a979-fe7c-4328-b0fe-93bd9ae7dc9a"}}},{"cell_type":"markdown","source":["##### DataFrame Writer"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a899bde8-98ac-4798-89e2-fc0e81936de3"}}},{"cell_type":"code","source":["# Parquet method with configuration: Snappy compression, overwrite mode\n\nusersOutputPath = workingDir + \"/users.parquet\"\n\n(usersDF\n .write\n .option(\"compression\", \"snappy\")\n .mode(\"overwrite\")\n .parquet(usersOutputPath)\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b883a286-1f6c-49a7-a8c2-7690b26a16af"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["The snappy is the default compression format for the parquet file."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"26d90484-1540-447d-9f97-be73b4d0d3e2"}}},{"cell_type":"code","source":["# As with DataFrameReader, Spark's Python API also allows you to specify the DataFrameWriter options as parameters to the \"parquet\" method\n\n(usersDF\n .write\n .parquet(usersOutputPath, compression=\"snappy\", mode=\"overwrite\")\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b5b8d5a3-c873-4c21-9626-d863fb2b3e6a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["storesDF.write.parquet(filePath)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8ea8306e-fe21-45a5-a83e-c7adcb43374b"}}},{"cell_type":"markdown","source":["- append: Append contents of this DataFrame to existing data.\n- overwrite: Overwrite existing data.\n- error: Throw an exception if data already exists."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"45f51339-9437-47fd-82e4-da43943e9073"}}},{"cell_type":"markdown","source":["There is no `repartition()` operation for DataFrameWriter — **the partitionBy()** operation should be used instead."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e2cb031d-b2ab-45f8-9944-df3e716d2cf2"}}},{"cell_type":"markdown","source":["`df1.write.mode(\"overwrite\").option(\"path\", \"data/flights_delay.csv\").save()` The code block will save the DataFrame in parquet file format.\n\nDataFrame writer default format is the parquet file. So the data will be saved in the parquet file format. The path option specifies the directory location for the data files. So Spark will save parquet data files in the data/flights_delay.csv directory.\n\n`df.write.mode(\"overwrite\").save(\"data/myTable\")`\n\nFor saving a DataFrame in parquet format, you must call the parquet() or save() method. There is no method such as path(). The default format is parquet so you can skip setting the format() for saving data in parquet format."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2d846e25-8923-4b67-9fb0-6c917fe1b31c"}}},{"cell_type":"markdown","source":["##### Write DataFrame to table"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6f95e20f-f154-40cd-ad3c-b3f5e3b10df1"}}},{"cell_type":"code","source":["# Write \"eventsDF\" to a table using the DataFrameWriter method \"saveAsTable\"\n\n# This creates a global table, unlike the local view created by the DataFrame method \"createOrReplaceTempView\"\n\neventsDF.write.mode(\"overwrite\").saveAsTable(\"events_p\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7917cb45-046c-4eef-b847-4b46f7b46fdc"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##### Delta lake \n\nIn almost all cases, the best practice is to use Delta Lake format, especially whenever the data will be referenced from a Databricks workspace. Delta Lake is an open source technology designed to work with Spark to bring reliability to data lakes.\n\nFeatures:\n- ACID transactions\n- Scalable metadata handline\n- Unified streaming and batch processing\n- Time travel (data versioning)\n- Schema enforcement and evolution\n- Audit history\n- Parquet format\n- Compatible with Apache Spark API"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"75049a90-a966-404e-8c64-86a14626ac10"}}},{"cell_type":"code","source":["eventsOutputPath = workingDir + \"/delta/events\"\n\n(eventsDF\n .write\n .format(\"delta\")\n .mode(\"overwrite\")\n .save(eventsOutputPath)\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e59dd7f4-ccd2-4909-8245-f7f72eba06a0"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["By default Spark is case insensitive; however, you can make Spark case sensitive by setting the configuration:\n`-- in SQL`\n`set spark.sql.caseSensitive true`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4ef8aaf8-8b0c-4bd8-9912-2d561c37f8fe"}}},{"cell_type":"markdown","source":["##### Columns and expressions \n\nYou can select, manipulate and remove columns from dataframes and these operations are represented using expressions. \nA **COLUMN** is a logical construction that will be computed based on the data in a dataframe using an expression. Columns can only be transformed within the context of a dataframe. \n\nHow to refer to a column:\n- df[\"columnName\"]\n- df.columnName\n- col(\"columnName\")\n- col(\"columnName.field\")\n\nHowever, do not forget to place the column name in double-quotes.\n\n(In scala also $)\n\nCreate columns from an expression: \n- col(\"a\") + col(\"b\")  \n- col(\"a\").cast(\"int\") * 100\n\n--> from pyspark.sql.functions import col"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"637ab4d4-22dc-4cc7-ab0a-1a5c279ebd35"}}},{"cell_type":"markdown","source":["An expression is a set of transformations on one or more values in a record in a DataFrame. Think of it like a function that takes as input one or more column names, resolves them, and then potentially applies more expressions to create a single value for each record in the dataset.\n\nIn the simplest case, an expression, created via the expr function, is just a DataFrame column reference. In the simplest case, expr(\"someCol\") is equivalent to col(\"someCol\").\n\nIf you use col() and want to perform transformations on that column, you must perform those on that column reference. When using an expression, the expr function can actually parse transformations and column references from a string and can subsequently be passed into further transformations. \n\nExample: `expr(\"someCol - 5\")` is the same transformation as performing `col(\"someCol\") - 5`, or even `expr(\"someCol\") - 5`.\n\n`(((col(\"someCol\") + 5) * 200) - 6) < col(\"otherCol\")`\n\n`expr(\"(((someCol + 5) * 200) - 6) < otherCol\")`\n\n-----\n\n`df.select(\"name\", expr(\"salary * 0.20 as increment\"))` -> Expression to create an alias."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"77e2b59a-2fe3-43f7-be7b-ba2af55ceb06"}}},{"cell_type":"markdown","source":["##### Column Operators and Methods\n| Method | Description |\n| --- | --- |\n| \\*, + , <, >= | Math and comparison operators |\n| ==, != | Equality and inequality tests (Scala operators are `===` and `=!=`) |\n| alias | Gives the column an alias |\n| cast, astype | Casts the column to a different data type |\n| isNull, isNotNull, isNan | Is null, is not null, is NaN |\n| asc, desc | Returns a sort expression based on ascending/descending order of the column |"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b8658e2e-a4cf-4822-96fc-c07b06ab9987"}}},{"cell_type":"markdown","source":["How to chain two conditions -> A valid answer would be &. Operators like && or and are not valid. Other boolean operators that would be valid in Spark are | and ~."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2ce34bd2-2ca1-4f2f-b2dd-438d7ae98e56"}}},{"cell_type":"markdown","source":["##### Working with Nulls\n\nYou have coalesce() and ifnull() as valid approaches to replace null values. The coalesce() is available as a DataFrame function as well as a Spark SQL function. However, ifnull() is a SparkSQL function and it is not available as a DataFrame function so it does not work as a DataFrame function.\n\n`df.withColumn(\"Year\", expr(\"coalesce(Year, '2021')\"))`\n\n`df.withColumn(\"Year\", coalesce(col(\"Year\"), lit(\"2021\")))`\n\n`df.withColumn(\"Year\", expr(\"ifnull(Year, '2021')\"))`\n\nWRONG ->`df.withColumn(\"Year\", ifnull(col(\"Year\"), \"2021\"))`\n\n**coalesce()**  allows you to select the first non-null value from a set of columns by using the coalesce function.\n`from pyspark.sql.functions import coalesce\ndf.select(coalesce(col(\"Description\"), col(\"CustomerId\"))).show()\n`\n\n`df.withColumn(\"Year\", coalesce(col(\"Year\"), lit(\"2021\")))` You must use the lit() function to add a literal value for a column.\n\n**ifnull** allows you to select the second value if the first is null, and defaults to the first. Alternatively, you could use **nullif**, which returns null if the two values are equal or else returns the second if they are not. **nvl** returns the second value if the first is null, but defaults to the first. Finally, **nvl2** returns the second value if the first is not null; otherwise, it will return the last specified value."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e528845f-fa03-4854-9ca3-e0ba86c6392c"}}},{"cell_type":"markdown","source":["**isnull()**\n\nThe isnull() function takes a single argument and returns true if expr is null, or false otherwise. So using isnull() is incorrect. The ifnull() is a valid SQL function for the purpose. However, you do not need lit() in Spark SQL. In fact, we do not have a lit() function in Spark SQL because it is not needed.\n\n`df.withColumn(\"Year\", expr(\"ifnull(Year, '2021')\"))`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"043100ef-db30-47fc-a227-9f7204745303"}}},{"cell_type":"markdown","source":["Example: \n\n`df.withColumn(\"count2\", col(\"count\").cast(\"long\"))`\n\n`df.withColumn(\"salary\", col(\"salary\").cast(\"double\"))` -> remember \"double\", \"long\" with \" \"\n\nYou can also use Spark SQL functions such as INT(), DOUBLE(), DATE(), etc to cast a value. `df.select(\"name\", expr(\"INT(age)\"), expr(\"DOUBLE(salary)\"))`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b37505cc-4ec2-437a-bcad-43b3ca01e1c0"}}},{"cell_type":"markdown","source":["Like for asc(), asc_null_last() does not take any argument, but is applied to column to return a sort expression based on ascending order of the column, and null values appear after non-null values. -> `df.orderBy(col(\"created_date\").asc_nulls_last())`\n\n* asc_nulls_first(columnName: String): Column - Similar to asc function but null values return first and then non-null values.\n* asc_nulls_last(columnName: String): Column - Similar to asc function but non-null values return first and then null values.\n\n* desc_nulls_first(columnName: String): Column - Similar to desc function but null values return first and then non-null values.\n* desc_nulls_last(columnName: String): Column - Similar to desc function but non-null values return first and then null values.\n\nFor optimization purposes, it’s sometimes advisable to sort within each partition before another set of transformations. You can use the sortWithinPartitions method to do this:\n`spark.read.format(\"json\").load(\"/data/flight-data/json/*-summary.json\")\\\n.sortWithinPartitions(\"count\")`\n\nWe do not have a Spark SQL function for desc_nulls_first() so the following option is incorrect.\n`df.orderBy(expr(\"desc_nulls_first(Year)\"))`\n\nThe following expression looks correct but it does not work correctly in PySpark. `expr(\"salary desc\")`\nThe expr(\"salary\").desc() is equivalent to col(\"salary\").desc() so both the options are correct."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"51f8f171-469e-43d1-9a43-ed34941def3b"}}},{"cell_type":"markdown","source":["##### DataFrame Transformation Methods\n| Method | Description |\n| --- | --- |\n| select | Returns a new DataFrame by computing given expression for each element |\n| drop | Returns a new DataFrame with a column dropped |\n| withColumnRenamed | Returns a new DataFrame with a column renamed |\n| withColumn | Returns a new DataFrame by adding a column or replacing the existing column that has the same name |\n| filter, where | Filters rows using the given condition |\n| sort, orderBy | Returns a new DataFrame sorted by the given expressions |\n| dropDuplicates, distinct | Returns a new DataFrame with duplicate rows removed |\n| limit | Returns a new DataFrame by taking the first n rows |\n| groupBy | Groups the DataFrame using the specified columns, so we can run aggregation on them |"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c712dfb9-b7c8-4849-aaf6-5c6fc8635801"}}},{"cell_type":"markdown","source":["**drop_duplicates(subset=None)** is an alias for dropDuplicates()."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7b7c5ba4-4a0a-45eb-82ce-c05e16a1e7d6"}}},{"cell_type":"markdown","source":["dropna(how='any', thresh=None, subset=None)\n\n**thresh** – int, default None If specified, drop rows that have less than thresh non-null values. This overwrites the how parameter.\n\n`df.na.drop(thresh=1)` -> Means keep the row if at least one column is not null."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0b0381ad-6a4e-4728-9f20-2c1d866c2d97"}}},{"cell_type":"markdown","source":["The rarely used **between()** method. It exists and resolves to ((storeId >= 20) AND (storeId <= 30)) in SQL."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f31acf43-28c8-495c-8471-fa7ecc51451e"}}},{"cell_type":"markdown","source":["Example:\n\n`df.filter(col(\"count\") < 2).show(2)`\n`df.where(\"count < 2\").show(2)`\n\n\n`df.where(col(\"InvoiceNo\") != 536365)\\\n.select(\"InvoiceNo\", \"Description\")\\\n.show(5, False)`\n\n-> Cleaner way: \n`df.where(\"InvoiceNo = 536365\")\n.show(5, false)`\n`df.where(\"InvoiceNo <> 536365\")\n.show(5, false)`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"138b52d4-117d-42b9-8e23-ace024ef2fab"}}},{"cell_type":"markdown","source":["df.where(\"salary > 5000\") -> equivalent to:\n\ndf.where(expr(\"salary > 5000\"))\n\ndf.filter(col(\"salary\") > 5000)\n\ndf.filter(expr(\"salary > 5000\"))\n\n-!- df.filter(\"salary\" > 5000) is WRONG\n\n-----\n\ndf.where(\"salary > 5000 and age > 30\") -> equivalent to:\n\ndf.filter((df.salary > 5000) & (df.age > 30))\n\ndf.filter(\"salary > 5000\").filter(\"age > 30\")\n\ndf.filter(col(\"salary\") > 5000) & (col (\"age\") > 30) You must apply the col() to convert it to a column expression. If not python will try comparing a string \"salary\" with the number 5000."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b1366284-6d6b-4835-9578-cb1fcdd44e56"}}},{"cell_type":"markdown","source":["**selectExpr** method when you’re working with expressions in strings. \n\n`df.select(expr(\"DEST_COUNTRY_NAME as destination\").alias(\"DEST_COUNTRY_NAME\"))\\\n.show(2)`\n`df.selectExpr(\"DEST_COUNTRY_NAME as newColumnName\", \"DEST_COUNTRY_NAME\").show(2)`\n\nIn the next example, we’ll set a Boolean flag for when the origin country is the same as the destination country:\n`df.withColumn(\"withinCountry\", expr(\"ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME\"))\\\n.show(2)`\n\n\n`df.selectExpr(\"name\", \"case when (salary < 5000) then salary * 0.20 else 0 end as increment\")`\n\nThe select() method does not accept column expressions. You must use  selectExpr() or wrap your column expressions into expr(). Spark if/else expression is syntactically incorrect. The case statement is syntactically correct."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5330efd7-f22a-427d-ba21-0a0e8cef144d"}}},{"cell_type":"markdown","source":["**LIMIT**\n\n`df.limit(100).where(\"salary > 4000\")` This first line of code will limit 100 records and then apply the filter condition.\n\n`df.where(\"salary > 4000\").limit(100)` This second line of code will first apply the filter and then limit the results to 100 records.\n\nThe limit and where clauses will be applied in the same order as specified in the code. Reordering the limit() clause is not an optimization but it will be logically incorrect. Hence, Spark does not try to push down the limit clause."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0c4b6486-3880-4ffe-bc79-d2c55da87455"}}},{"cell_type":"markdown","source":["When we call **groupBy**, we end up with a **RelationalGroupedDataset**, which is a fancy name for a DataFrame that has a grouping specified but needs the user to specify an aggregation before it can be queried further. We basically specified that we’re going to be grouping by a key (or set of keys) and that now we’re going to perform an aggregation over each one of those keys.\n\n**rollup()** Calculates subtotals and a grand total over (ordered) combination of groups. Beside cube and rollup multi-dimensional aggregate operators. \n\n`df.rollup(\"Year\", \"CourseName\"),agg(expr(\"sum(Students)\").alias(\"TotalStudents\")).orderBy(\"Year\", \"CourseName\")`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ce93b538-bf35-46fa-a6b5-4a23f76d5c3f"}}},{"cell_type":"markdown","source":["**Random Samples** \nSometimes, you might just want to sample some random records from your DataFrame. You can do this by using the sample method on a DataFrame, which makes it possible for you to specify\na fraction of rows to extract from a DataFrame and whether you’d like to sample with or without replacement:\n\n`seed = 5\nwithReplacement = False\nfraction = 0.5\ndf.sample(withReplacement, fraction, seed).count()`\n\n**sample()** \nPySpark sampling (pyspark.sql.DataFrame.sample()) is a mechanism to get random sample records from the dataset, this is helpful when you have a larger dataset and wanted to analyze/test a subset of the data for example 10% of the original file.\n\nsample(withReplacement, fraction, seed=None)\n* withReplacement – Sample with replacement or not (default False). Some times you may need to get a random sample with repeated values. By using the value true, results in repeated values.\n* fraction – Fraction of rows to generate, range [0.0, 1.0]. Note that it doesn’t guarantee to provide the exact number of the fraction of records. (By using fraction between 0 to 1, it returns the approximate number of the fraction of the dataset. For example, 0.1 returns 10% of the rows. However, this does not guarantee it returns the exact 10% of the records.)\n* seed – Seed for sampling (default a random seed). Used to reproduce the same random sampling. Every time you run a sample() function it returns a different set of sampling records, however sometimes during the development and testing phase you may need to regenerate the same sample every time as you need to compare the results from your previous run. To get consistent same random sampling uses the same slice value for every run. Change slice value to get different results.\n`print(df.sample(0.1,123).collect())\n//Output: 36,37,41,43,56,66,69,75,83\nprint(df.sample(0.1,123).collect())\n//Output: 36,37,41,43,56,66,69,75,83\nprint(df.sample(0.1,456).collect())\n//Output: 19,21,42,48,49,50,75,80\n`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"00c6752e-7129-43bc-87b5-6e3150b80af9"}}},{"cell_type":"markdown","source":["**Random Splits**\nRandom splits can be helpful when you need to break up your DataFrame into a random “splits” of the original DataFrame. In this next example, we’ll split our DataFrame into two different\nDataFrames by setting the weights by which we will split the DataFrame (these are the arguments to the function). Because this method is designed to be randomized, we will also specify a seed (just replace seed with a number of your choosing in the code block). It’s important to note that if you don’t specify a proportion for each DataFrame that adds up to one, they will be normalized so that they do:\n\n`dataFrames = df.randomSplit([0.25, 0.75], seed)\ndataFrames[0].count() > dataFrames[1].count() # False\n`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1a6b744c-5d1c-409f-97af-b9bff6eb92e8"}}},{"cell_type":"markdown","source":["**Union() and UnionAll()**\n\nDataFrames are immutable. This means users cannot append to DataFrames because that would be changing it. To append to a DataFrame, you must union the original DataFrame along with the new DataFrame. This just concatenates the two DataFramess. To union two DataFrames, you must be sure that they have the same schema and number of columns; otherwise, the union will fail.\n\n* Dataframe union() – union() method of the DataFrame is used to combine two DataFrame’s of the same structure/schema. If schemas are not the same it returns an error.\n* DataFrame unionAll() – unionAll() is deprecated since Spark “2.0.0” version and replaced with union().\n\nNote: In other SQL’s, Union eliminates the duplicates but UnionAll combines two datasets including duplicate records. But, in spark both behave the same and use DataFrame duplicate function to remove duplicate rows. Union() does not remove duplicates, it works like union all in spark sql.\n\nUNION operation in Spark SQL combines two tables and also removes duplicates. In spark DataFrame API I have to use: `df3 = df1.union(df2).distinct()`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e6d60b51-1190-4c70-a99a-b0a5e7e5f72f"}}},{"cell_type":"markdown","source":["**monotonically_increasing_id()** A column that generates monotonically increasing 64-bit integers.\n\nThe generated ID is guaranteed to be monotonically increasing and unique, but not consecutive. The current implementation puts the partition ID in the upper 31 bits, and the record number within each partition in the lower 33 bits. The assumption is that the data frame has less than 1 billion partitions, and each partition has less than 8 billion records.\n\n`df.withColumn(\"ID\", monotonically_increasing_id())`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"387062b0-3917-46af-b2ce-97a62b27fb89"}}},{"cell_type":"code","source":["# \"selectExpr()\" Selects a list of SQL expressions\n\nappleDF = eventsDF.selectExpr(\"user_id\", \"device in ('macOS', 'iOS') as apple_user\")\nappleDF.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"51249151-ed4c-4b45-915b-774a906403b6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----------------+----------+\n|          user_id|apple_user|\n+-----------------+----------+\n|UA000000107379500|      true|\n|UA000000107359357|     false|\n|UA000000107375547|      true|\n|UA000000107370581|      true|\n|UA000000107377108|     false|\n+-----------------+----------+\nonly showing top 5 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----------------+----------+\n|          user_id|apple_user|\n+-----------------+----------+\n|UA000000107379500|      true|\n|UA000000107359357|     false|\n|UA000000107375547|      true|\n|UA000000107370581|      true|\n|UA000000107377108|     false|\n+-----------------+----------+\nonly showing top 5 rows\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# \"withColumn()\" Returns a new DataFrame by adding a column or replacing an existing column that has the same name.\n\nfrom pyspark.sql.functions import col\n\nmobileDF = eventsDF.withColumn(\"mobile\", col(\"device\").isin(\"iOS\", \"Android\"))\nmobileDF.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c7589358-09ac-4d4f-8859-c478a010e707"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------+------------------+----------+------------------------+----------------+-------------------+--------------------+--------------+--------------------------+-----------------+------+\n| device|         ecommerce|event_name|event_previous_timestamp| event_timestamp|                geo|               items|traffic_source|user_first_touch_timestamp|          user_id|mobile|\n+-------+------------------+----------+------------------------+----------------+-------------------+--------------------+--------------+--------------------------+-----------------+------+\n|  macOS|{null, null, null}|  warranty|        1593878899217692|1593878946592107|     {Montrose, MI}|                  []|        google|          1593878899217692|UA000000107379500| false|\n|Windows|{null, null, null}|     press|        1593876662175340|1593877011756535|  {Northampton, MA}|                  []|        google|          1593876662175340|UA000000107359357| false|\n|  macOS|{null, null, null}|  add_item|        1593878792892652|1593878815459100|      {Salinas, CA}|[{null, M_STAN_T,...|       youtube|          1593878455472030|UA000000107375547| false|\n|    iOS|{null, null, null}|mattresses|        1593878178791663|1593878809276923|      {Everett, MA}|                  []|      facebook|          1593877903116176|UA000000107370581|  true|\n|Windows|{null, null, null}|mattresses|                    null|1593878628143633|{Cottage Grove, MN}|                  []|        google|          1593878628143633|UA000000107377108| false|\n+-------+------------------+----------+------------------------+----------------+-------------------+--------------------+--------------+--------------------------+-----------------+------+\nonly showing top 5 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------+------------------+----------+------------------------+----------------+-------------------+--------------------+--------------+--------------------------+-----------------+------+\n| device|         ecommerce|event_name|event_previous_timestamp| event_timestamp|                geo|               items|traffic_source|user_first_touch_timestamp|          user_id|mobile|\n+-------+------------------+----------+------------------------+----------------+-------------------+--------------------+--------------+--------------------------+-----------------+------+\n|  macOS|{null, null, null}|  warranty|        1593878899217692|1593878946592107|     {Montrose, MI}|                  []|        google|          1593878899217692|UA000000107379500| false|\n|Windows|{null, null, null}|     press|        1593876662175340|1593877011756535|  {Northampton, MA}|                  []|        google|          1593876662175340|UA000000107359357| false|\n|  macOS|{null, null, null}|  add_item|        1593878792892652|1593878815459100|      {Salinas, CA}|[{null, M_STAN_T,...|       youtube|          1593878455472030|UA000000107375547| false|\n|    iOS|{null, null, null}|mattresses|        1593878178791663|1593878809276923|      {Everett, MA}|                  []|      facebook|          1593877903116176|UA000000107370581|  true|\n|Windows|{null, null, null}|mattresses|                    null|1593878628143633|{Cottage Grove, MN}|                  []|        google|          1593878628143633|UA000000107377108| false|\n+-------+------------------+----------+------------------------+----------------+-------------------+--------------------+--------------+--------------------------+-----------------+------+\nonly showing top 5 rows\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["locationDF = eventsDF.withColumnRenamed(\"geo\", \"location\")\nlocationDF.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f779bd88-0ea5-4f3f-9288-4632565cedaa"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------+------------------+----------+------------------------+----------------+-------------------+--------------------+--------------+--------------------------+-----------------+\n| device|         ecommerce|event_name|event_previous_timestamp| event_timestamp|           location|               items|traffic_source|user_first_touch_timestamp|          user_id|\n+-------+------------------+----------+------------------------+----------------+-------------------+--------------------+--------------+--------------------------+-----------------+\n|  macOS|{null, null, null}|  warranty|        1593878899217692|1593878946592107|     {Montrose, MI}|                  []|        google|          1593878899217692|UA000000107379500|\n|Windows|{null, null, null}|     press|        1593876662175340|1593877011756535|  {Northampton, MA}|                  []|        google|          1593876662175340|UA000000107359357|\n|  macOS|{null, null, null}|  add_item|        1593878792892652|1593878815459100|      {Salinas, CA}|[{null, M_STAN_T,...|       youtube|          1593878455472030|UA000000107375547|\n|    iOS|{null, null, null}|mattresses|        1593878178791663|1593878809276923|      {Everett, MA}|                  []|      facebook|          1593877903116176|UA000000107370581|\n|Windows|{null, null, null}|mattresses|                    null|1593878628143633|{Cottage Grove, MN}|                  []|        google|          1593878628143633|UA000000107377108|\n+-------+------------------+----------+------------------------+----------------+-------------------+--------------------+--------------+--------------------------+-----------------+\nonly showing top 5 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------+------------------+----------+------------------------+----------------+-------------------+--------------------+--------------+--------------------------+-----------------+\n| device|         ecommerce|event_name|event_previous_timestamp| event_timestamp|           location|               items|traffic_source|user_first_touch_timestamp|          user_id|\n+-------+------------------+----------+------------------------+----------------+-------------------+--------------------+--------------+--------------------------+-----------------+\n|  macOS|{null, null, null}|  warranty|        1593878899217692|1593878946592107|     {Montrose, MI}|                  []|        google|          1593878899217692|UA000000107379500|\n|Windows|{null, null, null}|     press|        1593876662175340|1593877011756535|  {Northampton, MA}|                  []|        google|          1593876662175340|UA000000107359357|\n|  macOS|{null, null, null}|  add_item|        1593878792892652|1593878815459100|      {Salinas, CA}|[{null, M_STAN_T,...|       youtube|          1593878455472030|UA000000107375547|\n|    iOS|{null, null, null}|mattresses|        1593878178791663|1593878809276923|      {Everett, MA}|                  []|      facebook|          1593877903116176|UA000000107370581|\n|Windows|{null, null, null}|mattresses|                    null|1593878628143633|{Cottage Grove, MN}|                  []|        google|          1593878628143633|UA000000107377108|\n+-------+------------------+----------+------------------------+----------------+-------------------+--------------------+--------------+--------------------------+-----------------+\nonly showing top 5 rows\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# \"filter()\" Filters rows using the given SQL expression or column based condition.\n\npurchasesDF = eventsDF.filter(\"ecommerce.total_item_quantity > 0\")\npurchasesDF.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fff4f99c-fe96-4906-9334-35d50a6533ec"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------+--------------+----------+------------------------+----------------+------------------+--------------------+--------------+--------------------------+-----------------+\n| device|     ecommerce|event_name|event_previous_timestamp| event_timestamp|               geo|               items|traffic_source|user_first_touch_timestamp|          user_id|\n+-------+--------------+----------+------------------------+----------------+------------------+--------------------+--------------+--------------------------+-----------------+\n|  Linux|{1195.0, 1, 1}|  finalize|        1593878893766134|1593878897648871|     {Shawnee, KS}|[{null, M_STAN_K,...|        google|          1593876996316576|UA000000107362263|\n|    iOS|{1045.0, 1, 1}|  finalize|        1593878485345763|1593878487460247|     {Detroit, MI}|[{null, M_STAN_Q,...|      facebook|          1593877230282722|UA000000107364432|\n|Android| {595.0, 1, 1}|  finalize|        1593877930076602|1593878966392505|{East Chicago, IN}|[{null, M_STAN_T,...|        google|          1593876889575474|UA000000107361347|\n|    iOS|{2290.0, 2, 2}|  finalize|        1593877650094042|1593877652106953|     {Warwick, RI}|[{null, M_PREM_F,...|        google|          1593876687337581|UA000000107359573|\n|  macOS| {945.0, 1, 1}|  finalize|        1593879151529456|1593879197837168|   {Boonville, MO}|[{null, M_STAN_F,...|      facebook|          1593878603312910|UA000000107376872|\n+-------+--------------+----------+------------------------+----------------+------------------+--------------------+--------------+--------------------------+-----------------+\nonly showing top 5 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------+--------------+----------+------------------------+----------------+------------------+--------------------+--------------+--------------------------+-----------------+\n| device|     ecommerce|event_name|event_previous_timestamp| event_timestamp|               geo|               items|traffic_source|user_first_touch_timestamp|          user_id|\n+-------+--------------+----------+------------------------+----------------+------------------+--------------------+--------------+--------------------------+-----------------+\n|  Linux|{1195.0, 1, 1}|  finalize|        1593878893766134|1593878897648871|     {Shawnee, KS}|[{null, M_STAN_K,...|        google|          1593876996316576|UA000000107362263|\n|    iOS|{1045.0, 1, 1}|  finalize|        1593878485345763|1593878487460247|     {Detroit, MI}|[{null, M_STAN_Q,...|      facebook|          1593877230282722|UA000000107364432|\n|Android| {595.0, 1, 1}|  finalize|        1593877930076602|1593878966392505|{East Chicago, IN}|[{null, M_STAN_T,...|        google|          1593876889575474|UA000000107361347|\n|    iOS|{2290.0, 2, 2}|  finalize|        1593877650094042|1593877652106953|     {Warwick, RI}|[{null, M_PREM_F,...|        google|          1593876687337581|UA000000107359573|\n|  macOS| {945.0, 1, 1}|  finalize|        1593879151529456|1593879197837168|   {Boonville, MO}|[{null, M_STAN_F,...|      facebook|          1593878603312910|UA000000107376872|\n+-------+--------------+----------+------------------------+----------------+------------------+--------------------+--------------+--------------------------+-----------------+\nonly showing top 5 rows\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# \"dropDuplicates()\" Returns a new DataFrame with duplicate rows removed, optionally considering only a subset of columns.\n# Alias: \"distinct\"\n\ndistinctUsersDF = eventsDF.dropDuplicates([\"user_id\"])\ndistinctUsersDF.show(5)\n\n\n# distinctDF = purchasesDF.select(\"event_name\").distinct()\n# distinctDF = purchasesDF.dropDuplicates([\"event_name\"])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a9937cf0-4eae-48e8-9082-1a6a7435e904"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------+------------------+-------------+------------------------+----------------+-----------------+--------------------+--------------+--------------------------+-----------------+\n| device|         ecommerce|   event_name|event_previous_timestamp| event_timestamp|              geo|               items|traffic_source|user_first_touch_timestamp|          user_id|\n+-------+------------------+-------------+------------------------+----------------+-----------------+--------------------+--------------+--------------------------+-----------------+\n|    iOS|{null, null, null}|     checkout|        1592547736518007|1592548321455992|  {San Bruno, CA}|[{NEWBED10, M_STA...|         email|          1592196947865522|UA000000102357807|\n|Android|{null, null, null}|     add_item|        1592573713168269|1592574347642610|     {Mobile, AL}|[{NEWBED10, M_STA...|         email|          1592198812458125|UA000000102358054|\n|  macOS|{null, null, null}|shipping_info|        1592545562314108|1592545941007576|      {Largo, FL}|[{NEWBED10, P_FOA...|         email|          1592199427202331|UA000000102358165|\n|    iOS|{null, null, null}|     register|        1592553755082252|1592558423806848|{Mounds View, MN}|[{NEWBED10, M_STA...|         email|          1592205037961396|UA000000102359895|\n|    iOS|{null, null, null}|     add_item|        1592583618219827|1592584060201694|  {Gibraltar, MI}|[{NEWBED10, M_STA...|         email|          1592205125802184|UA000000102359929|\n+-------+------------------+-------------+------------------------+----------------+-----------------+--------------------+--------------+--------------------------+-----------------+\nonly showing top 5 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------+------------------+-------------+------------------------+----------------+-----------------+--------------------+--------------+--------------------------+-----------------+\n| device|         ecommerce|   event_name|event_previous_timestamp| event_timestamp|              geo|               items|traffic_source|user_first_touch_timestamp|          user_id|\n+-------+------------------+-------------+------------------------+----------------+-----------------+--------------------+--------------+--------------------------+-----------------+\n|    iOS|{null, null, null}|     checkout|        1592547736518007|1592548321455992|  {San Bruno, CA}|[{NEWBED10, M_STA...|         email|          1592196947865522|UA000000102357807|\n|Android|{null, null, null}|     add_item|        1592573713168269|1592574347642610|     {Mobile, AL}|[{NEWBED10, M_STA...|         email|          1592198812458125|UA000000102358054|\n|  macOS|{null, null, null}|shipping_info|        1592545562314108|1592545941007576|      {Largo, FL}|[{NEWBED10, P_FOA...|         email|          1592199427202331|UA000000102358165|\n|    iOS|{null, null, null}|     register|        1592553755082252|1592558423806848|{Mounds View, MN}|[{NEWBED10, M_STA...|         email|          1592205037961396|UA000000102359895|\n|    iOS|{null, null, null}|     add_item|        1592583618219827|1592584060201694|  {Gibraltar, MI}|[{NEWBED10, M_STA...|         email|          1592205125802184|UA000000102359929|\n+-------+------------------+-------------+------------------------+----------------+-----------------+--------------------+--------------+--------------------------+-----------------+\nonly showing top 5 rows\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["eventsDF.distinct()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9ac18c68-c699-48d1-a9fe-e1fe50dbc0d3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[11]: DataFrame[device: string, ecommerce: struct<purchase_revenue_in_usd:double,total_item_quantity:bigint,unique_items:bigint>, event_name: string, event_previous_timestamp: bigint, event_timestamp: bigint, geo: struct<city:string,state:string>, items: array<struct<coupon:string,item_id:string,item_name:string,item_revenue_in_usd:double,price_in_usd:double,quantity:bigint>>, traffic_source: string, user_first_touch_timestamp: bigint, user_id: string]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[11]: DataFrame[device: string, ecommerce: struct<purchase_revenue_in_usd:double,total_item_quantity:bigint,unique_items:bigint>, event_name: string, event_previous_timestamp: bigint, event_timestamp: bigint, geo: struct<city:string,state:string>, items: array<struct<coupon:string,item_id:string,item_name:string,item_revenue_in_usd:double,price_in_usd:double,quantity:bigint>>, traffic_source: string, user_first_touch_timestamp: bigint, user_id: string]"]}}],"execution_count":0},{"cell_type":"markdown","source":["**Managed/unmanaged table**.\nWhen you define a table from files on disk, you are defining an unmanaged table.\nWhen you use saveAsTable on a DataFrame, you are instead creating a managed table for which Spark will track of all of the relevant information.\n\nIf you drop a managed table, both the data and the table definition will be removed. If you are dropping an unmanaged table, no data will be removed but you will no longer be able to refer to this data by the table name. Spark only manages metadata for unmanaged tables.\n\nManaged tables are stored in the Spark warehouse directory. The default location is determined by the `spark.sql.warehouse.dir` configuration in your Spark Session.\n\nSpark supports creating managed and unmanaged tables using Spark SQL and DataFrame APIs. \n\nAs soon as you add the ‘path’ option in the dataframe writer it will be treated as an external/unmanaged table.\n\n`df.write.option(\"path\", \"/data/\").saveAsTable(\"my_managed_table\")`\n\nThe default behavior of CREATE TABLE is to create a managed table. However, if you are setting a specific PATH, it becomes an external or unmanaged table. \n\n`spark.sql(\"\"\"CREATE TABLE flights_tbl(date STRING, delay INT, \n               distance INT, origin STRING, destination STRING) \n               USING csv OPTIONS (PATH '/tmp/flights/flights_tbl.csv')\"\"\")`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"af778ef4-2437-4fe3-9ae6-32147ebf75fb"}}},{"cell_type":"markdown","source":["In order to collect table-level statistic I use the expression `ANALYZE TABLE table_name COMPUTE STATISTICS`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"75a59c81-9360-4a60-a181-c1d05d4f8fb0"}}},{"cell_type":"markdown","source":["You can list the tables and other catalog objects using the `spark.catalog`\n\n`table_list = spark.catalog.listTables()`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"772b9a5e-9875-45fe-8240-9be6cec598bd"}}},{"cell_type":"markdown","source":["In this structure, the PersonalDetails is a child DataFrame inside a top-level root DataFrame. This approach is known as creating the **DataFrame of DataFrames**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f73b0c26-9768-4f2f-b3db-fdae555e6f57"}}},{"cell_type":"code","source":["root\n |-- ID: long (nullable = true)\n |-- PersonalDetails: struct (nullable = false)\n | |-- FName: string (nullable = true)\n | |-- LName: string (nullable = true)\n | |-- DOB: string (nullable = true)\n |-- Department: string (nullable = true)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5bc2513d-14c9-4fcd-bd67-d94d50c6cfc0"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"SparkCertification-DataFrames","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":1503221511970106}},"nbformat":4,"nbformat_minor":0}
