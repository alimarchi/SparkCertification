{"cells":[{"cell_type":"markdown","source":["#### Spark certification"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cb4eefed-4dcc-4189-aad5-c4fd0472ea47"}}},{"cell_type":"markdown","source":["##### Databricks platform\n\n* Magic commands: %python, %scala, %sql, %r, %sh, %md (%sh -> run shell commands on the driver)\n* Render HTML: displayHTML\n* DBFS = Databricks File System -> virtual file system, allows you to treat cloud object storage as though it were local files and directories on the cluster. You can run file system commands on DBFS using %fs. (example: %fs ls)\n* DBUtils: dbutils.fs (%fs), dbutils.notebook (%run), dbutils.widgets (example: dbutils.fs.ls(\"/databricks-datasets\"),\n%fs help)\n\nCREATE TABLE: You can use databricks sql commands to create a table:\n\n`%sql\nCREATE TABLE IF NOT EXISTS events USING parquet OPTIONS (path \"/mnt/training/ecommerce/events/events.parquet\");`\n\n* Databricks WIDGETS\nInput widgets allow you to add parameters to your notebooks and dashboards. The widget API consists of calls to create various types of input widgets, remove them, and get bound values. There are 4 types: text, dropdown, combobox, multiselect.\nDocumentation: dbutils.widgets.help()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f597a957-c138-4dc7-a203-49fd403a33f4"}}},{"cell_type":"markdown","source":["##### Spark overview\n\nSpark is standard unified analytics engine for big data processing. It's the largest open-source project in data processing.\nFast (faster than Hadoop), easy to use, unified.\nCore modules: spark SQL + dataframe, spark streaming, MLlib, Spark core API (all other functionality are builton top of it). \n\nSpark execution: it uses clusters, to distribute the work across different machines. The secret of spark performance is PARALLELISM, each parallelize action is refered to as a JOB. A job is broken down in STAGES. Then we have TASKS that depends on stages, created by the driver and assigned a partition of data to process. \n\nTASK: a combination of a block of data and a set of transformers that will run on a single executor. \n\nSTAGE: is a group of tasks that can be executed in parallel to compute the same set of operations on potentially multiple machines.\n\nComponents that spark use to coordinate work across a cluster of computers:\nDRIVER is the machine in which the application runs. It is responsible for 3 main things:\n* Maintaining information about spark application\n* Responding to the user's programm \n* Analyzing, distributing and scheduling work across the executors\n\nA WORKER node hosts the executor process. It has a fixed number of executors allocated at any point in time.\nEXECUTORS: each executor will hold a chunk of the data to be processed, this chunk is called a SPARK PARTITION. It's a collection of rows that sit on one physical machine in the cluster. -> this is completely separated from harddisk partitions which have to do a storage space on a hardrive. Executors are responsible for carrying out work assigned by a driver. Each executor is responsible for 2 things:\n* Execute the code assigned\n* Report the state of the computation back to the driver\n\nWorker nodes are fault-tolerant.\n\nSpark parallelize the work on 2 levels. One is split the work across the executors, the other is the SLOT. Each executors has a number of slots, each slot can be assigned a task. (Slot: CORE)\nSLOTS are resources for parallelization within a Spark application."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"61a43e80-31f3-49dd-8744-5ad5927596c9"}}},{"cell_type":"markdown","source":["In general, there should be one Spark **job** for one action. Actions always return results. Each job breaks down into a series of **stages**, the number of which depends on how many shuffle operations need to take place.\n\nSTAGES -> **Stages** in Spark represent groups of tasks that can be executed together to compute the same operation on multiple machines. Spark will try to pack as much work as possible (i.e., as many transformations as possible inside your job) into the same stage, but the engine starts new stages after operations called shuffles. A shuffle represents a physical repartitioning of the data—for example, sorting a DataFrame, or grouping data that was loaded from a file by key. Spark starts a new stage after each shuffle, and keeps track of what order the stages must run in to compute the final result.\n\nThe **spark.sql.shuffle.partitions** default value is 200, which means that when there is a shuffle performed during execution, it outputs 200 shuffle partitions by default. You can change this value, and the number of output partitions will change. (The number of partitions should be set according to the number of cores in your cluster to ensure efficient execution.)\n\nA good rule of thumb is that the number of partitions should be larger than the number of executors on your cluster, potentially by multiple factors depending on the workload. If you are running code on your local machine, it would behoove you to set this value lower because your local machine is unlikely to be able to execute that number of tasks in parallel.\n\nTASKS -> Stages in Spark consist of **tasks**. Each task corresponds to a combination of blocks of data and a set of transformations that will run on a single executor. If there is one big partition in our dataset, we will have one task. If there are 1,000 little partitions, we will have 1,000 tasks that can be executed in parallel. A task is just a unit of computation applied to a unit of data (the partition). Partitioning your data into a greater number of partitions means that more can be executed in parallel."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5fa9a6dd-8624-45df-aa7a-9a8a11437cf0"}}},{"cell_type":"markdown","source":["The **Spark driver** is the node in which the spark application's main method runs to coordinate the spark application. It contains the SparkContext object. It is responsible for scheduling the execution of data by various worker nodes in cluster mode. The spark driver should be as close as possible to worker nodes for optimal performance. \n\n-> NOT: the driver is horizontally scaled to increase overall processing throughput.\n\nThe spark driver is the controller of the execution of a Spark Application and maintains all of the state of the Spark cluster (the state and tasks of the executors). It must interface with the cluster manager in order to actually get physical resources and launch executors. At the end of the day, this is just a\nprocess on a physical machine that is responsible for maintaining the state of the application running on the cluster."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e56a3efb-f60b-4937-a403-a8869d193d0d"}}},{"cell_type":"markdown","source":["##### Cluster mode\n\n-> the spark driver runs in a worker node inside the cluster. In the cluster mode the cluster manager launches the driver process on a worker node inside the cluster, in addition to the executor processes. This means that the cluster manager is responsible for maintaining all Spark worker nodes. Therefore, the cluster manager places the driver on a worker node and the executors on separate worker nodes.\n\nWorker nodes are machines that host the executors responsible for the execution of tasks.\n\nCluster mode is probably the most common way of running Spark Applications. In cluster mode, a user submits a pre-compiled JAR, Python script, or R script to a cluster manager. The cluster manager then launches the driver process on a worker node inside the cluster, in addition to the executor processes. This means that the cluster manager is responsible for maintaining all Spark Application–related processes. \n\nThere is a single worker node that cotains the spark driver and the executors.\n\n**Cluster manager** creates worker nodes and allocates resources to them.\nThe cluster manager is responsible for maintaining a cluster of machines that will run your Spark Application(s). Somewhat confusingly, a cluster manager will have its own “driver” (sometimes called master) and “worker” abstractions. The core difference is that these are tied to physical machines rather than processes (as they are in Spark). When it comes time to actually run a Spark Application, we request resources from the cluster manager to run it.\n\n**Executors are Java Virtual Machines (JVMs) running on a worker node.**\n\nSpark executors are the processes that perform the tasks assigned by the Spark driver. Executors have one core responsibility: take the tasks assigned by the driver, run them, and report back their state (success or failure) and results. Each Spark Application has its own separate executor processes.\n\n\nWorker nodes are machines that host the executors responsible for the execution of tasks."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8aae2980-760a-41a0-9e6f-f8a3cca0ce43"}}},{"cell_type":"markdown","source":["##### Spark Deploy Modes\n\nSpark's execution/deployment mode determine swhere the driver and executors are physically located when a Spark application is run.\n\nSpark deployment mode (--deploy-mode) specifies where to run the driver program of your Spark application/job, Spark provides two deployment modes, client and cluster, you could use these to run Java, Scala, and PySpark applications.\n* Cluster: In cluster mode, the driver runs on one of the worker nodes, and this node shows as a driver on the Spark Web UI of your application. cluster mode is used to run production jobs.\n* Client: In client mode, the driver runs locally from where you are submitting your application using spark-submit command. Client mode is majorly used for interactive and debugging purposes. Note that in client mode only the driver runs locally and all tasks run on cluster worker nodes. Client mode is nearly the same as cluster mode except that the Spark driver remains on the client machine that submitted the application. This means that the client machine is responsible for maintaining the Spark driver process, and the cluster manager maintains the executor processses.\n\n\nIn Client mode, Spark runs driver in local machine, and in cluster mode, it runs driver on one of the nodes in the cluster.\n\n**Local Mode** is also known as Spark in-process is the default mode of spark. It does not require any resource manager. It runs everything on the same machine. Because of local mode, we are able to simply download spark and run without having to install any resource manager.\nLocal mode is a significant departure from the previous two modes: it runs the entire Spark Application on a single machine. It achieves parallelism through threads on that single machine. This is a common way to learn Spark, to test your applications, or experiment iteratively with local development. However, we do not recommend using local mode for running production applications."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"63b9624b-5e13-4aeb-bdf7-d8ec8c15daea"}}},{"cell_type":"markdown","source":["##### Spark SQL\n\nSpark SQL is a module used for structured data processing with multiple interfaces. We can also interact with Spark sql using the dataframe API. The same query can be expressed with SQL and the DataFrame API. \n\nQuery plans (sql queries, python/scala dataframe API) -> Optimized query plan -> RDDs -> execution\n\nResilient Distributed Datasets (RDDs) are the low-level representation of datasets processed by a Spark cluster. In early versions of Spark, you had to write code manipulating RDDs directly. In modern versions of Spark you should instead use the higher-level DataFrame APIs, which Spark automatically compiles into low-level RDD operations."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5cdb6391-7ff4-4509-aaee-99c7b1acd234"}}},{"cell_type":"markdown","source":["##### DataFrames\n\nDataFrames and Datasets are (distributed) table-like collections with well-defined rows and columns. To Spark, DataFrames and Datasets represent immutable, lazily evaluated plans that specify what operations to apply to data residing at a location to generate some output. When we perform an action on a DataFrame, we instruct Spark to perform the actual transformations and return the result. Tables and views are basically the same thing as DataFrames. We just execute SQL against them instead of DataFrame code.\n\n\n* DATAFRAMES: is a distributed collection of data grouped into named columns. They are a data abstraction in spark that help us think about data in a familiar, tabular way.\n* SCHEMA is what defines the column names and types of a Dataframe.\n* Dataframe TRANSFORMATIONS are methods that return a new Dataframe and are lazily evaluated. (example: select, where and orderBy)\n* Dataframe ACTIONS are methods that trigger computation (example: count, collect, show, first, foreach). An action is needed to trigger the execution of any dataframe transformations.\n\nRecall that expressing our query using methods in the DataFrame API returns results in a DataFrame. \n\nTo create a new DataFrame or Dataset, you call a transformation. To start computation or convert to native language types, you call an action."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f59abbb4-2cdf-49ed-a870-4f822df86532"}}},{"cell_type":"code","source":["# Access a dataframe's schema using the \"schema\" attribute.\n\nbudgetDF.schema\n\nbudgetDF.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9eb9526e-dd7d-4480-99b0-6c981c15288a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##### DataFrame Action Methods\n| Method | Description |\n| --- | --- |\n| show | Displays the top n rows of DataFrame in a tabular form |\n| count | Returns the number of rows in the DataFrame |\n| describe,  summary | Computes basic statistics for numeric and string columns |\n| first, head | Returns the the first row |\n| collect | Returns an array that contains all rows in this DataFrame |\n| take | Returns an array of the first n rows in the DataFrame |"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bbbdbf3f-874d-4216-af5b-4235c60e6ec1"}}},{"cell_type":"markdown","source":["Another common task is to compute summary statistics for a column or set of columns. We can use the **describe** method to achieve exactly this. This will take all numeric columns and\ncalculate the count, mean, standard deviation, min, and max."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"deda99e9-49cd-49cd-899f-4ee8f174a25c"}}},{"cell_type":"code","source":["salesDF = spark.read.parquet(\"/mnt/training/ecommerce/sales/sales.parquet\")\nsalesDF.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e087bb70-a342-492e-a0c9-c7aaaba9fa5b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------+--------------------+---------------------+-------------------+-----------------------+------------+--------------------+\n|order_id|               email|transaction_timestamp|total_item_quantity|purchase_revenue_in_usd|unique_items|               items|\n+--------+--------------------+---------------------+-------------------+-----------------------+------------+--------------------+\n|  257437|kmunoz@powell-dur...|     1592194221828900|                  1|                 1995.0|           1|[{null, M_PREM_K,...|\n|  282611|bmurillo@hotmail.com|     1592504237604072|                  1|                  940.5|           1|[{NEWBED10, M_STA...|\n|  257448| bradley74@gmail.com|     1592200438030141|                  1|                  945.0|           1|[{null, M_STAN_F,...|\n|  257440|jameshardin@campb...|     1592197217716495|                  1|                 1045.0|           1|[{null, M_STAN_Q,...|\n|  283949| whardin@hotmail.com|     1592510720760323|                  1|                  535.5|           1|[{NEWBED10, M_STA...|\n+--------+--------------------+---------------------+-------------------+-----------------------+------------+--------------------+\nonly showing top 5 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------+--------------------+---------------------+-------------------+-----------------------+------------+--------------------+\n|order_id|               email|transaction_timestamp|total_item_quantity|purchase_revenue_in_usd|unique_items|               items|\n+--------+--------------------+---------------------+-------------------+-----------------------+------------+--------------------+\n|  257437|kmunoz@powell-dur...|     1592194221828900|                  1|                 1995.0|           1|[{null, M_PREM_K,...|\n|  282611|bmurillo@hotmail.com|     1592504237604072|                  1|                  940.5|           1|[{NEWBED10, M_STA...|\n|  257448| bradley74@gmail.com|     1592200438030141|                  1|                  945.0|           1|[{null, M_STAN_F,...|\n|  257440|jameshardin@campb...|     1592197217716495|                  1|                 1045.0|           1|[{null, M_STAN_Q,...|\n|  283949| whardin@hotmail.com|     1592510720760323|                  1|                  535.5|           1|[{NEWBED10, M_STA...|\n+--------+--------------------+---------------------+-------------------+-----------------------+------------+--------------------+\nonly showing top 5 rows\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["salesDF.describe().show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"791d0b75-c0a2-40fe-b462-bf6929488bd9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------+-----------------+-------------------+---------------------+-------------------+-----------------------+-------------------+\n|summary|         order_id|              email|transaction_timestamp|total_item_quantity|purchase_revenue_in_usd|       unique_items|\n+-------+-----------------+-------------------+---------------------+-------------------+-----------------------+-------------------+\n|  count|           210370|             210370|               210370|             210370|                 210370|             210370|\n|   mean|         362617.5|               null| 1.593088009692872...|  1.146166278461758|     1042.7902657223433| 1.1214098968484099|\n| stddev|60728.73240210701|               null| 4.600583796479087...| 0.3930559465510659|      494.5537027838925|0.35208445487528167|\n|    min|           257433|aacosta@hotmail.com|     1592181560601984|                  1|                   53.1|                  1|\n|    max|           467802|  zzuniga@quinn.com|     1593879294724110|                  5|                 5830.0|                  5|\n+-------+-----------------+-------------------+---------------------+-------------------+-----------------------+-------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------+-----------------+-------------------+---------------------+-------------------+-----------------------+-------------------+\n|summary|         order_id|              email|transaction_timestamp|total_item_quantity|purchase_revenue_in_usd|       unique_items|\n+-------+-----------------+-------------------+---------------------+-------------------+-----------------------+-------------------+\n|  count|           210370|             210370|               210370|             210370|                 210370|             210370|\n|   mean|         362617.5|               null| 1.593088009692872...|  1.146166278461758|     1042.7902657223433| 1.1214098968484099|\n| stddev|60728.73240210701|               null| 4.600583796479087...| 0.3930559465510659|      494.5537027838925|0.35208445487528167|\n|    min|           257433|aacosta@hotmail.com|     1592181560601984|                  1|                   53.1|                  1|\n|    max|           467802|  zzuniga@quinn.com|     1593879294724110|                  5|                 5830.0|                  5|\n+-------+-----------------+-------------------+---------------------+-------------------+-----------------------+-------------------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["If I want to extract the value for column \"email\" from the first row of the dataframe SalesDF:\n\n`salesDF.first().email`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f86151d0-15f8-4ceb-a983-aee92e62b07c"}}},{"cell_type":"markdown","source":["##### Spark session\n\nFirst step of any spark application: create a SPARK SESSION -> is the single entry point to all dataframe api functionality. (It's automatically created in a databricks notebook as the variable spark, so if I just write \"spark\" in databricks and execute I have info about spark session and I can also access to Spark UI, see below). \n\nspark session methods: \n* sql: returns a dataframe representing the result of the given query\n* table: returns the specified table as a dataframe\n* read: returns a dataframereader that can be used to read data in as a dataframe\n* range: create a dataframe with a columns containing elements in a range from start to end (exclusive) with step value and number of partitions\n* createDataFrame: creates a dataframe from a list of tuples, primarily used for testing\n\nA **SparkContext** object within the SparkSession represents the connection to the Spark cluster. This class is how you communicate with some of Spark’s lower-level APIs, such as RDDs. Through a SparkContext, you can create RDDs, accumulators, and broadcast variables, and you can run code on the cluster."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7ebbd284-66dc-4e03-bdb4-5de57a75ec75"}}},{"cell_type":"markdown","source":["`spark-submit`-> it lets you send your application code to a cluster and launch it to execute there. Upon submission, the application will run until it exits (completes the task) or encounters an error.\nYou can do this with all of Spark’s support cluster managers including Standalone, Mesos, and YARN. The simplest example is running an application on your local machine. \n\nA way to specify Spark configurations directly in your Spark application or on the command line when submitting the application with spark-submit, using the --conf flag:\n\n`spark-submit --conf spark.sql.shuffle.partitions=5 --conf`\n\n\nTo avoid job failures due to resource starvation or gradual performance degradation, there are a handful of Spark configurations that you can enable or alter. These configurations affect three Spark components: the Spark driver, the executor, and the shuffle service running on the executor."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e0ce7825-4773-44a0-acfc-aa2229c3f4b4"}}},{"cell_type":"markdown","source":["##### Static versus dynamic resource allocation\n\nIf more resources are needed as tasks queue up in the driver due to a larger than anticipated workload, Spark cannot accommodate or allocate extra resources.\nIf instead you use Spark’s **dynamic resource allocation** configuration, the Spark driver can request more or fewer compute resources as the demand of large workloads flows and ebbs. In scenarios where your workloads are dynamic—that is, they vary in their demand for compute capacity using dynamic allocation helps to accommodate sudden peaks.\nBy default spark.dynamicAllocation.enabled is set to false.\n\n`spark.dynamicAllocation.enabled true`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6e21e6ea-8d1f-406d-ab77-f29295248077"}}},{"cell_type":"code","source":["spark"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9fc21995-151a-44c3-8088-17d19a5cfdd7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=511229577565779#setting/sparkui/0617-070350-58wbx4rr/driver-5820759735915099498\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.2.1</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=511229577565779#setting/sparkui/0617-070350-58wbx4rr/driver-5820759735915099498\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.2.1</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "]}}],"execution_count":0},{"cell_type":"markdown","source":["You can use the SparkSession method `table` to create a DataFrame from the `products` table. Let's save this in the variable `productsDF`.\n\n`productsDF = spark.table(\"products\")`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c7afe4d0-a501-4441-adf0-9da9085e45b2"}}},{"cell_type":"markdown","source":["We can use `display()` to output the results of a dataframe.\n\n`display(budgetDF)`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8dff149d-dd36-44ba-9c57-d88f2b1c0790"}}},{"cell_type":"code","source":["display(budgetDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"85914ee3-d697-49d6-afc7-df1d8f8088df"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##### Convert between DataFrames and SQL\n\n`createOrReplaceTempView` creates a temporary view based on the DataFrame. The lifetime of the temporary view is tied to the SparkSession that was used to create the DataFrame.\n\n`budgetDF.createOrReplaceTempView(\"budget\")`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f9872614-862c-4b78-8989-9fd018de0c0d"}}},{"cell_type":"markdown","source":["##### Data Sources\n\n* CSV, text file that use commas or other delimiters to separate values. There's an optional header.\n* Apache Parquet, a columnar storage format that provide compressed and efficient columnar data representation. Unlike csv, parquet allows you to load in only the columns you need since the value for a single record are not stored together. Schema is stored in a footer on the file, you don't need to infer it. Compression means you don't loose space storing missing values. \n* Delta lake: open source technology designed to be used with spark to build robust data lakes."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"80b12259-f582-4df6-9f8f-47caa2d8f2f0"}}},{"cell_type":"markdown","source":["##### DataFrame reader/writer\n\nDataFrameReader (interface used to load a dataframe from external storage systems)\n`spark.read.parquet(\"path/to/file\")`\n\nDataFrameReader is accessible through the SparkSession attribute `read`. This class includes methods to load DataFrames from different external storage systems.\n\nDataFrameWriter: write a dataframe to external storage systems\n`(df.write\n  .option(\"compression\", \"snappy\")\n  .mode(\"overwrite\")\n  .parquet(outPath)\n)`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cb651c50-bd3f-4f88-ad70-3c94c2ede221"}}},{"cell_type":"markdown","source":["##### DataFrame Reader"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9db46570-f8dd-4a18-91bb-f9c648e91c35"}}},{"cell_type":"code","source":["# Read from CSV with the DataFrameReader's \"csv\" method and the following options: Tab separator, use first line as header, infer schema\n\nusersCsvPath = \"/mnt/training/ecommerce/users/users-500k.csv\"\n\nusersDF = (spark\n           .read\n           .option(\"sep\", \"\\t\")\n           .option(\"header\", True)\n           .option(\"inferSchema\", True)\n           .csv(usersCsvPath)\n          )\n\nusersDF.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d10bc2db-732e-4e01-8d50-d9cb8383f14a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- user_id: string (nullable = true)\n |-- user_first_touch_timestamp: long (nullable = true)\n |-- email: string (nullable = true)\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- user_id: string (nullable = true)\n |-- user_first_touch_timestamp: long (nullable = true)\n |-- email: string (nullable = true)\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# Spark's Python API also allows you to specify the DataFrameReader options as parameters to the \"csv\" method\n\nusersDF = (spark\n           .read\n           .csv(usersCsvPath, sep=\"\\t\", header=True, inferSchema=True)\n          )\n\nusersDF.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"14449bee-73d3-4a1a-835d-bf7d0018b99b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- user_id: string (nullable = true)\n |-- user_first_touch_timestamp: long (nullable = true)\n |-- email: string (nullable = true)\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- user_id: string (nullable = true)\n |-- user_first_touch_timestamp: long (nullable = true)\n |-- email: string (nullable = true)\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# Manually define the schema by creating a \"StructType\" with column names and data types\n\nfrom pyspark.sql.types import LongType, StringType, StructType, StructField\n\n# from pyspark.sql.types import ArrayType, DoubleType, IntegerType, LongType, StringType, StructType, StructField\n\nuserDefinedSchema = StructType([\n    StructField(\"user_id\", StringType(), True),\n    StructField(\"user_first_touch_timestamp\", LongType(), True),\n    StructField(\"email\", StringType(), True)\n])\n\n# Read from CSV using this user-defined schema instead of inferring the schema\n\nusersDF = (spark\n           .read\n           .option(\"sep\", \"\\t\")\n           .option(\"header\", True)\n           .schema(userDefinedSchema)\n           .csv(usersCsvPath)\n          )\n\n# Alternatively, define the schema using data definition language (DDL) syntax.\n\nDDLSchema = \"user_id string, user_first_touch_timestamp long, email string\"\n\nusersDF = (spark\n           .read\n           .option(\"sep\", \"\\t\")\n           .option(\"header\", True)\n           .schema(DDLSchema)\n           .csv(usersCsvPath)\n          )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"82c59320-8001-413a-9d92-f50cb4431db1"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Read from JSON files \n\neventsJsonPath = \"/mnt/training/ecommerce/events/events-500k.json\"\n\neventsDF = (spark\n            .read\n            .option(\"inferSchema\", True)\n            .json(eventsJsonPath)\n           )\n\neventsDF.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5bdef4e1-91ad-455e-af06-a379aaf5bb80"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- device: string (nullable = true)\n |-- ecommerce: struct (nullable = true)\n |    |-- purchase_revenue_in_usd: double (nullable = true)\n |    |-- total_item_quantity: long (nullable = true)\n |    |-- unique_items: long (nullable = true)\n |-- event_name: string (nullable = true)\n |-- event_previous_timestamp: long (nullable = true)\n |-- event_timestamp: long (nullable = true)\n |-- geo: struct (nullable = true)\n |    |-- city: string (nullable = true)\n |    |-- state: string (nullable = true)\n |-- items: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- coupon: string (nullable = true)\n |    |    |-- item_id: string (nullable = true)\n |    |    |-- item_name: string (nullable = true)\n |    |    |-- item_revenue_in_usd: double (nullable = true)\n |    |    |-- price_in_usd: double (nullable = true)\n |    |    |-- quantity: long (nullable = true)\n |-- traffic_source: string (nullable = true)\n |-- user_first_touch_timestamp: long (nullable = true)\n |-- user_id: string (nullable = true)\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- device: string (nullable = true)\n |-- ecommerce: struct (nullable = true)\n |    |-- purchase_revenue_in_usd: double (nullable = true)\n |    |-- total_item_quantity: long (nullable = true)\n |    |-- unique_items: long (nullable = true)\n |-- event_name: string (nullable = true)\n |-- event_previous_timestamp: long (nullable = true)\n |-- event_timestamp: long (nullable = true)\n |-- geo: struct (nullable = true)\n |    |-- city: string (nullable = true)\n |    |-- state: string (nullable = true)\n |-- items: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- coupon: string (nullable = true)\n |    |    |-- item_id: string (nullable = true)\n |    |    |-- item_name: string (nullable = true)\n |    |    |-- item_revenue_in_usd: double (nullable = true)\n |    |    |-- price_in_usd: double (nullable = true)\n |    |    |-- quantity: long (nullable = true)\n |-- traffic_source: string (nullable = true)\n |-- user_first_touch_timestamp: long (nullable = true)\n |-- user_id: string (nullable = true)\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["%scala\n\n// You can use the `StructType` Scala method `toDDL` to have a DDL-formatted string created for you. In a Python notebook, create a Scala cell to create the string to copy and paste.\n\nspark.read.parquet(\"/mnt/training/ecommerce/events/events.parquet\").schema.toDDL"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6bf10db2-95b5-4631-b63d-1cbafb441c76"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">res0: String = device STRING,ecommerce STRUCT&lt;purchase_revenue_in_usd: DOUBLE, total_item_quantity: BIGINT, unique_items: BIGINT&gt;,event_name STRING,event_previous_timestamp BIGINT,event_timestamp BIGINT,geo STRUCT&lt;city: STRING, state: STRING&gt;,items ARRAY&lt;STRUCT&lt;coupon: STRING, item_id: STRING, item_name: STRING, item_revenue_in_usd: DOUBLE, price_in_usd: DOUBLE, quantity: BIGINT&gt;&gt;,traffic_source STRING,user_first_touch_timestamp BIGINT,user_id STRING\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">res0: String = device STRING,ecommerce STRUCT&lt;purchase_revenue_in_usd: DOUBLE, total_item_quantity: BIGINT, unique_items: BIGINT&gt;,event_name STRING,event_previous_timestamp BIGINT,event_timestamp BIGINT,geo STRUCT&lt;city: STRING, state: STRING&gt;,items ARRAY&lt;STRUCT&lt;coupon: STRING, item_id: STRING, item_name: STRING, item_revenue_in_usd: DOUBLE, price_in_usd: DOUBLE, quantity: BIGINT&gt;&gt;,traffic_source STRING,user_first_touch_timestamp BIGINT,user_id STRING\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["##### DataFrame Writer"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a899bde8-98ac-4798-89e2-fc0e81936de3"}}},{"cell_type":"code","source":["# Parquet method with configuration: Snappy compression, overwrite mode\n\nusersOutputPath = workingDir + \"/users.parquet\"\n\n(usersDF\n .write\n .option(\"compression\", \"snappy\")\n .mode(\"overwrite\")\n .parquet(usersOutputPath)\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b883a286-1f6c-49a7-a8c2-7690b26a16af"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# As with DataFrameReader, Spark's Python API also allows you to specify the DataFrameWriter options as parameters to the \"parquet\" method\n\n(usersDF\n .write\n .parquet(usersOutputPath, compression=\"snappy\", mode=\"overwrite\")\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b5b8d5a3-c873-4c21-9626-d863fb2b3e6a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["storesDF.write.parquet(filePath)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8ea8306e-fe21-45a5-a83e-c7adcb43374b"}}},{"cell_type":"markdown","source":["There is no `repartition()` operation for DataFrameWriter — **the partitionBy()** operation should be used instead."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e2cb031d-b2ab-45f8-9944-df3e716d2cf2"}}},{"cell_type":"markdown","source":["##### Write DataFrame to table"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6f95e20f-f154-40cd-ad3c-b3f5e3b10df1"}}},{"cell_type":"code","source":["# Write \"eventsDF\" to a table using the DataFrameWriter method \"saveAsTable\"\n\n# This creates a global table, unlike the local view created by the DataFrame method \"createOrReplaceTempView\"\n\neventsDF.write.mode(\"overwrite\").saveAsTable(\"events_p\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7917cb45-046c-4eef-b847-4b46f7b46fdc"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##### Delta lake \n\nIn almost all cases, the best practice is to use Delta Lake format, especially whenever the data will be referenced from a Databricks workspace. Delta Lake is an open source technology designed to work with Spark to bring reliability to data lakes.\n\nFeatures:\n- ACID transactions\n- Scalable metadata handline\n- Unified streaming and batch processing\n- Time travel (data versioning)\n- Schema enforcement and evolution\n- Audit history\n- Parquet format\n- Compatible with Apache Spark API"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"75049a90-a966-404e-8c64-86a14626ac10"}}},{"cell_type":"code","source":["eventsOutputPath = workingDir + \"/delta/events\"\n\n(eventsDF\n .write\n .format(\"delta\")\n .mode(\"overwrite\")\n .save(eventsOutputPath)\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e59dd7f4-ccd2-4909-8245-f7f72eba06a0"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["By default Spark is case insensitive; however, you can make Spark case sensitive by setting the configuration:\n`-- in SQL`\n`set spark.sql.caseSensitive true`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4ef8aaf8-8b0c-4bd8-9912-2d561c37f8fe"}}},{"cell_type":"markdown","source":["##### Columns and expressions \n\nYou can select, manipulate and remove columns from dataframes and these operations are represented using expressions. \nA **COLUMN** is a logical construction that will be computed based on the data in a dataframe using an expression. Columns can only be transformed within the context of a dataframe. \n\nHow to refer to a column:\n- df[\"columnName\"]\n- df.columnName\n- col(\"columnName\")\n- col(\"columnName.field\")\n\n(In scala also $)\n\nCreate columns from an expression: \n- col(\"a\") + col(\"b\")  \n- col(\"a\").cast(\"int\") * 100\n\n--> from pyspark.sql.functions import col"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"637ab4d4-22dc-4cc7-ab0a-1a5c279ebd35"}}},{"cell_type":"markdown","source":["An expression is a set of transformations on one or more values in a record in a DataFrame. Think of it like a function that takes as input one or more column names, resolves them, and then potentially applies more expressions to create a single value for each record in the dataset.\n\nIn the simplest case, an expression, created via the expr function, is just a DataFrame column reference. In the simplest case, expr(\"someCol\") is equivalent to col(\"someCol\").\n\nIf you use col() and want to perform transformations on that column, you must perform those on that column reference. When using an expression, the expr function can actually parse transformations and column references from a string and can subsequently be passed into further transformations. \n\nExample: `expr(\"someCol - 5\")` is the same transformation as performing `col(\"someCol\") - 5`, or even `expr(\"someCol\") - 5`.\n\n`(((col(\"someCol\") + 5) * 200) - 6) < col(\"otherCol\")`\n\n`expr(\"(((someCol + 5) * 200) - 6) < otherCol\")`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"77e2b59a-2fe3-43f7-be7b-ba2af55ceb06"}}},{"cell_type":"markdown","source":["##### Column Operators and Methods\n| Method | Description |\n| --- | --- |\n| \\*, + , <, >= | Math and comparison operators |\n| ==, != | Equality and inequality tests (Scala operators are `===` and `=!=`) |\n| alias | Gives the column an alias |\n| cast, astype | Casts the column to a different data type |\n| isNull, isNotNull, isNan | Is null, is not null, is NaN |\n| asc, desc | Returns a sort expression based on ascending/descending order of the column |"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b8658e2e-a4cf-4822-96fc-c07b06ab9987"}}},{"cell_type":"markdown","source":["How to chain two conditions -> A valid answer would be &. Operators like && or and are not valid. Other boolean operators that would be valid in Spark are | and ~."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2ce34bd2-2ca1-4f2f-b2dd-438d7ae98e56"}}},{"cell_type":"markdown","source":["##### Working with Nulls\n\n**coalesce()**  allows you to select the first non-null value from a set of columns by using the coalesce function.\n`from pyspark.sql.functions import coalesce\ndf.select(coalesce(col(\"Description\"), col(\"CustomerId\"))).show()\n`\n\n**ifnull** allows you to select the second value if the first is null, and defaults to the first. Alternatively, you could use **nullif**, which returns null if the two values are equal or else returns the second if they are not. **nvl** returns the second value if the first is null, but defaults to the first. Finally, **nvl2** returns the second value if the first is not null; otherwise, it will return the last specified value."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e528845f-fa03-4854-9ca3-e0ba86c6392c"}}},{"cell_type":"markdown","source":["Example: \n\n`df.withColumn(\"count2\", col(\"count\").cast(\"long\"))`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b37505cc-4ec2-437a-bcad-43b3ca01e1c0"}}},{"cell_type":"markdown","source":["Like for asc(), asc_null_last() does not take any argument, but is applied to column to return a sort expression based on ascending order of the column, and null values appear after non-null values. -> `df.orderBy(col(\"created_date\").asc_null_last())`\n\n* asc_nulls_first(columnName: String): Column - Similar to asc function but null values return first and then non-null values.\n* asc_nulls_last(columnName: String): Column - Similar to asc function but non-null values return first and then null values.\n\n* desc_nulls_first(columnName: String): Column - Similar to desc function but null values return first and then non-null values.\n* desc_nulls_last(columnName: String): Column - Similar to desc function but non-null values return first and then null values.\n\nFor optimization purposes, it’s sometimes advisable to sort within each partition before another set of transformations. You can use the sortWithinPartitions method to do this:\n`spark.read.format(\"json\").load(\"/data/flight-data/json/*-summary.json\")\\\n.sortWithinPartitions(\"count\")`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"51f8f171-469e-43d1-9a43-ed34941def3b"}}},{"cell_type":"markdown","source":["##### DataFrame Transformation Methods\n| Method | Description |\n| --- | --- |\n| select | Returns a new DataFrame by computing given expression for each element |\n| drop | Returns a new DataFrame with a column dropped |\n| withColumnRenamed | Returns a new DataFrame with a column renamed |\n| withColumn | Returns a new DataFrame by adding a column or replacing the existing column that has the same name |\n| filter, where | Filters rows using the given condition |\n| sort, orderBy | Returns a new DataFrame sorted by the given expressions |\n| dropDuplicates, distinct | Returns a new DataFrame with duplicate rows removed |\n| limit | Returns a new DataFrame by taking the first n rows |\n| groupBy | Groups the DataFrame using the specified columns, so we can run aggregation on them |"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c712dfb9-b7c8-4849-aaf6-5c6fc8635801"}}},{"cell_type":"markdown","source":["The rarely used **between()** method. It exists and resolves to ((storeId >= 20) AND (storeId <= 30)) in SQL."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f31acf43-28c8-495c-8471-fa7ecc51451e"}}},{"cell_type":"markdown","source":["Example:\n\n`df.filter(col(\"count\") < 2).show(2)`\n`df.where(\"count < 2\").show(2)`\n\n\n`df.where(col(\"InvoiceNo\") != 536365)\\\n.select(\"InvoiceNo\", \"Description\")\\\n.show(5, False)`\n\n-> Cleaner way: \n`df.where(\"InvoiceNo = 536365\")\n.show(5, false)`\n`df.where(\"InvoiceNo <> 536365\")\n.show(5, false)`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"138b52d4-117d-42b9-8e23-ace024ef2fab"}}},{"cell_type":"markdown","source":["**selectExpr** method when you’re working with expressions in strings. \n\n`df.select(expr(\"DEST_COUNTRY_NAME as destination\").alias(\"DEST_COUNTRY_NAME\"))\\\n.show(2)`\n`df.selectExpr(\"DEST_COUNTRY_NAME as newColumnName\", \"DEST_COUNTRY_NAME\").show(2)`\n\nIn the next example, we’ll set a Boolean flag for when the origin country is the same as the destination country:\n`df.withColumn(\"withinCountry\", expr(\"ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME\"))\\\n.show(2)`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5330efd7-f22a-427d-ba21-0a0e8cef144d"}}},{"cell_type":"markdown","source":["When we call **groupBy**, we end up with a **RelationalGroupedDataset**, which is a fancy name for a DataFrame that has a grouping specified but needs the user to specify an aggregation before it can be queried further. We basically specified that we’re going to be grouping by a key (or set of keys) and that now we’re going to perform an aggregation over each one of those keys."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ce93b538-bf35-46fa-a6b5-4a23f76d5c3f"}}},{"cell_type":"markdown","source":["**Random Samples** \nSometimes, you might just want to sample some random records from your DataFrame. You can do this by using the sample method on a DataFrame, which makes it possible for you to specify\na fraction of rows to extract from a DataFrame and whether you’d like to sample with or without replacement:\n\n`seed = 5\nwithReplacement = False\nfraction = 0.5\ndf.sample(withReplacement, fraction, seed).count()`\n\n**sample()** \nPySpark sampling (pyspark.sql.DataFrame.sample()) is a mechanism to get random sample records from the dataset, this is helpful when you have a larger dataset and wanted to analyze/test a subset of the data for example 10% of the original file.\n\nsample(withReplacement, fraction, seed=None)\n* fraction – Fraction of rows to generate, range [0.0, 1.0]. Note that it doesn’t guarantee to provide the exact number of the fraction of records. (By using fraction between 0 to 1, it returns the approximate number of the fraction of the dataset. For example, 0.1 returns 10% of the rows. However, this does not guarantee it returns the exact 10% of the records.)\n* seed – Seed for sampling (default a random seed). Used to reproduce the same random sampling. Every time you run a sample() function it returns a different set of sampling records, however sometimes during the development and testing phase you may need to regenerate the same sample every time as you need to compare the results from your previous run. To get consistent same random sampling uses the same slice value for every run. Change slice value to get different results.\n`print(df.sample(0.1,123).collect())\n//Output: 36,37,41,43,56,66,69,75,83\nprint(df.sample(0.1,123).collect())\n//Output: 36,37,41,43,56,66,69,75,83\nprint(df.sample(0.1,456).collect())\n//Output: 19,21,42,48,49,50,75,80\n`\n* withReplacement – Sample with replacement or not (default False). Some times you may need to get a random sample with repeated values. By using the value true, results in repeated values."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"00c6752e-7129-43bc-87b5-6e3150b80af9"}}},{"cell_type":"markdown","source":["**Random Splits**\nRandom splits can be helpful when you need to break up your DataFrame into a random “splits” of the original DataFrame. In this next example, we’ll split our DataFrame into two different\nDataFrames by setting the weights by which we will split the DataFrame (these are the arguments to the function). Because this method is designed to be randomized, we will also specify a seed (just replace seed with a number of your choosing in the code block). It’s important to note that if you don’t specify a proportion for each DataFrame that adds up to one, they will be normalized so that they do:\n\n`dataFrames = df.randomSplit([0.25, 0.75], seed)\ndataFrames[0].count() > dataFrames[1].count() # False\n`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1a6b744c-5d1c-409f-97af-b9bff6eb92e8"}}},{"cell_type":"markdown","source":["**Union() and UnionAll()**\n\nDataFrames are immutable. This means users cannot append to DataFrames because that would be changing it. To append to a DataFrame, you must union the original DataFrame along with the new DataFrame. This just concatenates the two DataFramess. To union two DataFrames, you must be sure that they have the same schema and number of columns; otherwise, the union will fail.\n\n* Dataframe union() – union() method of the DataFrame is used to combine two DataFrame’s of the same structure/schema. If schemas are not the same it returns an error.\n* DataFrame unionAll() – unionAll() is deprecated since Spark “2.0.0” version and replaced with union().\n\nNote: In other SQL’s, Union eliminates the duplicates but UnionAll combines two datasets including duplicate records. But, in spark both behave the same and use DataFrame duplicate function to remove duplicate rows."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e6d60b51-1190-4c70-a99a-b0a5e7e5f72f"}}},{"cell_type":"code","source":["# \"selectExpr()\" Selects a list of SQL expressions\n\nappleDF = eventsDF.selectExpr(\"user_id\", \"device in ('macOS', 'iOS') as apple_user\")\nappleDF.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"51249151-ed4c-4b45-915b-774a906403b6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----------------+----------+\n|          user_id|apple_user|\n+-----------------+----------+\n|UA000000107379500|      true|\n|UA000000107359357|     false|\n|UA000000107375547|      true|\n|UA000000107370581|      true|\n|UA000000107377108|     false|\n+-----------------+----------+\nonly showing top 5 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----------------+----------+\n|          user_id|apple_user|\n+-----------------+----------+\n|UA000000107379500|      true|\n|UA000000107359357|     false|\n|UA000000107375547|      true|\n|UA000000107370581|      true|\n|UA000000107377108|     false|\n+-----------------+----------+\nonly showing top 5 rows\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# \"withColumn()\" Returns a new DataFrame by adding a column or replacing an existing column that has the same name.\n\nfrom pyspark.sql.functions import col\n\nmobileDF = eventsDF.withColumn(\"mobile\", col(\"device\").isin(\"iOS\", \"Android\"))\nmobileDF.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c7589358-09ac-4d4f-8859-c478a010e707"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------+------------------+----------+------------------------+----------------+-------------------+--------------------+--------------+--------------------------+-----------------+------+\n| device|         ecommerce|event_name|event_previous_timestamp| event_timestamp|                geo|               items|traffic_source|user_first_touch_timestamp|          user_id|mobile|\n+-------+------------------+----------+------------------------+----------------+-------------------+--------------------+--------------+--------------------------+-----------------+------+\n|  macOS|{null, null, null}|  warranty|        1593878899217692|1593878946592107|     {Montrose, MI}|                  []|        google|          1593878899217692|UA000000107379500| false|\n|Windows|{null, null, null}|     press|        1593876662175340|1593877011756535|  {Northampton, MA}|                  []|        google|          1593876662175340|UA000000107359357| false|\n|  macOS|{null, null, null}|  add_item|        1593878792892652|1593878815459100|      {Salinas, CA}|[{null, M_STAN_T,...|       youtube|          1593878455472030|UA000000107375547| false|\n|    iOS|{null, null, null}|mattresses|        1593878178791663|1593878809276923|      {Everett, MA}|                  []|      facebook|          1593877903116176|UA000000107370581|  true|\n|Windows|{null, null, null}|mattresses|                    null|1593878628143633|{Cottage Grove, MN}|                  []|        google|          1593878628143633|UA000000107377108| false|\n+-------+------------------+----------+------------------------+----------------+-------------------+--------------------+--------------+--------------------------+-----------------+------+\nonly showing top 5 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------+------------------+----------+------------------------+----------------+-------------------+--------------------+--------------+--------------------------+-----------------+------+\n| device|         ecommerce|event_name|event_previous_timestamp| event_timestamp|                geo|               items|traffic_source|user_first_touch_timestamp|          user_id|mobile|\n+-------+------------------+----------+------------------------+----------------+-------------------+--------------------+--------------+--------------------------+-----------------+------+\n|  macOS|{null, null, null}|  warranty|        1593878899217692|1593878946592107|     {Montrose, MI}|                  []|        google|          1593878899217692|UA000000107379500| false|\n|Windows|{null, null, null}|     press|        1593876662175340|1593877011756535|  {Northampton, MA}|                  []|        google|          1593876662175340|UA000000107359357| false|\n|  macOS|{null, null, null}|  add_item|        1593878792892652|1593878815459100|      {Salinas, CA}|[{null, M_STAN_T,...|       youtube|          1593878455472030|UA000000107375547| false|\n|    iOS|{null, null, null}|mattresses|        1593878178791663|1593878809276923|      {Everett, MA}|                  []|      facebook|          1593877903116176|UA000000107370581|  true|\n|Windows|{null, null, null}|mattresses|                    null|1593878628143633|{Cottage Grove, MN}|                  []|        google|          1593878628143633|UA000000107377108| false|\n+-------+------------------+----------+------------------------+----------------+-------------------+--------------------+--------------+--------------------------+-----------------+------+\nonly showing top 5 rows\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["locationDF = eventsDF.withColumnRenamed(\"geo\", \"location\")\nlocationDF.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f779bd88-0ea5-4f3f-9288-4632565cedaa"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------+------------------+----------+------------------------+----------------+-------------------+--------------------+--------------+--------------------------+-----------------+\n| device|         ecommerce|event_name|event_previous_timestamp| event_timestamp|           location|               items|traffic_source|user_first_touch_timestamp|          user_id|\n+-------+------------------+----------+------------------------+----------------+-------------------+--------------------+--------------+--------------------------+-----------------+\n|  macOS|{null, null, null}|  warranty|        1593878899217692|1593878946592107|     {Montrose, MI}|                  []|        google|          1593878899217692|UA000000107379500|\n|Windows|{null, null, null}|     press|        1593876662175340|1593877011756535|  {Northampton, MA}|                  []|        google|          1593876662175340|UA000000107359357|\n|  macOS|{null, null, null}|  add_item|        1593878792892652|1593878815459100|      {Salinas, CA}|[{null, M_STAN_T,...|       youtube|          1593878455472030|UA000000107375547|\n|    iOS|{null, null, null}|mattresses|        1593878178791663|1593878809276923|      {Everett, MA}|                  []|      facebook|          1593877903116176|UA000000107370581|\n|Windows|{null, null, null}|mattresses|                    null|1593878628143633|{Cottage Grove, MN}|                  []|        google|          1593878628143633|UA000000107377108|\n+-------+------------------+----------+------------------------+----------------+-------------------+--------------------+--------------+--------------------------+-----------------+\nonly showing top 5 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------+------------------+----------+------------------------+----------------+-------------------+--------------------+--------------+--------------------------+-----------------+\n| device|         ecommerce|event_name|event_previous_timestamp| event_timestamp|           location|               items|traffic_source|user_first_touch_timestamp|          user_id|\n+-------+------------------+----------+------------------------+----------------+-------------------+--------------------+--------------+--------------------------+-----------------+\n|  macOS|{null, null, null}|  warranty|        1593878899217692|1593878946592107|     {Montrose, MI}|                  []|        google|          1593878899217692|UA000000107379500|\n|Windows|{null, null, null}|     press|        1593876662175340|1593877011756535|  {Northampton, MA}|                  []|        google|          1593876662175340|UA000000107359357|\n|  macOS|{null, null, null}|  add_item|        1593878792892652|1593878815459100|      {Salinas, CA}|[{null, M_STAN_T,...|       youtube|          1593878455472030|UA000000107375547|\n|    iOS|{null, null, null}|mattresses|        1593878178791663|1593878809276923|      {Everett, MA}|                  []|      facebook|          1593877903116176|UA000000107370581|\n|Windows|{null, null, null}|mattresses|                    null|1593878628143633|{Cottage Grove, MN}|                  []|        google|          1593878628143633|UA000000107377108|\n+-------+------------------+----------+------------------------+----------------+-------------------+--------------------+--------------+--------------------------+-----------------+\nonly showing top 5 rows\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# \"filter()\" Filters rows using the given SQL expression or column based condition.\n\npurchasesDF = eventsDF.filter(\"ecommerce.total_item_quantity > 0\")\npurchasesDF.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fff4f99c-fe96-4906-9334-35d50a6533ec"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------+--------------+----------+------------------------+----------------+------------------+--------------------+--------------+--------------------------+-----------------+\n| device|     ecommerce|event_name|event_previous_timestamp| event_timestamp|               geo|               items|traffic_source|user_first_touch_timestamp|          user_id|\n+-------+--------------+----------+------------------------+----------------+------------------+--------------------+--------------+--------------------------+-----------------+\n|  Linux|{1195.0, 1, 1}|  finalize|        1593878893766134|1593878897648871|     {Shawnee, KS}|[{null, M_STAN_K,...|        google|          1593876996316576|UA000000107362263|\n|    iOS|{1045.0, 1, 1}|  finalize|        1593878485345763|1593878487460247|     {Detroit, MI}|[{null, M_STAN_Q,...|      facebook|          1593877230282722|UA000000107364432|\n|Android| {595.0, 1, 1}|  finalize|        1593877930076602|1593878966392505|{East Chicago, IN}|[{null, M_STAN_T,...|        google|          1593876889575474|UA000000107361347|\n|    iOS|{2290.0, 2, 2}|  finalize|        1593877650094042|1593877652106953|     {Warwick, RI}|[{null, M_PREM_F,...|        google|          1593876687337581|UA000000107359573|\n|  macOS| {945.0, 1, 1}|  finalize|        1593879151529456|1593879197837168|   {Boonville, MO}|[{null, M_STAN_F,...|      facebook|          1593878603312910|UA000000107376872|\n+-------+--------------+----------+------------------------+----------------+------------------+--------------------+--------------+--------------------------+-----------------+\nonly showing top 5 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------+--------------+----------+------------------------+----------------+------------------+--------------------+--------------+--------------------------+-----------------+\n| device|     ecommerce|event_name|event_previous_timestamp| event_timestamp|               geo|               items|traffic_source|user_first_touch_timestamp|          user_id|\n+-------+--------------+----------+------------------------+----------------+------------------+--------------------+--------------+--------------------------+-----------------+\n|  Linux|{1195.0, 1, 1}|  finalize|        1593878893766134|1593878897648871|     {Shawnee, KS}|[{null, M_STAN_K,...|        google|          1593876996316576|UA000000107362263|\n|    iOS|{1045.0, 1, 1}|  finalize|        1593878485345763|1593878487460247|     {Detroit, MI}|[{null, M_STAN_Q,...|      facebook|          1593877230282722|UA000000107364432|\n|Android| {595.0, 1, 1}|  finalize|        1593877930076602|1593878966392505|{East Chicago, IN}|[{null, M_STAN_T,...|        google|          1593876889575474|UA000000107361347|\n|    iOS|{2290.0, 2, 2}|  finalize|        1593877650094042|1593877652106953|     {Warwick, RI}|[{null, M_PREM_F,...|        google|          1593876687337581|UA000000107359573|\n|  macOS| {945.0, 1, 1}|  finalize|        1593879151529456|1593879197837168|   {Boonville, MO}|[{null, M_STAN_F,...|      facebook|          1593878603312910|UA000000107376872|\n+-------+--------------+----------+------------------------+----------------+------------------+--------------------+--------------+--------------------------+-----------------+\nonly showing top 5 rows\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# \"dropDuplicates()\" Returns a new DataFrame with duplicate rows removed, optionally considering only a subset of columns.\n# Alias: \"distinct\"\n\ndistinctUsersDF = eventsDF.dropDuplicates([\"user_id\"])\ndistinctUsersDF.show(5)\n\n\n# distinctDF = purchasesDF.select(\"event_name\").distinct()\n# distinctDF = purchasesDF.dropDuplicates([\"event_name\"])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a9937cf0-4eae-48e8-9082-1a6a7435e904"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------+------------------+-------------+------------------------+----------------+-----------------+--------------------+--------------+--------------------------+-----------------+\n| device|         ecommerce|   event_name|event_previous_timestamp| event_timestamp|              geo|               items|traffic_source|user_first_touch_timestamp|          user_id|\n+-------+------------------+-------------+------------------------+----------------+-----------------+--------------------+--------------+--------------------------+-----------------+\n|    iOS|{null, null, null}|     checkout|        1592547736518007|1592548321455992|  {San Bruno, CA}|[{NEWBED10, M_STA...|         email|          1592196947865522|UA000000102357807|\n|Android|{null, null, null}|     add_item|        1592573713168269|1592574347642610|     {Mobile, AL}|[{NEWBED10, M_STA...|         email|          1592198812458125|UA000000102358054|\n|  macOS|{null, null, null}|shipping_info|        1592545562314108|1592545941007576|      {Largo, FL}|[{NEWBED10, P_FOA...|         email|          1592199427202331|UA000000102358165|\n|    iOS|{null, null, null}|     register|        1592553755082252|1592558423806848|{Mounds View, MN}|[{NEWBED10, M_STA...|         email|          1592205037961396|UA000000102359895|\n|    iOS|{null, null, null}|     add_item|        1592583618219827|1592584060201694|  {Gibraltar, MI}|[{NEWBED10, M_STA...|         email|          1592205125802184|UA000000102359929|\n+-------+------------------+-------------+------------------------+----------------+-----------------+--------------------+--------------+--------------------------+-----------------+\nonly showing top 5 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------+------------------+-------------+------------------------+----------------+-----------------+--------------------+--------------+--------------------------+-----------------+\n| device|         ecommerce|   event_name|event_previous_timestamp| event_timestamp|              geo|               items|traffic_source|user_first_touch_timestamp|          user_id|\n+-------+------------------+-------------+------------------------+----------------+-----------------+--------------------+--------------+--------------------------+-----------------+\n|    iOS|{null, null, null}|     checkout|        1592547736518007|1592548321455992|  {San Bruno, CA}|[{NEWBED10, M_STA...|         email|          1592196947865522|UA000000102357807|\n|Android|{null, null, null}|     add_item|        1592573713168269|1592574347642610|     {Mobile, AL}|[{NEWBED10, M_STA...|         email|          1592198812458125|UA000000102358054|\n|  macOS|{null, null, null}|shipping_info|        1592545562314108|1592545941007576|      {Largo, FL}|[{NEWBED10, P_FOA...|         email|          1592199427202331|UA000000102358165|\n|    iOS|{null, null, null}|     register|        1592553755082252|1592558423806848|{Mounds View, MN}|[{NEWBED10, M_STA...|         email|          1592205037961396|UA000000102359895|\n|    iOS|{null, null, null}|     add_item|        1592583618219827|1592584060201694|  {Gibraltar, MI}|[{NEWBED10, M_STA...|         email|          1592205125802184|UA000000102359929|\n+-------+------------------+-------------+------------------------+----------------+-----------------+--------------------+--------------+--------------------------+-----------------+\nonly showing top 5 rows\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["eventsDF.distinct()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9ac18c68-c699-48d1-a9fe-e1fe50dbc0d3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[11]: DataFrame[device: string, ecommerce: struct<purchase_revenue_in_usd:double,total_item_quantity:bigint,unique_items:bigint>, event_name: string, event_previous_timestamp: bigint, event_timestamp: bigint, geo: struct<city:string,state:string>, items: array<struct<coupon:string,item_id:string,item_name:string,item_revenue_in_usd:double,price_in_usd:double,quantity:bigint>>, traffic_source: string, user_first_touch_timestamp: bigint, user_id: string]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[11]: DataFrame[device: string, ecommerce: struct<purchase_revenue_in_usd:double,total_item_quantity:bigint,unique_items:bigint>, event_name: string, event_previous_timestamp: bigint, event_timestamp: bigint, geo: struct<city:string,state:string>, items: array<struct<coupon:string,item_id:string,item_name:string,item_revenue_in_usd:double,price_in_usd:double,quantity:bigint>>, traffic_source: string, user_first_touch_timestamp: bigint, user_id: string]"]}}],"execution_count":0},{"cell_type":"markdown","source":["**Managed/unmanaged table**.\nWhen you define a table from files on disk, you are defining an unmanaged table.\nWhen you use saveAsTable on a DataFrame, you are instead creating a managed table for which Spark will track of all of the relevant information.\n\nIf you drop a managed table, both the data and the table definition will be removed. If you are dropping an unmanaged table, no data will be removed but you will no longer be able to refer to this data by the table name."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"af778ef4-2437-4fe3-9ae6-32147ebf75fb"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"SparkCertification-DataFrames","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":1503221511970106}},"nbformat":4,"nbformat_minor":0}
